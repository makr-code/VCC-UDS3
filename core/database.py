#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimized Unified Database Strategy v3.0 - With Security & Quality Framework
Umfassende Verwaltung ALLER verwaltungsrechtlicher Dokumente

DOKUMENTBEREICH:
- Normative Ebene: Gesetze, Verordnungen, Ausführungsbestimmungen, Richtlinien
- Verwaltungsentscheidungen: Bescheide, Verfügungen, Planfeststellungen
- Gerichtsentscheidungen: VG/OVG/BVerwG/BVerfG-Entscheidungen (nur ein Teilbereich!)
- Verwaltungsinterne Dokumente: Aktennotizen, Gutachten, Korrespondenz

DATENBANKROLLEN:
Vector DB: Semantische Suche über alle Dokumenttypen, Cross-Domain-Ähnlichkeit
Graph DB: Normenhierarchien, Verwaltungsverfahren, Behördenstrukturen, Präzedenzfälle
Relational DB: Metadaten, Fristen, Verfahrensstatus, Compliance-Monitoring

Erweitert um:
- Advanced Data Security (Hash-Werte, UUIDs, Verschlüsselung)
- Data Quality Management (Scoring, Validation, Monitoring)
- Integrity Verification und Quality Assurance
"""

import logging
import hashlib
import os
import uuid
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

# Import Security & Quality Framework
try:
    # KONSOLIDIERT: Nur noch compliance/security_quality.py verwenden
    from compliance.security_quality import (
        SecurityLevel,
        DataSecurityManager,
        create_security_manager,
        DataQualityManager,
        QualityMetric,
        create_quality_manager,
    )

    SECURITY_QUALITY_AVAILABLE = True
except ImportError:
    SECURITY_QUALITY_AVAILABLE = False
    print("Warning: Security & Quality Framework not available")

# Lazy Import DSGVO Core Framework (verhindert zirkuläre Imports)
UDS3_DSGVO_AVAILABLE = False
_dsgvo_imports = {}

def _lazy_import_dsgvo():
    """Lazy import der DSGVO Core Komponenten"""
    global UDS3_DSGVO_AVAILABLE, _dsgvo_imports
    
    if _dsgvo_imports:
        return _dsgvo_imports
        
    try:
        from uds3_dsgvo_core import (
            UDS3DSGVOCore,
            DSGVOOperationType, 
            PIIType,
            DSGVOProcessingBasis
        )
        
        _dsgvo_imports = {
            'UDS3DSGVOCore': UDS3DSGVOCore,
            'DSGVOOperationType': DSGVOOperationType,
            'PIIType': PIIType, 
            'DSGVOProcessingBasis': DSGVOProcessingBasis
        }
        
        UDS3_DSGVO_AVAILABLE = True
        logger = logging.getLogger(__name__)
        logger.info("✅ UDS3 DSGVO Core Framework lazy loaded")
        return _dsgvo_imports
        
    except ImportError as e:
        UDS3_DSGVO_AVAILABLE = False
        logger = logging.getLogger(__name__)
        logger.warning(f"⚠️ UDS3 DSGVO Core Framework nicht verfügbar: {e}")
        return {}

def get_dsgvo_class(class_name: str):
    """Helper um DSGVO Klassen lazy zu laden"""
    imports = _lazy_import_dsgvo()
    return imports.get(class_name)

# Import Delete Operations Module
try:
    from manager.delete import (
        SoftDeleteManager,
        HardDeleteManager,
        DeleteStrategy,
        CascadeStrategy,
        RestoreStrategy,
        DeleteOperationsOrchestrator,
        ARCHIVE_AVAILABLE,
    )
    DELETE_OPS_AVAILABLE = True
except ImportError:
    DELETE_OPS_AVAILABLE = False
    ARCHIVE_AVAILABLE = False
    print("Warning: Delete Operations module not available")

# Import Archive Operations Module
try:
    from manager.archive import (
        ArchiveManager,
        create_archive_manager,
        ArchiveResult,
        RestoreResult,
        RetentionPolicy,
        RetentionPeriod,
        ArchiveStatus,
    )
    ARCHIVE_OPS_AVAILABLE = True
except ImportError:
    ARCHIVE_OPS_AVAILABLE = False
    print("Warning: Archive Operations module not available")

# Import Streaming Operations Module
try:
    from manager.streaming import (
        StreamingManager,
        create_streaming_manager,
        StreamingProgress,
        StreamingStatus,
        StreamingOperation,
        StreamingSagaConfig,
        SagaRollbackRequired,
        DEFAULT_CHUNK_SIZE,
        calculate_optimal_chunk_size,
        format_bytes,
        format_duration,
    )
    STREAMING_OPS_AVAILABLE = True
except ImportError:
    STREAMING_OPS_AVAILABLE = False
    print("Warning: Streaming Operations module not available")

# Import Streaming Saga Integration
try:
    from manager.streaming_saga import (
        SagaStatus,
        SagaStep,
        SagaDefinition,
        SagaExecutionResult,
        execute_streaming_saga_with_rollback,
        build_streaming_upload_saga_definition,
        StreamingSagaMonitor,
        store_rollback_failures,
    )
    STREAMING_SAGA_AVAILABLE = True
except ImportError:
    STREAMING_SAGA_AVAILABLE = False
    print("Warning: Streaming Saga Integration not available")

# Import Advanced CRUD Operations Module
try:
    from api.crud import (
        AdvancedCRUDManager,
        BatchReadResult,
        ConditionalUpdateResult,
        UpsertResult,
        Condition,
        ConditionOperator,
        MergeStrategy,
        ReadStrategy,
    )
    ADVANCED_CRUD_AVAILABLE = True
except ImportError:
    ADVANCED_CRUD_AVAILABLE = False
    print("Warning: Advanced CRUD Operations module not available")

# Import Vector Filter Module
try:
    from api.vector_filter import (
        VectorFilter,
        SimilarityQuery,
        VectorQueryResult,
        create_vector_filter,
    )
    from api.filters import FilterOperator, SortOrder
    VECTOR_FILTER_AVAILABLE = True
except ImportError:
    VECTOR_FILTER_AVAILABLE = False
    print("Warning: Vector Filter module not available")

# Import Graph Filter Module
try:
    from api.graph_filter import (
        GraphFilter,
        NodeFilter,
        RelationshipFilter,
        GraphQueryResult,
        RelationshipDirection,
        create_graph_filter,
    )
    GRAPH_FILTER_AVAILABLE = True
except ImportError:
    GRAPH_FILTER_AVAILABLE = False
    print("Warning: Graph Filter module not available")

# Import Relational Filter Module
try:
    from api.relational_filter import (
        RelationalFilter,
        SelectField,
        JoinClause,
        JoinType,
        SQLDialect,
        AggregateFunction,
        RelationalQueryResult,
        create_relational_filter,
    )
    RELATIONAL_FILTER_AVAILABLE = True
except ImportError:
    RELATIONAL_FILTER_AVAILABLE = False
    print("Warning: Relational Filter module not available")

# Import UDS3 Relations Core
try:
    from .framework import (
        UDS3RelationsDataFramework,
        get_uds3_relations_framework,
    )

    UDS3_RELATIONS_AVAILABLE = True
except ImportError:
    UDS3_RELATIONS_AVAILABLE = False
    print("Warning: UDS3 Relations Data Framework not available")

# Import UDS3 Database Schemas Mixin (OPTIMIZED: Extracted from uds3_core.py)
try:
    from uds3.uds3_database_schemas import UDS3DatabaseSchemasMixin
    SCHEMAS_MIXIN_AVAILABLE = True
except ImportError:
    try:
        # Fallback: Relative Import
        from .schemas import UDS3DatabaseSchemasMixin
        SCHEMAS_MIXIN_AVAILABLE = True
    except ImportError:
        SCHEMAS_MIXIN_AVAILABLE = False
        # Fallback: Define empty Mixin
        class UDS3DatabaseSchemasMixin:  # type: ignore
            """Fallback Mixin wenn uds3_database_schemas.py nicht verfügbar"""
            pass

# Import UDS3 CRUD Strategies Mixin (OPTIMIZED: Extracted from uds3_core.py)
try:
    from uds3.uds3_crud_strategies import UDS3CRUDStrategiesMixin
    CRUD_STRATEGIES_MIXIN_AVAILABLE = True
except ImportError:
    try:
        # Fallback: Relative Import
        from api.crud_strategies import UDS3CRUDStrategiesMixin
        CRUD_STRATEGIES_MIXIN_AVAILABLE = True
    except ImportError:
        CRUD_STRATEGIES_MIXIN_AVAILABLE = False
        # Fallback: Define empty Mixin
        class UDS3CRUDStrategiesMixin:  # type: ignore
            """Fallback Mixin wenn uds3_crud_strategies.py nicht verfügbar"""
            pass

# Identity Service Integration
try:
    from uds3_identity_service import get_identity_service, IdentityServiceError

    IDENTITY_SERVICE_AVAILABLE = True
except ImportError:
    IDENTITY_SERVICE_AVAILABLE = False
    get_identity_service = None  # type: ignore
    IdentityServiceError = Exception  # type: ignore

# Saga Orchestrator Integration
try:
    from .uds3_saga_orchestrator import (
        get_saga_orchestrator,
        SagaDefinition,
        SagaExecutionError,
        SagaExecutionResult,
        SagaStatus,
        SagaStep,
    )

    SAGA_ORCHESTRATOR_AVAILABLE = True
except ImportError:
    try:
        from manager.saga import (
            get_saga_orchestrator,
            SagaDefinition,
            SagaExecutionError,
            SagaExecutionResult,
            SagaStatus,
            SagaStep,
        )

        SAGA_ORCHESTRATOR_AVAILABLE = True
    except ImportError:
        SAGA_ORCHESTRATOR_AVAILABLE = False
        get_saga_orchestrator = None  # type: ignore
        SagaDefinition = None  # type: ignore
        SagaExecutionError = Exception  # type: ignore
        SagaExecutionResult = None  # type: ignore
        SagaStatus = None  # type: ignore
        SagaStep = None  # type: ignore

# Saga Compliance & Governance Integration
try:
    from manager.compliance import (
        SagaComplianceEngine,
        SagaMonitoringInterface,
        SagaAdminInterface,
        SagaReportingInterface,
        ComplianceReport,
        ComplianceStatus,
        SagaHealthMetrics,
        Alert,
        AdminAction,
        AuditTrail,
        create_compliance_engine,
        create_monitoring_interface,
        create_admin_interface,
        create_reporting_interface,
    )
    SAGA_COMPLIANCE_AVAILABLE = True
except ImportError:
    SAGA_COMPLIANCE_AVAILABLE = False
    print("Warning: Saga Compliance & Governance module not available")

# Import VPB Operations Module - Not implemented
VPB_OPERATIONS_AVAILABLE = False

# Import File Storage Filter Module
try:
    from api.file_filter import (
        FileMetadata,
        FileSearchQuery,
        FileFilterResult,
        FileType,
        SizeUnit,
        SortOrder,
        FilterOperator,
        FileStorageBackend,
        LocalFileSystemBackend,
        FileStorageFilter,
        create_file_storage_filter,
        create_local_backend,
        create_search_query,
    )
    FILE_STORAGE_FILTER_AVAILABLE = True
except ImportError:
    FILE_STORAGE_FILTER_AVAILABLE = False
    print("Warning: File Storage Filter module not available")

# Import Polyglot Query Module
try:
    from api.query import (
        PolyglotQuery,
        JoinStrategy,
        ExecutionMode,
        DatabaseType,
        PolyglotQueryResult,
        create_polyglot_query,
    )
    POLYGLOT_QUERY_AVAILABLE = True
except ImportError:
    POLYGLOT_QUERY_AVAILABLE = False
    print("Warning: Polyglot Query module not available")

# Import Single Record Cache Module
try:
    from .cache import (
        SingleRecordCache,
        CacheEntry,
        CacheStatistics,
        CacheConfig,
        InvalidationStrategy,
        create_single_record_cache,
    )
    SINGLE_RECORD_CACHE_AVAILABLE = True
except ImportError:
    SINGLE_RECORD_CACHE_AVAILABLE = False
    print("Warning: Single Record Cache module not available")

try:
    from uds3.database.saga_crud import SagaDatabaseCRUD  # type: ignore
    DATABASE_MODULE_AVAILABLE = True
except Exception:  # pragma: no cover - optional dependency
    try:
        # OPTIMIZED (1. Okt 2025 - Todo #6a): 
        # SagaDatabaseCRUD Fallback extrahiert nach database/saga_step_builders.py
        from uds3.database.saga_step_builders import SagaDatabaseCRUD  # type: ignore
        DATABASE_MODULE_AVAILABLE = True
    except Exception:
        # Fallback: Mock-Klasse wenn database-Modul nicht verfügbar
        DATABASE_MODULE_AVAILABLE = False
        print("Warning: database module not available - using mock fallback")
        
        class SagaDatabaseCRUD:  # type: ignore
            """Mock SagaDatabaseCRUD für Fallback"""
            def __init__(self, *args, **kwargs):
                pass

try:
    from uds3.database.adapter_governance import AdapterGovernanceError  # type: ignore
except Exception:  # pragma: no cover - optional dependency
    AdapterGovernanceError = Exception  # type: ignore

logger = logging.getLogger(__name__)


class DatabaseRole(Enum):
    """Definiert die Hauptrollen der verschiedenen Datenbank-Typen für Verwaltungsrecht"""

    VECTOR = "semantic_search"  # Semantische Suche über alle Dokumenttypen
    GRAPH = "admin_relationships"  # Verwaltungsstrukturen, Normenhierarchien, Verfahren
    RELATIONAL = "admin_metadata"  # Metadaten, Fristen, Verfahrensstatus
    FILE = "file_storage"  # Speicherung der Originaldateien (optional)


class OperationType(Enum):
    """Definiert die verfügbaren CRUD-Operationen"""

    CREATE = "create"
    READ = "read"
    UPDATE = "update"
    DELETE = "delete"
    MERGE = "merge"  # Upsert-Operation
    ARCHIVE = "archive"  # Soft Delete
    RESTORE = "restore"  # Restore from Archive


class SyncStrategy(Enum):
    """Synchronisationsstrategien zwischen Datenbanken"""

    IMMEDIATE = "immediate"  # Sofortige Synchronisation
    DEFERRED = "deferred"  # Verzögerte Batch-Synchronisation
    EVENTUAL = "eventual"  # Eventual Consistency
    MANUAL = "manual"  # Manuelle Synchronisation


@dataclass
class DatabaseOptimization:
    """Optimierungsstrategien für spezifische Datenbank-Typen"""

    vector_dimensions: int = 1536  # OpenAI Ada-002 Standard
    vector_similarity_metric: str = "cosine"
    graph_index_properties: List[str] = None
    relational_indexes: List[str] = None
    batch_sizes: Dict[str, int] = None


class UnifiedDatabaseStrategy(UDS3DatabaseSchemasMixin, UDS3CRUDStrategiesMixin):
    """
    Erweiterte Unified Database Strategy mit Security & Quality Framework
    
    OPTIMIZED (1. Okt 2025): Mixins für modulare Code-Organisation
    - UDS3DatabaseSchemasMixin: Schema Definitions (extrahiert)
    - UDS3CRUDStrategiesMixin: CRUD Strategies (extrahiert)

    VECTOR DB (ChromaDB/Pinecone):
    - Semantische Suche in Dokumenteninhalten
    - Content-Embedding für Ähnlichkeitssuche
    - Chunk-basierte Vektoren für granulare Suche

    GRAPH DB (Neo4j/ArangoDB):
    - Dokument-Beziehungen und Vernetzung
    - Autorennetzwerke und Zitationsnetze
    - Rechtsprechungs-Hierarchien

    RELATIONAL DB (SQLite/PostgreSQL):
    - Strukturierte Metadaten und Keywords
    - Schnelle Filterung und Indexierung
    - Statistiken und Aggregationen

    ERWEITERT UM:
    - Data Security: Hash-basierte Integrität, UUIDs, Verschlüsselung
    - Data Quality: Multi-dimensionale Qualitätsbewertung
    - Cross-Database Validation: Konsistenzprüfung zwischen DBs
    """

    version = "UDS3.0_optimized"  # Version für Database-API-Logging

    def __init__(
        self,
        security_level: "SecurityLevel" = None,
        strict_quality: bool = False,
        *,
        enforce_governance: bool = True,
        naming_config: Optional[Dict[str, Any]] = None,
        enable_dynamic_naming: bool = True,
    ):
        # Initialize core managers and integrations (clean, consistently-indented)
        # Security & Quality
        if SECURITY_QUALITY_AVAILABLE:
            self.security_manager = create_security_manager(
                security_level or SecurityLevel.INTERNAL
            )
            self.quality_manager = create_quality_manager(strict_quality)
        else:
            self.security_manager = None
            self.quality_manager = None

        # Relations framework
        if UDS3_RELATIONS_AVAILABLE:
            try:
                self.relations_framework = get_uds3_relations_framework()
                self.relations_enabled = True
                logger.info("✅ UDS3 Relations Data Framework integriert")
            except Exception:
                self.relations_framework = None
                self.relations_enabled = False
                logger.warning(
                    "⚠️ UDS3 Relations Data Framework konnte nicht initialisiert werden"
                )
        else:
            self.relations_framework = None
            self.relations_enabled = False

        # Identity service
        if IDENTITY_SERVICE_AVAILABLE and get_identity_service is not None:
            try:
                self.identity_service = get_identity_service()
                logger.info("✅ UDS3 Identity Service integriert")
            except IdentityServiceError as exc:
                self.identity_service = None
                logger.warning(
                    f"⚠️ UDS3 Identity Service konnte nicht initialisiert werden: {exc}"
                )
            except Exception as exc:
                self.identity_service = None
                logger.error(f"🚨 UDS3 Identity Service Fehler: {exc}")
        else:
            self.identity_service = None

        # Saga orchestrator
        if SAGA_ORCHESTRATOR_AVAILABLE and get_saga_orchestrator is not None:
            try:
                self.saga_orchestrator = get_saga_orchestrator()
                logger.info("✅ UDS3 Saga Orchestrator integriert")
            except Exception as exc:
                self.saga_orchestrator = None
                logger.warning(
                    f"⚠️ UDS3 Saga Orchestrator konnte nicht initialisiert werden: {exc}"
                )
        else:
            self.saga_orchestrator = None

        # Backend-Attribute für Delete/Archive Operations (Fix für fehlende Attribute)
        self.vector_backend = None
        self.graph_backend = None  
        self.relational_backend = None
        self.file_backend = None

        # Delete Operations Manager
        self.soft_delete_manager = None
        self.hard_delete_manager = None
        self.delete_ops_orchestrator = None
        if DELETE_OPS_AVAILABLE:
            try:
                self.soft_delete_manager = SoftDeleteManager(self)
                self.hard_delete_manager = HardDeleteManager(self)
                self.delete_ops_orchestrator = DeleteOperationsOrchestrator(self)
                logger.info("✅ Delete Operations Manager integriert")
            except Exception as exc:
                logger.warning(
                    f"⚠️ Delete Operations Manager konnte nicht initialisiert werden: {exc}"
                )
        
        # Archive Operations Manager
        self.archive_manager = None
        if ARCHIVE_OPS_AVAILABLE:
            try:
                self.archive_manager = create_archive_manager(self)
                logger.info("✅ Archive Operations Manager integriert")
            except Exception as exc:
                logger.warning(
                    f"⚠️ Archive Operations Manager konnte nicht initialisiert werden: {exc}"
                )
        
        # Streaming Operations Manager
        self.streaming_manager = None
        self.streaming_saga_monitor = None
        if STREAMING_OPS_AVAILABLE:
            try:
                self.streaming_manager = create_streaming_manager(
                    storage_backend=None,  # Will be set later if available
                    chunk_size=DEFAULT_CHUNK_SIZE
                )
                logger.info("✅ Streaming Operations Manager integriert (chunk_size=5MB)")
            except Exception as exc:
                logger.warning(
                    f"⚠️ Streaming Operations Manager konnte nicht initialisiert werden: {exc}"
                )
        
        # Streaming Saga Monitor
        if STREAMING_SAGA_AVAILABLE:
            try:
                self.streaming_saga_monitor = StreamingSagaMonitor()
                logger.info("✅ Streaming Saga Monitor integriert")
            except Exception as exc:
                logger.warning(
                    f"⚠️ Streaming Saga Monitor konnte nicht initialisiert werden: {exc}"
                )
            except Exception as exc:
                logger.warning(
                    f"⚠️ Streaming Operations Manager konnte nicht initialisiert werden: {exc}"
                )
        
        # Advanced CRUD Operations Manager
        self.advanced_crud_manager = None
        if ADVANCED_CRUD_AVAILABLE:
            try:
                self.advanced_crud_manager = AdvancedCRUDManager(self)
                logger.info("✅ Advanced CRUD Manager integriert")
            except Exception as exc:
                logger.warning(
                    f"⚠️ Advanced CRUD Manager konnte nicht initialisiert werden: {exc}"
                )
        
        # Single Record Cache
        self.single_record_cache = None
        self.cache_enabled = False
        if SINGLE_RECORD_CACHE_AVAILABLE:
            try:
                # Default cache config: 1000 entries, 5min TTL
                self.single_record_cache = create_single_record_cache(
                    max_size=1000,
                    default_ttl_seconds=300.0,
                    enable_auto_cleanup=True
                )
                self.cache_enabled = True
                logger.info("✅ Single Record Cache integriert (1000 entries, 300s TTL)")
            except Exception as exc:
                self.single_record_cache = None
                self.cache_enabled = False
                logger.warning(
                    f"⚠️ Single Record Cache konnte nicht initialisiert werden: {exc}"
                )
                
        # DSGVO Core Integration wird später initialisiert (nach _database_manager)
        self.dsgvo_core = None

        # Naming Strategy (NEU!)
        self.enable_dynamic_naming = enable_dynamic_naming
        self.naming_manager = None
        if enable_dynamic_naming:
            try:
                from api.naming_integration import NamingContextManager
                self.naming_manager = NamingContextManager(**(naming_config or {}))
                logger.info("✅ UDS3 Dynamic Naming Strategy aktiviert")
            except ImportError as exc:
                logger.warning(f"⚠️ Dynamic Naming nicht verfügbar: {exc}")
                self.enable_dynamic_naming = False
            except Exception as exc:
                logger.warning(f"⚠️ Naming Strategy Fehler: {exc}")
                self.enable_dynamic_naming = False

        # Database and governance placeholders
        self._database_manager = None
        self._adapter_governance = None
        self.enforce_governance = enforce_governance
        self.saga_crud = SagaDatabaseCRUD(manager_getter=self._resolve_database_manager)
        
        # Search API (lazy-loaded)
        self._search_api = None
        
        # Wrap SagaCRUD with Dynamic Naming (falls aktiviert)
        if self.enable_dynamic_naming and self.naming_manager:
            try:
                from api.naming_integration import create_naming_enabled_saga_crud
                self.saga_crud = create_naming_enabled_saga_crud(
                    saga_crud_instance=self.saga_crud,
                    naming_manager=self.naming_manager
                )
                logger.info("✅ SagaCRUD mit Dynamic Naming erweitert")
            except Exception as exc:
                logger.warning(f"⚠️ SagaCRUD Naming-Wrapper Fehler: {exc}")

        # Basic structures
        self.document_mapping: dict[Any, Any] = {}
        self.batch_operations: list[Any] = []
        self.batch_size = 100

        self.optimization_config = DatabaseOptimization(
            batch_sizes={"vector": 50, "graph": 100, "relational": 200}
        )

        # Database roles
        self.database_roles = {
            DatabaseRole.VECTOR: {
                "primary_data": ["content_embeddings", "chunk_vectors"],
                "use_cases": [
                    "semantic_search",
                    "similarity_matching",
                    "content_discovery",
                ],
                "storage_format": "embeddings",
                "optimization": "high_dimensional_search",
            },
            DatabaseRole.GRAPH: {
                "primary_data": ["relationships", "hierarchies", "networks"],
                "use_cases": ["traversal_queries", "path_finding", "network_analysis"],
                "storage_format": "nodes_and_edges",
                "optimization": "relationship_traversal",
            },
            DatabaseRole.RELATIONAL: {
                "primary_data": ["metadata", "keywords", "statistics"],
                "use_cases": ["fast_filtering", "aggregations", "keyword_search"],
                "storage_format": "structured_tables",
                "optimization": "indexed_queries",
            },
            DatabaseRole.FILE: {
                "primary_data": [
                    "original_files",
                    "binary_assets",
                    "supplemental_material",
                ],
                "use_cases": [
                    "archival_storage",
                    "evidence_persistence",
                    "reprocessing_source",
                ],
                "storage_format": "filesystem_or_object_store",
                "optimization": "hierarchical_storage_tier",
            },
        }

        # Schemas and strategies
        self.vector_schema = self._create_vector_schema()  # From UDS3DatabaseSchemasMixin
        self.graph_schema = self._create_graph_schema()  # From UDS3DatabaseSchemasMixin
        self.relational_schema = self._create_relational_schema()  # From UDS3DatabaseSchemasMixin
        self.file_storage_schema = self._create_file_storage_schema()  # From UDS3DatabaseSchemasMixin

        self.sync_rules = self._create_sync_rules()
        self.crud_strategies = self._create_crud_strategies()  # From UDS3CRUDStrategiesMixin
        self.conflict_resolution = self._create_conflict_resolution_rules()  # From UDS3CRUDStrategiesMixin

    def _resolve_database_manager(self):
        if self._database_manager is not None:
            return self._database_manager
        try:
            from uds3.database import database_api  # type: ignore

            self._database_manager = database_api.get_database_manager()
            
            # Initialize DSGVO Core now that database manager is available
            self._initialize_dsgvo_core()
            
        except (
            Exception
        ) as exc:  # pragma: no cover - Fallback falls globale Factory scheitert
            logger.debug("Falle auf lokalen DatabaseManager zurück: %s", exc)
            try:
                from uds3.database.database_manager import DatabaseManager  # type: ignore
            except Exception:

                class DatabaseManager:  # type: ignore
                    def __init__(self, cfg=None):
                        self._cfg = cfg or {}

                    def get_adapter_governance(self):
                        return None

            try:
                # Versuche zentrale neue Config im Paket uds3
                from . import config as _config

                def _get_database_backend_dict():
                    # Erwartetes Format: { 'postgis': {...}, 'vector': {...}, ... }
                    return {k: v for k, v in _config.DATABASES.items()}

                config_like = type(
                    "C",
                    (),
                    {
                        "get_database_backend_dict": staticmethod(
                            _get_database_backend_dict
                        )
                    },
                )
                cfg_obj = config_like()
            except Exception:
                # Fallback: alte Erwartungen weiter unterstützen
                try:
                    # Legacy fallback: try to import a top-level `config` module if present
                    # Prefer the package-local `uds3.config` where possible
                    from config import config  # type: ignore

                    cfg_obj = config
                except Exception:
                    try:
                        from . import config as _config

                        class _LocalConfigLike:
                            @staticmethod
                            def get_database_backend_dict():
                                try:
                                    return {k: v for k, v in _config.DATABASES.items()}
                                except Exception:
                                    return {}

                        cfg_obj = _LocalConfigLike()
                    except Exception:

                        class _ConfigStub:  # type: ignore
                            def get_database_backend_dict(self):
                                return {}

                        cfg_obj = _ConfigStub()

            self._database_manager = DatabaseManager(
                cfg_obj.get_database_backend_dict()
            )
        return self._database_manager
    
    def _initialize_dsgvo_core(self):
        """Initialize DSGVO Core after database manager is available"""
        if self.dsgvo_core is not None:
            return  # Already initialized
            
        try:
            UDS3DSGVOCore = get_dsgvo_class('UDS3DSGVOCore')
            if UDS3DSGVOCore:
                # Initialize mit Database Manager (nutzt UDS3 database_api)
                self.dsgvo_core = UDS3DSGVOCore(
                    database_manager=self._database_manager,  # Nutze UDS3 Database Manager
                    retention_years=7,
                    auto_anonymize=True,
                    strict_mode=True
                )
                logger.info("✅ UDS3 DSGVO Core integriert - Zentrale DSGVO-Compliance aktiv")
            else:
                logger.info("⚠️ UDS3 DSGVO Core nicht verfügbar - überspringe DSGVO-Integration")
        except Exception as exc:
            self.dsgvo_core = None
            logger.warning(
                f"⚠️ UDS3 DSGVO Core konnte nicht initialisiert werden: {exc}"
            )

    @property
    def search_api(self):
        """
        Lazy-loaded Search API property for unified search operations.
        
        Provides high-level search interface across Vector, Graph and Relational backends.
        
        Returns:
            UDS3SearchAPI: Search API instance
        
        Usage:
            >>> strategy = get_optimized_unified_strategy()
            >>> results = await strategy.search_api.hybrid_search(query)
        
        Features:
            - Vector Search (ChromaDB) - Semantic similarity
            - Graph Search (Neo4j) - Text + Relationships
            - Keyword Search (PostgreSQL) - Full-text search
            - Hybrid Search - Weighted combination
        
        See Also:
            - uds3.search.UDS3SearchAPI: Search API documentation
            - docs/UDS3_SEARCH_API_PRODUCTION_GUIDE.md: Production guide
        """
        if self._search_api is None:
            try:
                from search.search_api import UDS3SearchAPI
                self._search_api = UDS3SearchAPI(self)
                logger.info("✅ UDS3 Search API lazy-loaded")
            except Exception as exc:
                logger.error(f"❌ Failed to load UDS3 Search API: {exc}")
                raise ImportError(
                    f"Could not initialize UDS3 Search API. "
                    f"Make sure 'uds3.search' module is installed. "
                    f"Error: {exc}"
                ) from exc
        return self._search_api

    def _get_adapter_governance(self):
        if self._adapter_governance is not None:
            return self._adapter_governance
        manager = self._resolve_database_manager()
        getter = getattr(manager, "get_adapter_governance", None)
        if callable(getter):
            self._adapter_governance = getter()
        return self._adapter_governance

    def _enforce_adapter_governance(
        self, backend_key: str, operation: str, payload: Any
    ) -> None:
        if not self.enforce_governance:
            return
        governance = self._get_adapter_governance()
        if not governance:
            return
        try:
            governance.ensure_operation_allowed(backend_key, operation)
            governance.enforce_payload(backend_key, operation, payload)
        except AdapterGovernanceError as exc:
            raise SagaExecutionError(str(exc))

    # ================================================================
    # SCHEMA DEFINITIONS - MOVED TO uds3_database_schemas.py (UDS3DatabaseSchemasMixin)
    # ================================================================
    # _create_vector_schema() → UDS3DatabaseSchemasMixin
    # _create_graph_schema() → UDS3DatabaseSchemasMixin
    # _create_relational_schema() → UDS3DatabaseSchemasMixin
    # _create_file_storage_schema() → UDS3DatabaseSchemasMixin
    #
    # EXTRACTION: Lines 512-841 + 4156-4170 (~350 LOC)
    # ================================================================

    def _create_sync_rules(self) -> Dict:
        """Definiert Synchronisationsregeln zwischen den Datenbanken"""
        return {
            "document_creation": {
                "trigger": "new_document",
                "sequence": [
                    {
                        "target": "relational",
                        "operation": "insert_metadata",
                        "data": "document_properties",
                    },
                    {
                        "target": "graph",
                        "operation": "create_document_node",
                        "data": "essential_properties",
                    },
                    {
                        "target": "vector",
                        "operation": "create_embeddings",
                        "data": "content_chunks",
                    },
                ],
            },
            "relationship_update": {
                "trigger": "new_relationship",
                "sequence": [
                    {
                        "target": "graph",
                        "operation": "create_relationship",
                        "data": "relationship_data",
                    },
                    {
                        "target": "relational",
                        "operation": "update_statistics",
                        "data": "relationship_count",
                    },
                ],
            },
            "content_update": {
                "trigger": "content_change",
                "sequence": [
                    {
                        "target": "vector",
                        "operation": "update_embeddings",
                        "data": "new_content",
                    },
                    {
                        "target": "relational",
                        "operation": "update_keywords",
                        "data": "extracted_keywords",
                    },
                    {
                        "target": "relational",
                        "operation": "update_timestamp",
                        "data": "updated_at",
                    },
                ],
            },
        }

    def create_optimized_processing_plan(
        self, file_path: str, content: str, chunks: List[str], **metadata
    ) -> Dict:
        """
        Erstellt einen optimierten Verarbeitungsplan für alle drei DB-Typen

        Args:
            file_path: Pfad zur Datei
            content: Volltext des Dokuments
            chunks: Liste der Text-Chunks
            **metadata: Zusätzliche Metadaten

        Returns:
            Dict: Optimierter Processing-Plan mit DB-spezifischen Operationen
        """
        document_id = self._generate_document_id(file_path, content[:1000])

        plan = {
            "strategy_version": "3.0_optimized",
            "document_id": document_id,
            "created_at": datetime.now().isoformat(),
            "databases": {
                "vector": self._create_vector_operations(document_id, chunks, metadata),
                "graph": self._create_graph_operations(document_id, content, metadata),
                "relational": self._create_relational_operations(
                    document_id, file_path, content, metadata
                ),
                "file_storage": self._create_file_storage_operations(
                    document_id, file_path, content, metadata
                ),
            },
            "sync_plan": self._create_sync_plan(document_id),
            "optimization_hints": self._create_optimization_hints(chunks, metadata),
        }

        return plan

    # ========== CRUD OPERATIONS ==========

    def create_secure_document(
        self,
        file_path: str,
        content: str,
        chunks: List[str],
        security_level: "SecurityLevel" = None,
        **metadata,
    ) -> Dict:
        """
        Erstellt ein neues Dokument mit erweiterten Security & Quality Features

        Args:
            file_path: Pfad zur Quelldatei
            content: Dokumenteninhalt
            chunks: Text-Chunks für Vektorisierung
            security_level: Sicherheitsstufe für das Dokument
            **metadata: Zusätzliche Metadaten

        Returns:
            Dict: Vollständiges Ergebnis inkl. Security- und Quality-Informationen
        """
        create_result = {
            "operation_type": "CREATE_SECURE_DOCUMENT",
            "timestamp": datetime.now().isoformat(),
            "file_path": file_path,
            "success": False,
            "security_info": {},
            "quality_score": {},
            "database_operations": {},
            "validation_results": {},
            "issues": [],
        }

        metadata_copy = dict(metadata)
        context = {
            "file_path": file_path,
            "content": content,
            "chunks": chunks,
            "metadata": metadata_copy,
            "security_level": security_level,
            "create_result": create_result,
        }
        if metadata_copy.get("aktenzeichen"):
            context["identity_key"] = metadata_copy["aktenzeichen"]
            context["aktenzeichen"] = metadata_copy["aktenzeichen"]

        step_specs = self._build_create_document_step_specs(context)
        saga_status = None

        try:
            if self.saga_orchestrator and SagaDefinition and SagaStep:
                saga_definition = SagaDefinition(
                    name="create_secure_document",
                    steps=[
                        SagaStep(
                            name=spec["name"],
                            action=spec["action"],
                            compensation=spec["compensation"],
                        )
                        for spec in step_specs
                    ],
                )
                saga_execution = self.saga_orchestrator.execute(
                    saga_definition, context
                )
                context = saga_execution.context
                saga_status = saga_execution.status
                create_result["saga"] = {
                    "saga_id": saga_execution.saga_id,
                    "status": saga_execution.status.value,
                    "errors": saga_execution.errors,
                    "compensation_errors": saga_execution.compensation_errors,
                }
                create_result["issues"].extend(saga_execution.errors)
                create_result["issues"].extend(saga_execution.compensation_errors)
            else:
                local_result = self._execute_saga_steps_locally(step_specs, context)
                context = local_result["context"]
                saga_status = local_result["status"]
                if local_result["errors"] or local_result["compensation_errors"]:
                    create_result.setdefault("saga", {})
                    create_result["saga"].update(
                        {
                            "status": str(getattr(saga_status, "value", saga_status)),
                            "errors": local_result["errors"],
                            "compensation_errors": local_result["compensation_errors"],
                        }
                    )
                    create_result["issues"].extend(local_result["errors"])
                    create_result["issues"].extend(local_result["compensation_errors"])

            # Erfolg wurde in den Saga-Schritten bestimmt (Validation-Step)
            if create_result["success"] and saga_status is not None:
                create_result.setdefault("saga", {})
                create_result["saga"].setdefault(
                    "status", getattr(saga_status, "value", str(saga_status))
                )

        except Exception as exc:
            create_result["success"] = False
            create_result["error"] = str(exc)
            logger.error(f"Secure document creation failed: {exc}")

        return create_result

    def update_secure_document(
        self,
        document_id: str,
        updates: Dict[str, Any],
        *,
        sync_strategy: SyncStrategy = SyncStrategy.IMMEDIATE,
    ) -> Dict[str, Any]:
        """Aktualisiert ein bestehendes Dokument über alle Backends via Saga."""

        update_result: Dict[str, Any] = {
            "operation_type": "UPDATE_SECURE_DOCUMENT",
            "document_id": document_id,
            "timestamp": datetime.now().isoformat(),
            "sync_strategy": sync_strategy.value,
            "updates": dict(updates or {}),
            "success": False,
            "database_operations": {},
            "validation_results": {},
            "issues": [],
        }

        context = {
            "document_id": document_id,
            "updates": dict(updates or {}),
            "sync_strategy": sync_strategy,
            "update_result": update_result,
        }
        updates_copy = context["updates"]
        if updates_copy.get("aktenzeichen"):
            context["identity_key"] = updates_copy["aktenzeichen"]
            context["aktenzeichen"] = updates_copy["aktenzeichen"]
        mapping = self.document_mapping.get(document_id)
        if mapping:
            if mapping.get("identity_key"):
                context.setdefault("identity_key", mapping["identity_key"])
                context.setdefault("aktenzeichen", mapping["identity_key"])
            if mapping.get("uuid"):
                context.setdefault("identity_uuid", mapping["uuid"])

        step_specs = self._build_update_document_step_specs(context)
        saga_status = None

        try:
            if self.saga_orchestrator and SagaDefinition and SagaStep:
                saga_definition = SagaDefinition(
                    name="update_secure_document",
                    steps=[
                        SagaStep(
                            name=spec["name"],
                            action=spec["action"],
                            compensation=spec["compensation"],
                        )
                        for spec in step_specs
                    ],
                )
                saga_execution = self.saga_orchestrator.execute(
                    saga_definition, context
                )
                context = saga_execution.context
                saga_status = saga_execution.status
                update_result["saga"] = {
                    "saga_id": saga_execution.saga_id,
                    "status": saga_execution.status.value,
                    "errors": saga_execution.errors,
                    "compensation_errors": saga_execution.compensation_errors,
                }
                update_result["issues"].extend(saga_execution.errors)
                update_result["issues"].extend(saga_execution.compensation_errors)
            else:
                local_result = self._execute_saga_steps_locally(step_specs, context)
                context = local_result["context"]
                saga_status = local_result["status"]
                if local_result["errors"] or local_result["compensation_errors"]:
                    update_result.setdefault("saga", {})
                    update_result["saga"].update(
                        {
                            "status": str(getattr(saga_status, "value", saga_status)),
                            "errors": local_result["errors"],
                            "compensation_errors": local_result["compensation_errors"],
                        }
                    )
                    update_result["issues"].extend(local_result["errors"])
                    update_result["issues"].extend(local_result["compensation_errors"])

            if update_result["success"] and saga_status is not None:
                update_result.setdefault("saga", {})
                update_result["saga"].setdefault(
                    "status", getattr(saga_status, "value", str(saga_status))
                )

        except Exception as exc:  # pragma: no cover - Schutz gegen unerwartete Fehler
            update_result["success"] = False
            update_result["error"] = str(exc)
            logger.error("Secure document update failed: %s", exc, exc_info=True)

        return update_result

    def delete_secure_document(
        self,
        document_id: str,
        *,
        strategy: Optional[str] = "soft",
        reason: Optional[str] = None,
        deleted_by: Optional[str] = None,
        cascade: Optional[str] = "selective",
    ) -> Dict[str, Any]:
        """
        Löscht ein Dokument mit verschiedenen Strategien.
        
        Args:
            document_id: ID des zu löschenden Dokuments
            strategy: "soft" (default), "hard", oder "archive"
            reason: Optionaler Grund für die Löschung
            deleted_by: Optionaler Benutzer-/System-Identifier
            cascade: Cascade-Strategie ("none", "selective", "full")
        
        Returns:
            Dict mit Löschergebnis
        """
        logger.info(f"Deleting document {document_id} with strategy={strategy}")
        
        # Use new SoftDeleteManager if available
        if strategy == "soft" and self.soft_delete_manager:
            result = self.soft_delete_manager.soft_delete_document(
                document_id=document_id,
                reason=reason,
                deleted_by=deleted_by
            )
            return result.to_dict()
        
        # Use new HardDeleteManager if available
        elif strategy == "hard" and self.hard_delete_manager:
            cascade_strategy = CascadeStrategy[cascade.upper()] if cascade else CascadeStrategy.SELECTIVE
            result = self.hard_delete_manager.hard_delete_document(
                document_id=document_id,
                cascade=cascade_strategy,
                reason=reason,
                deleted_by=deleted_by
            )
            return result.to_dict()
        
        # Legacy implementation (fallback)
        soft_delete = (strategy == "soft" or strategy == "archive")
        cascade_delete = (cascade == "full" or cascade == "selective")

        delete_result: Dict[str, Any] = {
            "operation_type": "ARCHIVE_SECURE_DOCUMENT"
            if soft_delete
            else "DELETE_SECURE_DOCUMENT",
            "document_id": document_id,
            "timestamp": datetime.now().isoformat(),
            "soft_delete": soft_delete,
            "cascade_delete": cascade_delete,
            "success": False,
            "database_operations": {},
            "issues": [],
        }

        context = {
            "document_id": document_id,
            "soft_delete": soft_delete,
            "cascade_delete": cascade_delete,
            "delete_result": delete_result,
        }
        mapping = self.document_mapping.get(document_id)
        if mapping:
            if mapping.get("identity_key"):
                context.setdefault("identity_key", mapping["identity_key"])
                context.setdefault("aktenzeichen", mapping["identity_key"])
            if mapping.get("uuid"):
                context.setdefault("identity_uuid", mapping["uuid"])

        step_specs = self._build_delete_document_step_specs(context)
        saga_status = None

        try:
            if self.saga_orchestrator and SagaDefinition and SagaStep:
                saga_definition = SagaDefinition(
                    name="delete_secure_document",
                    steps=[
                        SagaStep(
                            name=spec["name"],
                            action=spec["action"],
                            compensation=spec["compensation"],
                        )
                        for spec in step_specs
                    ],
                )
                saga_execution = self.saga_orchestrator.execute(
                    saga_definition, context
                )
                context = saga_execution.context
                saga_status = saga_execution.status
                delete_result["saga"] = {
                    "saga_id": saga_execution.saga_id,
                    "status": saga_execution.status.value,
                    "errors": saga_execution.errors,
                    "compensation_errors": saga_execution.compensation_errors,
                }
                delete_result["issues"].extend(saga_execution.errors)
                delete_result["issues"].extend(saga_execution.compensation_errors)
            else:
                local_result = self._execute_saga_steps_locally(step_specs, context)
                context = local_result["context"]
                saga_status = local_result["status"]
                if local_result["errors"] or local_result["compensation_errors"]:
                    delete_result.setdefault("saga", {})
                    delete_result["saga"].update(
                        {
                            "status": str(getattr(saga_status, "value", saga_status)),
                            "errors": local_result["errors"],
                            "compensation_errors": local_result["compensation_errors"],
                        }
                    )
                    delete_result["issues"].extend(local_result["errors"])
                    delete_result["issues"].extend(local_result["compensation_errors"])

            if delete_result["success"] and saga_status is not None:
                delete_result.setdefault("saga", {})
                delete_result["saga"].setdefault(
                    "status", getattr(saga_status, "value", str(saga_status))
                )

        except Exception as exc:  # pragma: no cover - Schutz gegen unerwartete Fehler
            delete_result["success"] = False
            delete_result["error"] = str(exc)
            logger.error("Secure document delete failed: %s", exc, exc_info=True)

        return delete_result

    def restore_document(
        self,
        document_id: str,
        *,
        strategy: str = "preserve_audit",
    ) -> Dict[str, Any]:
        """
        Stellt ein soft-deleted Dokument wieder her.
        
        Args:
            document_id: ID des wiederherzustellenden Dokuments
            strategy: "keep_metadata", "clear_metadata", oder "preserve_audit" (default)
        
        Returns:
            Dict mit Restore-Ergebnis
        """
        logger.info(f"Restoring document {document_id} with strategy={strategy}")
        
        if not self.soft_delete_manager:
            return {
                "success": False,
                "document_id": document_id,
                "error": "SoftDeleteManager not available"
            }
        
        try:
            restore_strategy = RestoreStrategy[strategy.upper()]
        except KeyError:
            restore_strategy = RestoreStrategy.PRESERVE_AUDIT
        
        result = self.soft_delete_manager.restore_document(
            document_id=document_id,
            strategy=restore_strategy
        )
        return result.to_dict()

    def list_deleted_documents(
        self,
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 100,
        offset: int = 0
    ) -> List[Dict]:
        """
        Listet soft-deleted Dokumente auf.
        
        Args:
            filters: Optionale Filter (z.B. deleted_after, deleted_by)
            limit: Maximale Anzahl Ergebnisse
            offset: Pagination Offset
        
        Returns:
            Liste von gelöschten Dokument-Metadaten
        """
        logger.info("Listing soft-deleted documents")
        
        if not self.soft_delete_manager:
            logger.warning("SoftDeleteManager not available")
            return []
        
        return self.soft_delete_manager.list_deleted(
            filters=filters,
            limit=limit,
            offset=offset
        )

    def purge_old_deleted_documents(
        self,
        retention_days: int = 90,
        hard_delete: bool = True
    ) -> Dict[str, Any]:
        """
        Löscht alte soft-deleted Dokumente permanent.
        
        Args:
            retention_days: Lösche Dokumente älter als N Tage
            hard_delete: Wenn True, hard delete; wenn False, archivieren
        
        Returns:
            Dict mit Purge-Statistiken
        """
        logger.info(f"Purging deleted documents older than {retention_days} days")
        
        if not self.soft_delete_manager:
            return {
                "success": False,
                "error": "SoftDeleteManager not available"
            }
        
        result = self.soft_delete_manager.purge_old_deleted(
            retention_days=retention_days,
            hard_delete=hard_delete
        )
        return result.to_dict()
    
    # ========================================================================
    # ADVANCED CRUD OPERATIONS (Batch Read, Conditional Update, Upsert)
    # ========================================================================
    
    def batch_read_documents(
        self,
        document_ids: List[str],
        *,
        strategy: str = "parallel",
        max_workers: int = 10,
        include_content: bool = True,
        include_relationships: bool = False,
        timeout: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Liest mehrere Dokumente parallel
        
        Args:
            document_ids: Liste der Dokument-IDs
            strategy: Read-Strategie ("parallel", "sequential", "priority")
            max_workers: Maximale Anzahl paralleler Worker
            include_content: Vollständigen Content einschließen
            include_relationships: Graph-Relationships einschließen
            timeout: Timeout in Sekunden pro Dokument
        
        Returns:
            Dict mit BatchReadResult
        """
        logger.info(f"Batch reading {len(document_ids)} documents")
        
        if not self.advanced_crud_manager:
            return {
                "success": False,
                "error": "AdvancedCRUDManager not available"
            }
        
        # Map strategy string to ReadStrategy enum
        strategy_map = {
            "parallel": ReadStrategy.PARALLEL,
            "sequential": ReadStrategy.SEQUENTIAL,
            "priority": ReadStrategy.PRIORITY
        }
        read_strategy = strategy_map.get(strategy, ReadStrategy.PARALLEL)
        
        result = self.advanced_crud_manager.batch_read_documents(
            document_ids,
            strategy=read_strategy,
            max_workers=max_workers,
            include_content=include_content,
            include_relationships=include_relationships,
            timeout=timeout
        )
        return result.to_dict()
    
    def conditional_update_document(
        self,
        document_id: str,
        updates: Dict[str, Any],
        conditions: List[Dict[str, Any]],
        *,
        version_check: Optional[int] = None,
        atomic: bool = True
    ) -> Dict[str, Any]:
        """
        Aktualisiert ein Dokument nur wenn Bedingungen erfüllt sind
        
        Args:
            document_id: Dokument-ID
            updates: Dictionary mit Updates
            conditions: Liste von Conditions (dict mit field, operator, value)
            version_check: Erwartete Version (für optimistic locking)
            atomic: Atomic execution (rollback bei Fehler)
        
        Returns:
            Dict mit ConditionalUpdateResult
        """
        logger.info(f"Conditional update for document {document_id}")
        
        if not self.advanced_crud_manager:
            return {
                "success": False,
                "error": "AdvancedCRUDManager not available"
            }
        
        # Convert dict conditions to Condition objects
        condition_objs = []
        for cond in conditions:
            # Map operator string to ConditionOperator enum
            operator_map = {
                "==": ConditionOperator.EQ,
                "!=": ConditionOperator.NE,
                ">": ConditionOperator.GT,
                "<": ConditionOperator.LT,
                ">=": ConditionOperator.GTE,
                "<=": ConditionOperator.LTE,
                "exists": ConditionOperator.EXISTS,
                "not_exists": ConditionOperator.NOT_EXISTS
            }
            operator = operator_map.get(cond["operator"], ConditionOperator.EQ)
            
            condition_objs.append(Condition(
                field=cond["field"],
                operator=operator,
                value=cond.get("value")
            ))
        
        result = self.advanced_crud_manager.conditional_update_document(
            document_id,
            updates,
            condition_objs,
            version_check=version_check,
            atomic=atomic
        )
        return result.to_dict()
    
    def upsert_document(
        self,
        document_id: str,
        document_data: Dict[str, Any],
        *,
        merge_strategy: str = "merge",
        create_if_missing: bool = True
    ) -> Dict[str, Any]:
        """
        Upsert: Update or Insert Document
        
        Args:
            document_id: Dokument-ID
            document_data: Dokument-Daten
            merge_strategy: Strategie für Merge ("replace", "merge", "keep_existing")
            create_if_missing: Dokument erstellen wenn nicht vorhanden
        
        Returns:
            Dict mit UpsertResult
        """
        logger.info(f"Upserting document {document_id}")
        
        if not self.advanced_crud_manager:
            return {
                "success": False,
                "error": "AdvancedCRUDManager not available"
            }
        
        # Map strategy string to MergeStrategy enum
        strategy_map = {
            "replace": MergeStrategy.REPLACE,
            "merge": MergeStrategy.MERGE,
            "keep_existing": MergeStrategy.KEEP_EXISTING
        }
        merge_strat = strategy_map.get(merge_strategy, MergeStrategy.MERGE)
        
        result = self.advanced_crud_manager.upsert_document(
            document_id,
            document_data,
            merge_strategy=merge_strat,
            create_if_missing=create_if_missing
        )
        return result.to_dict()
    
    def batch_update_documents(
        self,
        updates: Dict[str, Dict[str, Any]],
        *,
        max_workers: int = 10,
        continue_on_error: bool = True
    ) -> Dict[str, Dict[str, Any]]:
        """
        Aktualisiert mehrere Dokumente parallel
        
        Args:
            updates: Dict mit document_id -> updates
            max_workers: Maximale Anzahl paralleler Worker
            continue_on_error: Weitermachen bei Fehlern
        
        Returns:
            Dict mit document_id -> ConditionalUpdateResult
        """
        logger.info(f"Batch updating {len(updates)} documents")
        
        if not self.advanced_crud_manager:
            return {
                "error": "AdvancedCRUDManager not available"
            }
        
        results = self.advanced_crud_manager.batch_update_documents(
            updates,
            max_workers=max_workers,
            continue_on_error=continue_on_error
        )
        
        # Convert results to dict format
        return {
            doc_id: result.to_dict()
            for doc_id, result in results.items()
        }
    
    # ========================================================================
    # VECTOR FILTER OPERATIONS
    # ========================================================================
    
    def create_vector_filter(
        self,
        collection_name: str = "default"
    ) -> 'VectorFilter':
        """
        Erstellt einen VectorFilter für Vector DB Queries
        
        Args:
            collection_name: Name der Vector Collection
        
        Returns:
            VectorFilter instance
        
        Example:
            ```python
            filter = core.create_vector_filter("documents")
            results = (filter
                       .by_similarity(embedding, threshold=0.8)
                       .by_metadata("status", "==", "active")
                       .execute())
            ```
        """
        logger.info(f"Creating vector filter for collection: {collection_name}")
        
        if not VECTOR_FILTER_AVAILABLE:
            logger.error("VectorFilter not available")
            raise ImportError("VectorFilter module not available")
        
        # Get vector backend
        vector_backend = None
        if hasattr(self, 'vector_db') and self.vector_db:
            vector_backend = self.vector_db
        elif hasattr(self, 'chroma_client'):
            vector_backend = self.chroma_client
        
        return create_vector_filter(
            backend=vector_backend,
            collection_name=collection_name
        )
    
    def query_vector_similarity(
        self,
        query_embedding: List[float],
        collection_name: str = "default",
        threshold: float = 0.7,
        top_k: int = 10,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Convenience method für Vector Similarity Search
        
        Args:
            query_embedding: Query embedding vector
            collection_name: Collection name
            threshold: Minimum similarity threshold
            top_k: Number of results
            metadata_filters: Optional metadata filters (dict with field->value)
        
        Returns:
            Dict mit query results
        
        Example:
            ```python
            results = core.query_vector_similarity(
                embedding,
                collection_name="documents",
                threshold=0.8,
                top_k=5,
                metadata_filters={"status": "active"}
            )
            ```
        """
        logger.info(f"Vector similarity query: collection={collection_name}, top_k={top_k}")
        
        if not VECTOR_FILTER_AVAILABLE:
            return {
                "success": False,
                "error": "VectorFilter not available"
            }
        
        try:
            # Create filter
            filter = self.create_vector_filter(collection_name)
            
            # Add similarity
            filter.by_similarity(
                query_embedding=query_embedding,
                threshold=threshold,
                top_k=top_k
            )
            
            # Add metadata filters if provided
            if metadata_filters:
                for field, value in metadata_filters.items():
                    filter.by_metadata(field, FilterOperator.EQ, value)
            
            # Execute
            result = filter.execute()
            return result.to_dict()
        
        except Exception as e:
            logger.error(f"Vector similarity query failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    # GRAPH FILTER OPERATIONS
    
    def create_graph_filter(
        self,
        start_node_label: Optional[str] = None
    ) -> 'GraphFilter':
        """
        Create a GraphFilter instance for Neo4j/Cypher queries.
        
        Args:
            start_node_label: Optional label for starting nodes (e.g., "Document")
        
        Returns:
            GraphFilter instance
        
        Example:
            ```python
            filter = core.create_graph_filter("Document")
            results = (filter
                       .by_property("status", "==", "active")
                       .by_relationship("REFERENCES", "OUTGOING")
                       .by_node_type("Document")
                       .execute())
            ```
        """
        logger.info(f"Creating graph filter with start node: {start_node_label}")
        
        if not GRAPH_FILTER_AVAILABLE:
            logger.error("GraphFilter not available")
            raise ImportError("GraphFilter module not available")
        
        # Get graph backend
        graph_backend = None
        if hasattr(self, 'graph_db') and self.graph_db:
            graph_backend = self.graph_db
        elif hasattr(self, 'neo4j_driver'):
            graph_backend = self.neo4j_driver
        
        return create_graph_filter(
            backend=graph_backend,
            start_node_label=start_node_label
        )
    
    def query_graph_pattern(
        self,
        node_label: str,
        properties: Optional[Dict[str, Any]] = None,
        relationship_type: Optional[str] = None,
        relationship_direction: str = "OUTGOING",
        target_node_label: Optional[str] = None,
        max_depth: int = 1,
        limit: int = 100
    ) -> Dict[str, Any]:
        """
        Convenience method for common graph pattern queries.
        
        Args:
            node_label: Starting node label (e.g., "Document")
            properties: Optional node properties to filter
            relationship_type: Optional relationship type (e.g., "REFERENCES")
            relationship_direction: Relationship direction (OUTGOING, INCOMING, BOTH)
            target_node_label: Optional target node label
            max_depth: Maximum traversal depth
            limit: Maximum number of results
        
        Returns:
            Dict with query results
        
        Example:
            ```python
            # Find all Documents that reference other Documents
            result = core.query_graph_pattern(
                node_label="Document",
                properties={"status": "active"},
                relationship_type="REFERENCES",
                relationship_direction="OUTGOING",
                target_node_label="Document",
                max_depth=2,
                limit=50
            )
            ```
        """
        try:
            # Create filter
            filter = self.create_graph_filter(node_label)
            
            # Add property filters
            if properties:
                for field, value in properties.items():
                    filter.by_property(field, FilterOperator.EQ, value)
            
            # Add relationship if specified
            if relationship_type:
                filter.by_relationship(relationship_type, relationship_direction)
                filter.with_depth(1, max_depth)
                
                # Add target node if specified
                if target_node_label:
                    filter.by_node_type(target_node_label)
            
            # Set limit
            filter.limit(limit)
            
            # Execute
            result = filter.execute()
            return result.to_dict()
        
        except Exception as e:
            logger.error(f"Graph pattern query failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    # RELATIONAL FILTER OPERATIONS
    
    def create_relational_filter(
        self,
        dialect: str = "sqlite"
    ) -> 'RelationalFilter':
        """
        Create a RelationalFilter instance for SQL queries.
        
        Args:
            dialect: SQL dialect ("sqlite", "postgresql", "mysql")
        
        Returns:
            RelationalFilter instance
        
        Example:
            ```python
            filter = core.create_relational_filter("sqlite")
            results = (filter
                       .select("name", "email")
                       .from_table("users")
                       .where("active", FilterOperator.EQ, True)
                       .limit(10)
                       .execute())
            ```
        """
        logger.info(f"Creating relational filter with dialect: {dialect}")
        
        if not RELATIONAL_FILTER_AVAILABLE:
            logger.error("RelationalFilter not available")
            raise ImportError("RelationalFilter module not available")
        
        # Map dialect string to SQLDialect enum
        dialect_map = {
            "sqlite": SQLDialect.SQLITE,
            "postgresql": SQLDialect.POSTGRESQL,
            "postgres": SQLDialect.POSTGRESQL,
            "mysql": SQLDialect.MYSQL,
        }
        
        sql_dialect = dialect_map.get(dialect.lower(), SQLDialect.SQLITE)
        
        # Get relational backend
        relational_backend = None
        if hasattr(self, 'db_conn') and self.db_conn:
            relational_backend = self.db_conn
        elif hasattr(self, 'sqlite_conn'):
            relational_backend = self.sqlite_conn
        elif hasattr(self, 'pg_conn'):
            relational_backend = self.pg_conn
        
        return create_relational_filter(
            backend=relational_backend,
            dialect=sql_dialect
        )
    
    def query_sql(
        self,
        table: str,
        select_fields: Optional[List[str]] = None,
        where_conditions: Optional[Dict[str, Any]] = None,
        joins: Optional[List[Dict[str, str]]] = None,
        order_by: Optional[str] = None,
        order_direction: str = "ASC",
        limit: int = 100,
        offset: int = 0
    ) -> Dict[str, Any]:
        """
        Convenience method for SQL queries.
        
        Args:
            table: Table name
            select_fields: Fields to select (default: all)
            where_conditions: WHERE conditions as dict {field: value}
            joins: JOIN clauses as list of dicts
                   [{"table": "orders", "on_left": "users.id", "on_right": "orders.user_id"}]
            order_by: Field to order by
            order_direction: Order direction ("ASC" or "DESC")
            limit: Maximum results
            offset: Result offset
        
        Returns:
            Dict with query results
        
        Example:
            ```python
            # Simple query
            result = core.query_sql(
                table="documents",
                where_conditions={"status": "active"},
                order_by="created_at",
                order_direction="DESC",
                limit=50
            )
            
            # Query with JOIN
            result = core.query_sql(
                table="documents",
                select_fields=["documents.title", "users.name"],
                joins=[{
                    "table": "users",
                    "on_left": "documents.user_id",
                    "on_right": "users.id"
                }],
                where_conditions={"documents.status": "active"},
                limit=20
            )
            ```
        """
        try:
            # Create filter
            filter = self.create_relational_filter()
            
            # SELECT clause
            if select_fields:
                filter.select(*select_fields)
            else:
                filter.select("*")
            
            # FROM clause
            filter.from_table(table)
            
            # JOIN clauses
            if joins:
                for join in joins:
                    filter.left_join(
                        table=join["table"],
                        on_left=join["on_left"],
                        on_right=join["on_right"]
                    )
            
            # WHERE clause
            if where_conditions:
                first = True
                for field, value in where_conditions.items():
                    if first:
                        filter.where(field, FilterOperator.EQ, value)
                        first = False
                    else:
                        filter.and_where(field, FilterOperator.EQ, value)
            
            # ORDER BY
            if order_by:
                order = SortOrder.ASC if order_direction.upper() == "ASC" else SortOrder.DESC
                filter.order_by(order_by, order)
            
            # LIMIT and OFFSET
            filter.limit(limit)
            if offset > 0:
                filter.offset(offset)
            
            # Execute
            result = filter.execute()
            return result.to_dict()
        
        except Exception as e:
            logger.error(f"SQL query failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    # ==================================================================
    # Saga Compliance & Governance Methods
    # ==================================================================
    
    def create_compliance_engine(self) -> Optional["SagaComplianceEngine"]:
        """
        Create Saga Compliance Engine.
        
        Returns:
            SagaComplianceEngine instance if available, None otherwise
        
        Example:
            ```python
            compliance = core.create_compliance_engine()
            report = compliance.check_saga_compliance("saga-123")
            print(f"Status: {report.compliance_status.value}")
            ```
        """
        if not SAGA_COMPLIANCE_AVAILABLE:
            logger.warning("Saga Compliance module not available")
            return None
        
        return create_compliance_engine(orchestrator=self.saga_orchestrator)
    
    def create_monitoring_interface(self) -> Optional["SagaMonitoringInterface"]:
        """
        Create Saga Monitoring Interface.
        
        Returns:
            SagaMonitoringInterface instance if available, None otherwise
        
        Example:
            ```python
            monitoring = core.create_monitoring_interface()
            health = monitoring.get_saga_health("saga-123")
            print(f"Health Score: {health.health_score}")
            ```
        """
        if not SAGA_COMPLIANCE_AVAILABLE:
            logger.warning("Saga Compliance module not available")
            return None
        
        return create_monitoring_interface(orchestrator=self.saga_orchestrator)
    
    def create_admin_interface(self) -> Optional["SagaAdminInterface"]:
        """
        Create Saga Admin Interface.
        
        Returns:
            SagaAdminInterface instance if available, None otherwise
        
        Example:
            ```python
            admin = core.create_admin_interface()
            admin.pause_saga("saga-123", "Maintenance window", "admin-user")
            ```
        """
        if not SAGA_COMPLIANCE_AVAILABLE:
            logger.warning("Saga Compliance module not available")
            return None
        
        return create_admin_interface(orchestrator=self.saga_orchestrator)
    
    def create_reporting_interface(
        self,
        compliance_engine: Optional["SagaComplianceEngine"] = None,
        monitoring: Optional["SagaMonitoringInterface"] = None
    ) -> Optional["SagaReportingInterface"]:
        """
        Create Saga Reporting Interface.
        
        Args:
            compliance_engine: Optional SagaComplianceEngine instance
            monitoring: Optional SagaMonitoringInterface instance
        
        Returns:
            SagaReportingInterface instance if available, None otherwise
        
        Example:
            ```python
            compliance = core.create_compliance_engine()
            monitoring = core.create_monitoring_interface()
            reporting = core.create_reporting_interface(compliance, monitoring)
            
            audit = reporting.generate_audit_trail("saga-123")
            data = reporting.export_compliance_data("saga-123", "json")
            ```
        """
        if not SAGA_COMPLIANCE_AVAILABLE:
            logger.warning("Saga Compliance module not available")
            return None
        
        return create_reporting_interface(
            compliance_engine=compliance_engine,
            monitoring=monitoring
        )
    
    def check_saga_compliance(self, saga_id: str) -> Optional["ComplianceReport"]:
        """
        Quick compliance check for a saga.
        
        Args:
            saga_id: Saga identifier
        
        Returns:
            ComplianceReport if available, None otherwise
        
        Example:
            ```python
            report = core.check_saga_compliance("saga-123")
            if report:
                print(f"Status: {report.compliance_status.value}")
                print(f"Violations: {len(report.violations)}")
            ```
        """
        compliance = self.create_compliance_engine()
        if compliance:
            return compliance.check_saga_compliance(saga_id)
        return None
    
    def get_saga_health(self, saga_id: str) -> Optional["SagaHealthMetrics"]:
        """
        Get health metrics for a saga.
        
        Args:
            saga_id: Saga identifier
        
        Returns:
            SagaHealthMetrics if available, None otherwise
        
        Example:
            ```python
            health = core.get_saga_health("saga-123")
            if health:
                print(f"Status: {health.status.value}")
                print(f"Health Score: {health.health_score:.2f}")
                print(f"Completion: {health.completion_percentage}%")
            ```
        """
        monitoring = self.create_monitoring_interface()
        if monitoring:
            return monitoring.get_saga_health(saga_id)
        return None
    
    def pause_saga(self, saga_id: str, reason: str, performed_by: str) -> bool:
        """
        Pause a running saga.
        
        Args:
            saga_id: Saga identifier
            reason: Reason for pausing
            performed_by: Admin user performing action
        
        Returns:
            True if successful, False otherwise
        
        Example:
            ```python
            success = core.pause_saga(
                "saga-123",
                "Investigating timeout issue",
                "admin-user"
            )
            ```
        """
        admin = self.create_admin_interface()
        if admin:
            return admin.pause_saga(saga_id, reason, performed_by)
        return False
    
    def generate_saga_audit_trail(self, saga_id: str) -> Optional["AuditTrail"]:
        """
        Generate audit trail for saga execution.
        
        Args:
            saga_id: Saga identifier
        
        Returns:
            AuditTrail if available, None otherwise
        
        Example:
            ```python
            audit = core.generate_saga_audit_trail("saga-123")
            if audit:
                print(f"Events: {len(audit.events)}")
                print(f"User Actions: {len(audit.user_actions)}")
                
                # Export as JSON
                import json
                print(json.dumps(audit.to_dict(), indent=2))
            ```
        """
        reporting = self.create_reporting_interface()
        if reporting:
            return reporting.generate_audit_trail(saga_id)
        return None

    # ==================================================================
    # VPB Operations Methods
    # ==================================================================
    
    def create_vpb_crud_manager(
        self,
        storage_backend: Optional[Any] = None
    ) -> Optional["VPBCRUDManager"]:
        """
        Create VPB CRUD Manager.
        
        Args:
            storage_backend: Optional storage backend (defaults to in-memory)
        
        Returns:
            VPBCRUDManager instance if available, None otherwise
        
        Example:
            ```python
            crud = core.create_vpb_crud_manager()
            process = create_vpb_process(
                "Baugenehmigung",
                "Building permit process",
                LegalContext.BAURECHT,
                AuthorityLevel.GEMEINDE
            )
            result = crud.create_process(process)
            print(f"Created: {result['process_id']}")
            ```
        """
        if not VPB_OPERATIONS_AVAILABLE:
            logger.warning("VPB Operations module not available")
            return None
        
        return create_vpb_crud_manager(storage_backend=storage_backend)
    
    def create_vpb_mining_engine(self) -> Optional["VPBProcessMiningEngine"]:
        """
        Create VPB Process Mining Engine.
        
        Returns:
            VPBProcessMiningEngine instance if available, None otherwise
        
        Example:
            ```python
            engine = core.create_vpb_mining_engine()
            analysis = engine.analyze_process(process)
            print(f"Complexity: {analysis.complexity.value}")
            print(f"Automation: {analysis.automation_potential:.1%}")
            print(f"Bottlenecks: {len(analysis.bottlenecks)}")
            ```
        """
        if not VPB_OPERATIONS_AVAILABLE:
            logger.warning("VPB Operations module not available")
            return None
        
        return create_vpb_process_mining_engine()
    
    def create_vpb_reporting_interface(
        self,
        crud_manager: Optional["VPBCRUDManager"] = None,
        mining_engine: Optional["VPBProcessMiningEngine"] = None
    ) -> Optional["VPBReportingInterface"]:
        """
        Create VPB Reporting Interface.
        
        Args:
            crud_manager: Optional VPBCRUDManager instance
            mining_engine: Optional VPBProcessMiningEngine instance
        
        Returns:
            VPBReportingInterface instance if available, None otherwise
        
        Example:
            ```python
            crud = core.create_vpb_crud_manager()
            mining = core.create_vpb_mining_engine()
            reporting = core.create_vpb_reporting_interface(crud, mining)
            
            report = reporting.generate_process_report("vpb-123")
            complexity = reporting.generate_complexity_report()
            automation = reporting.generate_automation_report()
            
            # Export data
            json_data = reporting.export_process_data("vpb-123", "json")
            ```
        """
        if not VPB_OPERATIONS_AVAILABLE:
            logger.warning("VPB Operations module not available")
            return None
        
        # Create default instances if not provided
        if crud_manager is None:
            crud_manager = self.create_vpb_crud_manager()
        if mining_engine is None:
            mining_engine = self.create_vpb_mining_engine()
        
        return create_vpb_reporting_interface(
            crud_manager=crud_manager,
            mining_engine=mining_engine
        )
    
    def analyze_vpb_process(
        self,
        process_id: str,
        crud_manager: Optional["VPBCRUDManager"] = None
    ) -> Optional["ProcessAnalysisResult"]:
        """
        Analyze a VPB process (convenience method).
        
        Args:
            process_id: Process identifier
            crud_manager: Optional VPBCRUDManager instance
        
        Returns:
            ProcessAnalysisResult if available, None otherwise
        
        Example:
            ```python
            analysis = core.analyze_vpb_process("vpb-123")
            if analysis:
                print(f"Complexity: {analysis.complexity.value}")
                print(f"Automation: {analysis.automation_potential:.1%}")
                print(f"Duration: {analysis.estimated_duration_days:.1f} days")
                
                for rec in analysis.recommendations:
                    print(f"- {rec}")
            ```
        """
        if not VPB_OPERATIONS_AVAILABLE:
            logger.warning("VPB Operations module not available")
            return None
        
        # Create instances if not provided
        if crud_manager is None:
            crud_manager = self.create_vpb_crud_manager()
        
        mining_engine = self.create_vpb_mining_engine()
        
        if not crud_manager or not mining_engine:
            return None
        
        # Read process
        process = crud_manager.read_process(process_id)
        if not process:
            logger.warning(f"Process {process_id} not found")
            return None
        
        # Analyze
        return mining_engine.analyze_process(process)
    
    def generate_vpb_report(
        self,
        process_id: str,
        report_type: str = "process",
        crud_manager: Optional["VPBCRUDManager"] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Generate VPB report (convenience method).
        
        Args:
            process_id: Process identifier
            report_type: Type of report ("process", "complexity", "automation")
            crud_manager: Optional VPBCRUDManager instance
        
        Returns:
            Report data as dict if available, None otherwise
        
        Example:
            ```python
            # Process report
            report = core.generate_vpb_report("vpb-123", "process")
            
            # Complexity report (all processes)
            complexity = core.generate_vpb_report("", "complexity")
            
            # Automation report (all processes)
            automation = core.generate_vpb_report("", "automation")
            ```
        """
        if not VPB_OPERATIONS_AVAILABLE:
            logger.warning("VPB Operations module not available")
            return None
        
        # Create instances
        if crud_manager is None:
            crud_manager = self.create_vpb_crud_manager()
        
        mining_engine = self.create_vpb_mining_engine()
        reporting = self.create_vpb_reporting_interface(crud_manager, mining_engine)
        
        if not reporting:
            return None
        
        # Generate report based on type
        if report_type == "process":
            return reporting.generate_process_report(process_id)
        elif report_type == "complexity":
            return reporting.generate_complexity_report()
        elif report_type == "automation":
            return reporting.generate_automation_report()
        else:
            logger.warning(f"Unknown report type: {report_type}")
            return None

    # ==================================================================
    # File Storage Filter Methods
    # ==================================================================
    
    def create_file_storage_filter(
        self,
        backend: Optional["FileStorageBackend"] = None
    ) -> Optional["FileStorageFilter"]:
        """
        Create File Storage Filter.
        
        Args:
            backend: Optional FileStorageBackend (defaults to LocalFileSystemBackend)
        
        Returns:
            FileStorageFilter instance if available, None otherwise
        
        Example:
            ```python
            file_filter = core.create_file_storage_filter()
            
            # Search for Python files
            files = file_filter.filter_by_extension(["py"], ".")
            print(f"Found {len(files)} Python files")
            ```
        """
        if not FILE_STORAGE_FILTER_AVAILABLE:
            logger.warning("File Storage Filter module not available")
            return None
        
        return create_file_storage_filter(backend=backend)
    
    def create_file_backend(self) -> Optional["LocalFileSystemBackend"]:
        """
        Create Local File System Backend.
        
        Returns:
            LocalFileSystemBackend instance if available, None otherwise
        
        Example:
            ```python
            backend = core.create_file_backend()
            files = backend.scan_directory(".", recursive=True)
            print(f"Found {len(files)} files")
            ```
        """
        if not FILE_STORAGE_FILTER_AVAILABLE:
            logger.warning("File Storage Filter module not available")
            return None
        
        return create_local_backend()
    
    def create_file_search_query(self, **kwargs) -> Optional["FileSearchQuery"]:
        """
        Create File Search Query.
        
        Args:
            **kwargs: Query parameters (extensions, min_size_bytes, file_types, etc.)
        
        Returns:
            FileSearchQuery instance if available, None otherwise
        
        Example:
            ```python
            query = core.create_file_search_query(
                extensions=["py", "txt"],
                min_size_bytes=1024,
                sort_by="size_bytes",
                limit=10
            )
            ```
        """
        if not FILE_STORAGE_FILTER_AVAILABLE:
            logger.warning("File Storage Filter module not available")
            return None
        
        return create_search_query(**kwargs)
    
    def search_files(
        self,
        query: Optional["FileSearchQuery"] = None,
        base_directory: str = ".",
        **query_kwargs
    ) -> Optional["FileFilterResult"]:
        """
        Search files with query (convenience method).
        
        Args:
            query: Optional FileSearchQuery instance
            base_directory: Base directory to search
            **query_kwargs: Query parameters if query not provided
        
        Returns:
            FileFilterResult if available, None otherwise
        
        Example:
            ```python
            # Using query object
            query = core.create_file_search_query(extensions=["py"])
            result = core.search_files(query, ".")
            
            # Using kwargs
            result = core.search_files(
                base_directory=".",
                extensions=["py", "txt"],
                min_size_bytes=1024,
                sort_by="name"
            )
            
            print(f"Found {result.filtered_count} files in {result.execution_time_ms}ms")
            ```
        """
        if not FILE_STORAGE_FILTER_AVAILABLE:
            logger.warning("File Storage Filter module not available")
            return None
        
        # Create query if not provided
        if query is None:
            query = create_search_query(**query_kwargs)
        
        # Create filter and search
        file_filter = self.create_file_storage_filter()
        if not file_filter:
            return None
        
        return file_filter.search(query, base_directory)
    
    def get_file_metadata(self, file_path: str) -> Optional["FileMetadata"]:
        """
        Get metadata for a single file (convenience method).
        
        Args:
            file_path: Path to file
        
        Returns:
            FileMetadata if available, None otherwise
        
        Example:
            ```python
            metadata = core.get_file_metadata("example.py")
            if metadata:
                print(f"File: {metadata.name}")
                print(f"Size: {metadata.size_in_unit(SizeUnit.KB):.2f} KB")
                print(f"Type: {metadata.file_type.value}")
            ```
        """
        if not FILE_STORAGE_FILTER_AVAILABLE:
            logger.warning("File Storage Filter module not available")
            return None
        
        backend = self.create_file_backend()
        if not backend:
            return None
        
        return backend.get_file_metadata(file_path)
    
    def filter_files_by_extension(
        self,
        extensions: List[str],
        base_directory: str = ".",
        limit: Optional[int] = None
    ) -> List["FileMetadata"]:
        """
        Quick filter files by extensions (convenience method).
        
        Args:
            extensions: List of extensions (e.g., ["py", "txt"])
            base_directory: Base directory to search
            limit: Optional result limit
        
        Returns:
            List of FileMetadata objects
        
        Example:
            ```python
            # Find all Python and JavaScript files
            files = core.filter_files_by_extension(["py", "js"], ".")
            
            for f in files:
                print(f"{f.name}: {f.size_in_unit(SizeUnit.KB):.2f} KB")
            ```
        """
        if not FILE_STORAGE_FILTER_AVAILABLE:
            logger.warning("File Storage Filter module not available")
            return []
        
        file_filter = self.create_file_storage_filter()
        if not file_filter:
            return []
        
        return file_filter.filter_by_extension(extensions, base_directory, limit)
    
    def get_file_statistics(self, directory: str = ".") -> Optional[Dict[str, Any]]:
        """
        Get file statistics for directory (convenience method).
        
        Args:
            directory: Directory to analyze
        
        Returns:
            Statistics dictionary if available, None otherwise
        
        Example:
            ```python
            stats = core.get_file_statistics(".")
            
            print(f"Total files: {stats['total_files']}")
            print(f"Total size: {stats['total_size_mb']:.2f} MB")
            print(f"File types: {stats['file_types']}")
            print(f"Extensions: {stats['extensions']}")
            ```
        """
        if not FILE_STORAGE_FILTER_AVAILABLE:
            logger.warning("File Storage Filter module not available")
            return None
        
        file_filter = self.create_file_storage_filter()
        if not file_filter:
            return None
        
        return file_filter.get_statistics(directory)

    # ==================================================================
    # Polyglot Query Methods
    # ==================================================================
    
    def create_polyglot_query(
        self,
        execution_mode: Optional[str] = "smart"
    ) -> Optional["PolyglotQuery"]:
        """
        Create Polyglot Query for cross-database queries.
        
        Args:
            execution_mode: Execution mode ("parallel", "sequential", "smart")
        
        Returns:
            PolyglotQuery instance if available, None otherwise
        
        Example:
            ```python
            # Create polyglot query
            query = core.create_polyglot_query()
            
            # Configure queries for each database
            query.vector().by_similarity(
                embedding=my_embedding,
                threshold=0.8,
                top_k=100
            )
            
            query.graph().by_relationship(
                relationship_type="CITES",
                direction="OUTGOING"
            )
            
            query.relational().from_table("documents_metadata") \\
                .where("status", "=", "active") \\
                .limit(100)
            
            # Execute with INTERSECTION join (AND logic)
            result = query.join_strategy(JoinStrategy.INTERSECTION).execute()
            
            print(f"Found {result.joined_count} documents in all databases")
            print(f"Execution time: {result.total_execution_time_ms:.2f}ms")
            ```
        """
        if not POLYGLOT_QUERY_AVAILABLE:
            logger.warning("Polyglot Query module not available")
            return None
        
        # Map string to ExecutionMode enum
        mode_mapping = {
            "parallel": ExecutionMode.PARALLEL,
            "sequential": ExecutionMode.SEQUENTIAL,
            "smart": ExecutionMode.SMART
        }
        mode = mode_mapping.get(execution_mode.lower(), ExecutionMode.SMART)
        
        return create_polyglot_query(self, execution_mode=mode)
    
    def query_across_databases(
        self,
        vector_params: Optional[Dict[str, Any]] = None,
        graph_params: Optional[Dict[str, Any]] = None,
        relational_params: Optional[Dict[str, Any]] = None,
        file_storage_params: Optional[Dict[str, Any]] = None,
        join_strategy: str = "intersection",
        execution_mode: str = "smart"
    ) -> Optional["PolyglotQueryResult"]:
        """
        Execute cross-database query (convenience method).
        
        Args:
            vector_params: Vector query parameters (embedding, threshold, top_k)
            graph_params: Graph query parameters (relationship_type, direction, max_depth)
            relational_params: Relational query parameters (table, where_conditions, limit)
            file_storage_params: File storage parameters (extensions, base_directory, limit)
            join_strategy: Join strategy ("intersection", "union", "sequential")
            execution_mode: Execution mode ("parallel", "sequential", "smart")
        
        Returns:
            PolyglotQueryResult if available, None otherwise
        
        Example:
            ```python
            # Query across all databases
            result = core.query_across_databases(
                vector_params={
                    "embedding": my_embedding,
                    "threshold": 0.8,
                    "top_k": 100
                },
                graph_params={
                    "relationship_type": "CITES",
                    "direction": "OUTGOING"
                },
                relational_params={
                    "table": "documents_metadata",
                    "where_conditions": {"status": "active"},
                    "limit": 100
                },
                join_strategy="intersection",
                execution_mode="parallel"
            )
            
            if result and result.success:
                print(f"Found {result.joined_count} matching documents")
                for doc_id in result.joined_document_ids:
                    print(f"  - {doc_id}")
            ```
        """
        if not POLYGLOT_QUERY_AVAILABLE:
            logger.warning("Polyglot Query module not available")
            return None
        
        # Create polyglot query
        query = self.create_polyglot_query(execution_mode=execution_mode)
        if not query:
            return None
        
        # Setup vector query if params provided
        if vector_params:
            embedding = vector_params.get("embedding")
            if embedding:
                query.vector().by_similarity(
                    embedding=embedding,
                    threshold=vector_params.get("threshold", 0.7),
                    top_k=vector_params.get("top_k", 100),
                    collection_name=vector_params.get("collection_name", "default")
                )
        
        # Setup graph query if params provided
        if graph_params:
            relationship_type = graph_params.get("relationship_type")
            if relationship_type:
                query.graph().by_relationship(
                    relationship_type=relationship_type,
                    direction=graph_params.get("direction", "OUTGOING"),
                    node_id=graph_params.get("node_id"),
                    max_depth=graph_params.get("max_depth", 1)
                )
        
        # Setup relational query if params provided
        if relational_params:
            table = relational_params.get("table", "documents_metadata")
            builder = query.relational().from_table(table)
            
            # Add WHERE conditions
            where_conditions = relational_params.get("where_conditions", {})
            for field, value in where_conditions.items():
                builder.where(field, "=", value)
            
            # Add LIMIT
            limit = relational_params.get("limit", 100)
            builder.limit(limit)
        
        # Setup file storage query if params provided
        if file_storage_params:
            extensions = file_storage_params.get("extensions", [])
            if extensions:
                query.file_storage().by_extension(
                    extensions=extensions,
                    base_directory=file_storage_params.get("base_directory", "."),
                    limit=file_storage_params.get("limit")
                )
        
        # Set join strategy
        strategy_mapping = {
            "intersection": JoinStrategy.INTERSECTION,
            "union": JoinStrategy.UNION,
            "sequential": JoinStrategy.SEQUENTIAL
        }
        strategy = strategy_mapping.get(join_strategy.lower(), JoinStrategy.INTERSECTION)
        query.join_strategy(strategy)
        
        # Execute query
        return query.execute()
    
    def join_query_results(
        self,
        results: Dict[str, List[Dict[str, Any]]],
        join_strategy: str = "intersection",
        id_field: str = "document_id"
    ) -> List[str]:
        """
        Join query results from multiple databases (utility method).
        
        Args:
            results: Dict mapping database name to list of results
            join_strategy: Join strategy ("intersection", "union", "sequential")
            id_field: Field name containing document ID
        
        Returns:
            List of joined document IDs
        
        Example:
            ```python
            # Manual result joining
            results = {
                "vector": [{"document_id": "doc1"}, {"document_id": "doc2"}],
                "graph": [{"document_id": "doc2"}, {"document_id": "doc3"}],
                "relational": [{"document_id": "doc2"}, {"document_id": "doc4"}]
            }
            
            # INTERSECTION: doc2 (in all databases)
            joined = core.join_query_results(results, join_strategy="intersection")
            print(joined)  # ["doc2"]
            
            # UNION: doc1, doc2, doc3, doc4 (in any database)
            joined = core.join_query_results(results, join_strategy="union")
            print(joined)  # ["doc1", "doc2", "doc3", "doc4"]
            ```
        """
        # Extract document IDs from each database's results
        id_sets = {}
        
        for db_name, db_results in results.items():
            ids = set()
            for result in db_results:
                doc_id = result.get(id_field)
                if doc_id:
                    ids.add(str(doc_id))
            id_sets[db_name] = ids
        
        # Apply join strategy
        if join_strategy.lower() == "intersection":
            # AND: Document must be in ALL databases
            if not id_sets:
                return []
            
            joined_ids = set.intersection(*id_sets.values())
        
        elif join_strategy.lower() == "union":
            # OR: Document in ANY database
            joined_ids = set()
            for ids in id_sets.values():
                joined_ids |= ids
        
        elif join_strategy.lower() == "sequential":
            # Pipeline: Use first DB results
            db_names = list(results.keys())
            if not db_names:
                return []
            
            joined_ids = id_sets.get(db_names[0], set())
        
        else:
            # Default to INTERSECTION
            if not id_sets:
                return []
            joined_ids = set.intersection(*id_sets.values())
        
        return sorted(list(joined_ids))

    # ==================================================================
    # Single Record Cache Management Methods
    # ==================================================================

    def enable_cache(self, max_size: int = 1000, ttl_seconds: float = 300.0):
        """
        Aktiviert den Single Record Cache
        
        Args:
            max_size: Maximale Anzahl Einträge im Cache
            ttl_seconds: Time-To-Live in Sekunden
        
        Example:
            ```python
            # Enable cache with custom settings
            uds.enable_cache(max_size=5000, ttl_seconds=600.0)
            ```
        """
        if not SINGLE_RECORD_CACHE_AVAILABLE:
            logger.warning("Single Record Cache module not available")
            return
        
        try:
            if self.single_record_cache:
                self.single_record_cache.stop()
            
            self.single_record_cache = create_single_record_cache(
                max_size=max_size,
                default_ttl_seconds=ttl_seconds,
                enable_auto_cleanup=True
            )
            self.cache_enabled = True
            logger.info(f"✅ Cache enabled: {max_size} entries, {ttl_seconds}s TTL")
        except Exception as e:
            logger.error(f"Failed to enable cache: {e}")
    
    def disable_cache(self):
        """
        Deaktiviert den Single Record Cache
        
        Example:
            ```python
            uds.disable_cache()
            ```
        """
        self.cache_enabled = False
        if self.single_record_cache:
            self.single_record_cache.stop()
            self.single_record_cache = None
        logger.info("Cache disabled")
    
    def clear_cache(self):
        """
        Leert den gesamten Cache
        
        Example:
            ```python
            # Clear all cached entries
            uds.clear_cache()
            ```
        """
        if self.single_record_cache:
            self.single_record_cache.clear()
            logger.info("Cache cleared")
    
    def invalidate_cache(self, document_id: str) -> bool:
        """
        Invalidiert einen einzelnen Cache-Eintrag
        
        Args:
            document_id: Dokument-ID
        
        Returns:
            True wenn Entry existierte
        
        Example:
            ```python
            # Invalidate cache after document update
            uds.update_document("doc123", {"status": "published"})
            uds.invalidate_cache("doc123")
            ```
        """
        if self.single_record_cache:
            return self.single_record_cache.invalidate(document_id)
        return False
    
    def invalidate_cache_pattern(self, pattern: str) -> int:
        """
        Invalidiert alle Cache-Einträge die Pattern matchen
        
        Args:
            pattern: Regex pattern
        
        Returns:
            Anzahl invalidierter Einträge
        
        Example:
            ```python
            # Invalidate all user documents
            count = uds.invalidate_cache_pattern(r"^user_")
            print(f"Invalidated {count} entries")
            ```
        """
        if self.single_record_cache:
            return self.single_record_cache.invalidate_pattern(pattern)
        return 0
    
    def get_cache_statistics(self) -> Optional[Dict[str, Any]]:
        """
        Gibt Cache-Statistiken zurück
        
        Returns:
            Dict mit Cache-Statistiken oder None
        
        Example:
            ```python
            stats = uds.get_cache_statistics()
            print(f"Hit rate: {stats['hit_rate']}%")
            print(f"Total requests: {stats['total_requests']}")
            ```
        """
        if self.single_record_cache:
            stats = self.single_record_cache.get_statistics()
            return stats.to_dict()
        return None
    
    def get_cache_info(self) -> Optional[Dict[str, Any]]:
        """
        Gibt detaillierte Cache-Informationen zurück
        
        Returns:
            Dict mit Cache-Informationen oder None
        
        Example:
            ```python
            info = uds.get_cache_info()
            print(f"Current size: {info['current_size']}/{info['config']['max_size']}")
            print(f"Usage: {info['usage_percent']}%")
            ```
        """
        if self.single_record_cache:
            return self.single_record_cache.get_info()
        return None

    # ==================================================================
    # Archive Operations Methods
    # ==================================================================
    
    def archive_document(
        self,
        document_id: str,
        retention_policy: Optional[str] = None,
        retention_days: Optional[int] = None,
        archived_by: Optional[str] = None,
        reason: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Archiviert ein Dokument in Langzeitspeicher.
        
        Args:
            document_id: Dokument-ID
            retention_policy: Benannte Retention-Policy
            retention_days: Override für Retention-Tage
            archived_by: User/System-Identifier
            reason: Grund für Archivierung
        
        Returns:
            Dict mit Archivierungs-Details
        
        Example:
            ```python
            result = uds.archive_document(
                "doc123",
                retention_policy="long_term",
                archived_by="admin",
                reason="Annual archive"
            )
            print(f"Archived: {result['archive_id']}")
            print(f"Expires: {result['retention_until']}")
            ```
        """
        if not self.archive_manager:
            return {
                'success': False,
                'error': 'Archive Manager not available'
            }
        
        try:
            result = self.archive_manager.archive_document(
                document_id=document_id,
                retention_policy=retention_policy,
                retention_days=retention_days,
                archived_by=archived_by,
                reason=reason
            )
            return result.to_dict()
        except Exception as e:
            logger.error(f"Archive operation failed: {e}")
            return {
                'success': False,
                'document_id': document_id,
                'error': str(e)
            }
    
    def restore_archived_document(
        self,
        document_id: str,
        strategy: str = "replace",
        restored_by: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Stellt ein archiviertes Dokument wieder her.
        
        Args:
            document_id: Dokument-ID
            strategy: Restore-Strategie ("replace", "merge", "new_version", "fail_if_exists")
            restored_by: User/System-Identifier
        
        Returns:
            Dict mit Restore-Details
        
        Example:
            ```python
            result = uds.restore_archived_document(
                "doc123",
                strategy="replace",
                restored_by="admin"
            )
            print(f"Restored: {result['success']}")
            print(f"Affected DBs: {result['affected_databases']}")
            ```
        """
        if not self.archive_manager:
            return {
                'success': False,
                'error': 'Archive Manager not available'
            }
        
        try:
            # Convert string strategy to enum
            from manager.archive import RestoreStrategy
            strategy_map = {
                'replace': RestoreStrategy.REPLACE,
                'merge': RestoreStrategy.MERGE,
                'new_version': RestoreStrategy.NEW_VERSION,
                'fail_if_exists': RestoreStrategy.FAIL_IF_EXISTS
            }
            restore_strategy = strategy_map.get(strategy.lower(), RestoreStrategy.REPLACE)
            
            result = self.archive_manager.restore_document(
                document_id=document_id,
                strategy=restore_strategy,
                restored_by=restored_by
            )
            return result.to_dict()
        except Exception as e:
            logger.error(f"Restore operation failed: {e}")
            return {
                'success': False,
                'document_id': document_id,
                'error': str(e)
            }
    
    def list_archived_documents(
        self,
        status: Optional[str] = None,
        limit: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        Listet archivierte Dokumente auf.
        
        Args:
            status: Filter nach Status (None = alle)
            limit: Maximale Anzahl
        
        Returns:
            Liste von ArchiveMetadata-Dicts
        
        Example:
            ```python
            archived = uds.list_archived_documents(status="archived", limit=10)
            for doc in archived:
                print(f"{doc['document_id']}: {doc['archived_at']}")
            ```
        """
        if not self.archive_manager:
            return []
        
        try:
            # Convert string status to enum
            archive_status = None
            if status:
                from manager.archive import ArchiveStatus
                status_map = {
                    'archived': ArchiveStatus.ARCHIVED,
                    'restoring': ArchiveStatus.RESTORING,
                    'restored': ArchiveStatus.RESTORED,
                    'expired': ArchiveStatus.EXPIRED,
                    'deleted': ArchiveStatus.DELETED
                }
                archive_status = status_map.get(status.lower())
            
            archived = self.archive_manager.list_archived_documents(
                status=archive_status,
                limit=limit
            )
            return [a.to_dict() for a in archived]
        except Exception as e:
            logger.error(f"List archived failed: {e}")
            return []
    
    def get_archive_info(self) -> Dict[str, Any]:
        """
        Gibt Archive-Statistiken zurück.
        
        Returns:
            Dict mit Archive-Statistiken
        
        Example:
            ```python
            info = uds.get_archive_info()
            print(f"Total archived: {info['total_archived']}")
            print(f"Total size: {info['total_size_bytes']} bytes")
            print(f"Expiring soon: {info['expiring_soon']}")
            ```
        """
        if not self.archive_manager:
            return {
                'available': False,
                'error': 'Archive Manager not available'
            }
        
        try:
            info = self.archive_manager.get_archive_info()
            result = info.to_dict()
            result['available'] = True
            return result
        except Exception as e:
            logger.error(f"Get archive info failed: {e}")
            return {
                'available': False,
                'error': str(e)
            }
    
    def add_retention_policy(
        self,
        name: str,
        retention_days: int,
        auto_delete: bool = True,
        document_types: Optional[List[str]] = None
    ) -> bool:
        """
        Fügt eine Retention-Policy hinzu.
        
        Args:
            name: Policy-Name
            retention_days: Retention-Zeitraum in Tagen
            auto_delete: Automatische Löschung nach Ablauf
            document_types: Optional: Dokumenttypen
        
        Returns:
            True wenn erfolgreich
        
        Example:
            ```python
            success = uds.add_retention_policy(
                name="contracts",
                retention_days=2555,  # 7 years
                auto_delete=False
            )
            ```
        """
        if not self.archive_manager:
            logger.warning("Archive Manager not available")
            return False
        
        try:
            from manager.archive import RetentionPolicy
            policy = RetentionPolicy(
                name=name,
                retention_days=retention_days,
                auto_delete=auto_delete,
                document_types=document_types
            )
            self.archive_manager.add_retention_policy(policy)
            return True
        except Exception as e:
            logger.error(f"Add retention policy failed: {e}")
            return False
    
    def apply_retention_policies(self) -> Dict[str, Any]:
        """
        Wendet Retention-Policies an und löscht/markiert abgelaufene Dokumente.
        
        Returns:
            Dict mit Anwendungs-Details
        
        Example:
            ```python
            result = uds.apply_retention_policies()
            print(f"Expired: {result['expired_count']}")
            print(f"Deleted: {result['deleted_count']}")
            ```
        """
        if not self.archive_manager:
            return {
                'success': False,
                'error': 'Archive Manager not available'
            }
        
        try:
            return self.archive_manager.apply_retention_policies()
        except Exception as e:
            logger.error(f"Apply retention policies failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    # ==================================================================
    # Streaming Operations Methods (For Large Files)
    # ==================================================================
    
    def upload_large_file(
        self,
        file_path: str,
        destination: str,
        chunk_size: Optional[int] = None,
        progress_callback: Optional[Callable[[Any], None]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Upload einer großen Datei (z.B. 300+ MB PDF) in Chunks.
        
        Args:
            file_path: Pfad zur Datei
            destination: Ziel-Pfad/Identifier
            chunk_size: Optional: Chunk-Größe überschreiben
            progress_callback: Optional: Callback für Progress-Updates
            metadata: Optional: Zusätzliche Metadaten
        
        Returns:
            Dict mit Upload-Details (operation_id, status, etc.)
        
        Example:
            ```python
            def on_progress(progress):
                print(f"Progress: {progress.progress_percent:.1f}%")
                print(f"Speed: {progress.bytes_per_second/1024/1024:.1f} MB/s")
            
            result = uds.upload_large_file(
                "large_document.pdf",
                "storage/documents/large_document.pdf",
                progress_callback=on_progress
            )
            print(f"Upload ID: {result['operation_id']}")
            print(f"Status: {result['status']}")
            ```
        """
        if not self.streaming_manager:
            return {
                'success': False,
                'error': 'Streaming Manager not available'
            }
        
        try:
            operation_id = self.streaming_manager.upload_large_file(
                file_path=file_path,
                destination=destination,
                chunk_size=chunk_size,
                progress_callback=progress_callback,
                metadata=metadata
            )
            
            # Get final progress
            progress = self.streaming_manager.get_progress(operation_id)
            
            return {
                'success': True,
                'operation_id': operation_id,
                'status': progress.status.value if progress else 'unknown',
                'progress': progress.to_dict() if progress else None
            }
        except Exception as e:
            logger.error(f"Upload large file failed: {e}")
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }
    
    def download_large_file(
        self,
        source: str,
        output_path: str,
        chunk_size: Optional[int] = None,
        progress_callback: Optional[Callable[[Any], None]] = None
    ) -> Dict[str, Any]:
        """
        Download einer großen Datei in Chunks.
        
        Args:
            source: Quell-Pfad/Identifier
            output_path: Lokaler Ausgabe-Pfad
            chunk_size: Optional: Chunk-Größe überschreiben
            progress_callback: Optional: Callback für Progress-Updates
        
        Returns:
            Dict mit Download-Details
        
        Example:
            ```python
            result = uds.download_large_file(
                "storage/documents/large_document.pdf",
                "downloads/large_document.pdf",
                progress_callback=lambda p: print(f"{p.progress_percent:.1f}%")
            )
            ```
        """
        if not self.streaming_manager:
            return {
                'success': False,
                'error': 'Streaming Manager not available'
            }
        
        try:
            operation_id = self.streaming_manager.download_large_file(
                source=source,
                output_path=output_path,
                chunk_size=chunk_size,
                progress_callback=progress_callback
            )
            
            # Get final progress
            progress = self.streaming_manager.get_progress(operation_id)
            
            return {
                'success': True,
                'operation_id': operation_id,
                'status': progress.status.value if progress else 'unknown',
                'progress': progress.to_dict() if progress else None
            }
        except Exception as e:
            logger.error(f"Download large file failed: {e}")
            return {
                'success': False,
                'source': source,
                'error': str(e)
            }
    
    def resume_upload(
        self,
        operation_id: str,
        file_path: str,
        destination: str,
        progress_callback: Optional[Callable[[Any], None]] = None
    ) -> Dict[str, Any]:
        """
        Setzt einen unterbrochenen Upload fort.
        
        Args:
            operation_id: Original-Operation-ID
            file_path: Pfad zur Datei
            destination: Ziel-Pfad
            progress_callback: Optional: Callback für Progress-Updates
        
        Returns:
            Dict mit Resume-Details
        
        Example:
            ```python
            # Upload unterbrochen nach 50%
            result = uds.resume_upload(
                operation_id="upload-abc123",
                file_path="large_document.pdf",
                destination="storage/documents/large_document.pdf"
            )
            print(f"Resumed from: {result['resumed_from_chunk']}")
            ```
        """
        if not self.streaming_manager:
            return {
                'success': False,
                'error': 'Streaming Manager not available'
            }
        
        try:
            # Get original progress
            old_progress = self.streaming_manager.get_progress(operation_id)
            if not old_progress:
                return {
                    'success': False,
                    'error': f'Operation not found: {operation_id}'
                }
            
            resumed_operation_id = self.streaming_manager.resume_upload(
                operation_id=operation_id,
                file_path=file_path,
                destination=destination,
                progress_callback=progress_callback
            )
            
            # Get new progress
            progress = self.streaming_manager.get_progress(resumed_operation_id)
            
            return {
                'success': True,
                'operation_id': resumed_operation_id,
                'resumed_from_chunk': old_progress.current_chunk,
                'status': progress.status.value if progress else 'unknown',
                'progress': progress.to_dict() if progress else None
            }
        except Exception as e:
            logger.error(f"Resume upload failed: {e}")
            return {
                'success': False,
                'operation_id': operation_id,
                'error': str(e)
            }
    
    def get_streaming_status(
        self,
        operation_id: str
    ) -> Dict[str, Any]:
        """
        Gibt Status einer Streaming-Operation zurück.
        
        Args:
            operation_id: Operation-ID
        
        Returns:
            Dict mit Status-Details
        
        Example:
            ```python
            status = uds.get_streaming_status("upload-abc123")
            print(f"Progress: {status['progress_percent']:.1f}%")
            print(f"Speed: {status['bytes_per_second']/1024/1024:.1f} MB/s")
            print(f"ETA: {status['estimated_time_remaining']:.1f}s")
            ```
        """
        if not self.streaming_manager:
            return {
                'success': False,
                'error': 'Streaming Manager not available'
            }
        
        try:
            progress = self.streaming_manager.get_progress(operation_id)
            if not progress:
                return {
                    'success': False,
                    'error': f'Operation not found: {operation_id}'
                }
            
            result = progress.to_dict()
            result['success'] = True
            return result
        except Exception as e:
            logger.error(f"Get streaming status failed: {e}")
            return {
                'success': False,
                'operation_id': operation_id,
                'error': str(e)
            }
    
    def list_streaming_operations(
        self,
        status: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Listet alle Streaming-Operationen auf.
        
        Args:
            status: Optional: Filter nach Status ("pending", "in_progress", "completed", etc.)
        
        Returns:
            Liste von Streaming-Progress-Dicts
        
        Example:
            ```python
            operations = uds.list_streaming_operations(status="in_progress")
            for op in operations:
                print(f"{op['operation_id']}: {op['progress_percent']:.1f}%")
            ```
        """
        if not self.streaming_manager:
            return []
        
        try:
            # Convert string status to enum
            streaming_status = None
            if status:
                status_map = {
                    'pending': StreamingStatus.PENDING,
                    'in_progress': StreamingStatus.IN_PROGRESS,
                    'paused': StreamingStatus.PAUSED,
                    'completed': StreamingStatus.COMPLETED,
                    'failed': StreamingStatus.FAILED,
                    'cancelled': StreamingStatus.CANCELLED
                }
                streaming_status = status_map.get(status.lower())
            
            operations = self.streaming_manager.list_operations(status=streaming_status)
            return [op.to_dict() for op in operations]
        except Exception as e:
            logger.error(f"List streaming operations failed: {e}")
            return []
    
    def cancel_streaming_operation(
        self,
        operation_id: str
    ) -> Dict[str, Any]:
        """
        Bricht eine Streaming-Operation ab.
        
        Args:
            operation_id: Operation-ID
        
        Returns:
            Dict mit Cancel-Status
        
        Example:
            ```python
            result = uds.cancel_streaming_operation("upload-abc123")
            print(f"Cancelled: {result['success']}")
            ```
        """
        if not self.streaming_manager:
            return {
                'success': False,
                'error': 'Streaming Manager not available'
            }
        
        try:
            success = self.streaming_manager.cancel_operation(operation_id)
            return {
                'success': success,
                'operation_id': operation_id
            }
        except Exception as e:
            logger.error(f"Cancel streaming operation failed: {e}")
            return {
                'success': False,
                'operation_id': operation_id,
                'error': str(e)
            }
    
    def stream_to_vector_db(
        self,
        file_path: str,
        embedding_function: Callable[[str], List[float]],
        chunk_text_size: int = 1000,
        progress_callback: Optional[Callable[[Any], None]] = None
    ) -> Dict[str, Any]:
        """
        Streamt große Datei zur Vector DB mit Chunked Embeddings.
        
        Für große PDFs (300+ MB) mit Embeddings - vermeidet Out-of-Memory.
        
        Args:
            file_path: Pfad zur Datei
            embedding_function: Funktion zur Embedding-Generierung
            chunk_text_size: Text-Chunk-Größe (Zeichen)
            progress_callback: Optional: Callback für Progress-Updates
        
        Returns:
            Dict mit Streaming-Details
        
        Example:
            ```python
            def generate_embedding(text: str) -> List[float]:
                # Your embedding logic here
                return [0.1, 0.2, 0.3]  # Example
            
            result = uds.stream_to_vector_db(
                "large_document.pdf",
                embedding_function=generate_embedding,
                chunk_text_size=1000
            )
            print(f"Chunks processed: {result['chunks_processed']}")
            ```
        """
        if not self.streaming_manager:
            return {
                'success': False,
                'error': 'Streaming Manager not available'
            }
        
        try:
            operation_id = self.streaming_manager.stream_to_vector_db(
                file_path=file_path,
                embedding_function=embedding_function,
                chunk_text_size=chunk_text_size,
                progress_callback=progress_callback
            )
            
            # Get final progress
            progress = self.streaming_manager.get_progress(operation_id)
            
            return {
                'success': True,
                'operation_id': operation_id,
                'chunks_processed': progress.current_chunk if progress else 0,
                'status': progress.status.value if progress else 'unknown',
                'progress': progress.to_dict() if progress else None
            }
        except Exception as e:
            logger.error(f"Stream to vector DB failed: {e}")
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    # ==================================================================
    # Streaming Saga Operations (With Automatic Rollback)
    # ==================================================================
    
    def create_document_streaming(
        self,
        file_path: str,
        content: str,
        chunks: List[str],
        progress_callback: Optional[Callable[[Any], None]] = None,
        max_resume_attempts: int = 3,
        **metadata
    ) -> Dict[str, Any]:
        """
        Erstellt Dokument mit Streaming und automatischem Rollback.
        
        **Für große Dateien (50+ MB)**: Automatisches Streaming mit Resume-Support
        **Rollback bei:**
        - Resume fehlschlägt nach N Versuchen
        - Integrity Check schlägt fehl (Hash/Size Mismatch)
        - Kritische Fehler (File not found, Metadata corrupt)
        - Timeout überschritten
        
        **Features:**
        - Chunked Upload (Memory-efficient)
        - Automatic Retry (max 3 attempts)
        - Integrity Verification (Hash/Size/Count)
        - Automatic Rollback on Failure
        - Best-Effort Compensation with Logging
        - Progress Tracking with Callbacks
        
        Args:
            file_path: Pfad zur Datei
            content: Text-Inhalt
            chunks: Text-Chunks
            progress_callback: Optional: Progress-Callback
            max_resume_attempts: Maximale Resume-Versuche (default: 3)
            **metadata: Dokument-Metadaten
        
        Returns:
            Dict mit:
            - success: bool
            - saga_id: str
            - status: str (completed, compensated, compensation_failed)
            - document_id: str (if successful)
            - operation_id: str (streaming operation)
            - rollback_performed: bool (if rollback occurred)
            - rollback_status: str (success, partial_failure)
            - errors: List[str]
            - compensation_errors: List[str] (if rollback failed)
        
        Example:
            ```python
            def on_progress(progress):
                print(f"Progress: {progress.progress_percent:.1f}%")
                print(f"Speed: {format_bytes(progress.bytes_per_second)}/s")
                if progress.estimated_time_remaining:
                    print(f"ETA: {format_duration(progress.estimated_time_remaining)}")
            
            result = uds.create_document_streaming(
                file_path="large_document.pdf",  # 300 MB
                content=extracted_text,
                chunks=text_chunks,
                progress_callback=on_progress,
                max_resume_attempts=3,
                title="Large Administrative Document",
                category="Bescheid"
            )
            
            if result['success']:
                print(f"✅ Document created: {result['document_id']}")
                print(f"Saga: {result['saga_id']}")
            elif result.get('rollback_performed'):
                print(f"⚠️ Rollback performed: {result['rollback_status']}")
                print(f"Errors: {result['errors']}")
                if result.get('compensation_errors'):
                    print(f"❌ Manual cleanup required: {result['compensation_errors']}")
            else:
                print(f"❌ Failed: {result['errors']}")
            ```
        """
        # Check availability
        if not self.streaming_manager:
            return {
                'success': False,
                'error': 'Streaming Manager not available',
                'rollback_performed': False
            }
        
        if not STREAMING_SAGA_AVAILABLE:
            return {
                'success': False,
                'error': 'Streaming Saga Integration not available',
                'rollback_performed': False
            }
        
        # Config
        config = StreamingSagaConfig(
            max_resume_attempts=max_resume_attempts,
            resume_retry_delay=5.0,
            hash_verification_enabled=True,
            rollback_on_timeout=True,
            timeout_seconds=3600.0,  # 1 hour
            auto_rollback_on_failure=True
        )
        
        # Determine destination
        import os
        file_name = os.path.basename(file_path)
        destination = f"storage/documents/{file_name}"
        
        # Context for saga
        context = {
            'file_path': file_path,
            'destination': destination,
            'content': content,
            'chunks': chunks,
            'metadata': metadata,
            'streaming_manager': self.streaming_manager,
            'progress_callback': progress_callback,
            'security_level': metadata.get('security_level'),
            'embedding_function': self._get_embedding_function(),
            'vector_db': getattr(self, 'vector_db', None),
            'graph_db': getattr(self, 'graph_db', None),
            'relational_db': getattr(self, 'relational_db', None),
            'security_manager': self.security_manager,
            'create_result': {
                'success': False,
                'issues': [],
                'rollback_performed': False
            }
        }
        
        # Build saga definition
        try:
            saga_definition = build_streaming_upload_saga_definition(
                streaming_manager=self.streaming_manager,
                config=config,
                vector_db=context['vector_db'],
                graph_db=context['graph_db'],
                relational_db=context['relational_db'],
                security_manager=self.security_manager
            )
        except Exception as e:
            logger.error(f"Failed to build streaming saga definition: {e}")
            return {
                'success': False,
                'error': f'Failed to build saga: {e}',
                'rollback_performed': False
            }
        
        # Track saga
        saga_id = f"streaming-saga-{uuid.uuid4().hex[:12]}"
        if self.streaming_saga_monitor:
            self.streaming_saga_monitor.track_saga(saga_id, context)
        
        # Execute saga with automatic rollback
        try:
            saga_result = execute_streaming_saga_with_rollback(
                definition=saga_definition,
                context=context,
                config=config
            )
        except Exception as e:
            logger.critical(f"Saga execution crashed: {e}")
            return {
                'success': False,
                'saga_id': saga_id,
                'error': f'Saga execution failed: {e}',
                'rollback_performed': False
            }
        
        # Build result
        result = {
            'success': saga_result.status == SagaStatus.COMPLETED,
            'saga_id': saga_result.saga_id,
            'status': saga_result.status.value,
            'operation_id': context.get('operation_id'),
            'document_id': context.get('document_id'),
            'errors': saga_result.errors,
            'compensation_errors': saga_result.compensation_errors
        }
        
        # Rollback info
        if saga_result.status in [SagaStatus.COMPENSATED, SagaStatus.COMPENSATION_FAILED]:
            result['rollback_performed'] = True
            result['rollback_status'] = (
                'success' if saga_result.status == SagaStatus.COMPENSATED 
                else 'partial_failure'
            )
            
            if saga_result.compensation_errors:
                result['rollback_warnings'] = saga_result.compensation_errors
                # Store for manual cleanup
                self._store_rollback_failures(saga_result)
            
            # Update monitor
            if self.streaming_saga_monitor:
                compensation_success = (saga_result.status == SagaStatus.COMPENSATED)
                self.streaming_saga_monitor.saga_rolled_back(
                    saga_id, saga_result, compensation_success
                )
        else:
            result['rollback_performed'] = False
            
            # Update monitor for successful completion
            if self.streaming_saga_monitor and saga_result.status == SagaStatus.COMPLETED:
                self.streaming_saga_monitor.saga_completed(saga_id, saga_result)
        
        return result
    
    def _get_embedding_function(self) -> Optional[Callable[[str], List[float]]]:
        """
        Get embedding function for vector DB operations.
        
        Returns:
            Embedding function or None
        """
        # This should be configured based on your vector DB setup
        # For now, return None - will be set up when vector DB is available
        return None
    
    def _store_rollback_failures(self, saga_result: "SagaExecutionResult") -> None:
        """
        Speichert fehlgeschlagene Rollbacks für manuelle Intervention.
        
        Args:
            saga_result: Saga execution result with errors
        """
        if STREAMING_SAGA_AVAILABLE:
            try:
                store_rollback_failures(saga_result)
            except Exception as e:
                logger.error(f"Failed to store rollback failures: {e}")

    # ==================================================================
    # Document Operation Methods (Original)
    # ==================================================================

    def create_document_operation(
        self, file_path: str, content: str, chunks: List[str], **metadata
    ) -> Dict:
        """
        Erstellt eine CREATE-Operation für ein neues Dokument

        Returns:
            Dict: Vollständiger CREATE-Operationsplan
        """
        return self.create_optimized_processing_plan(
            file_path, content, chunks, **metadata
        )

    # ------------------------------------------------------------------
    # Saga-Unterstützung für Dokument-Erstellung
    # ------------------------------------------------------------------
    def _build_create_document_step_specs(
        self, context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        create_result = context["create_result"]
        metadata = context["metadata"]

        def _mark_optional_skip(
            result_dict: Dict[str, Any], message: Optional[str]
        ) -> Dict[str, Any]:
            result_dict["success"] = True
            result_dict["skipped"] = True
            if message:
                result_dict.setdefault("warning", message)
                create_result["issues"].append(message)
            return result_dict

        def _is_optional_backend_error(message: Optional[str]) -> bool:
            if not message:
                return False
            lowered = message.lower()
            return any(
                phrase in lowered
                for phrase in [
                    "nicht konfiguriert",
                    "nicht verfügbar",
                    "not configured",
                    "not available",
                ]
            )

        def security_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            metadata_local = ctx["metadata"]
            file_path_local = ctx["file_path"]
            content_local = ctx["content"]
            security_lvl = ctx["security_level"]
            issues = create_result["issues"]
            aktenzeichen = metadata_local.get("aktenzeichen")

            try:
                content_bytes = (content_local or "").encode("utf-8")
            except AttributeError:
                content_bytes = b""
            file_hash = hashlib.sha256(content_bytes).hexdigest()
            metadata_local.setdefault("file_hash", file_hash)
            metadata_local.setdefault("file_path", file_path_local)
            metadata_local.setdefault("file_size", len(content_bytes))

            identity_record = None
            document_uuid: Optional[str] = None
            document_id: Optional[str] = None

            if self.security_manager:
                security_info = self.security_manager.generate_secure_document_id(
                    content_local,
                    file_path_local,
                    security_lvl or SecurityLevel.INTERNAL,
                )
                create_result["security_info"] = security_info or {}
                create_result["security_info"].setdefault("file_hash", file_hash)
                document_id = security_info.get("document_id")
                document_uuid = security_info.get("document_uuid")

                audit_entry = self.security_manager.create_audit_log_entry(
                    "CREATE_DOCUMENT",
                    document_id,
                    details={"file_path": file_path_local},
                )
                create_result["audit_entry"] = audit_entry

                if document_uuid is None and document_id:
                    document_uuid = self._infer_uuid_from_document_id(document_id)

                if self.identity_service and document_uuid:
                    try:
                        identity_record = self.identity_service.ensure_identity(
                            document_uuid,
                            aktenzeichen=aktenzeichen,
                            source_system="uds3.security",
                            actor="uds3_core",
                        )
                    except IdentityServiceError as exc:
                        issues.append(f"Identity-Service Fehler: {exc}")
                elif self.identity_service:
                    identity_record = self.identity_service.generate_uuid(
                        source_system="uds3.security",
                        aktenzeichen=aktenzeichen,
                        actor="uds3_core",
                    )
                    document_uuid = identity_record.uuid
                    if not document_id:
                        document_id = self._format_document_id(document_uuid)
                    create_result.setdefault("security_info", {})["document_uuid"] = (
                        document_uuid
                    )
                    create_result["security_info"].setdefault(
                        "document_id", document_id
                    )
            else:
                if self.identity_service:
                    identity_record = self.identity_service.generate_uuid(
                        source_system="uds3.core",
                        aktenzeichen=aktenzeichen,
                        actor="uds3_core",
                    )
                    document_uuid = identity_record.uuid
                    document_id = self._format_document_id(document_uuid)
                    create_result["security_info"] = {
                        "document_id": document_id,
                        "document_uuid": document_uuid,
                        "file_hash": file_hash,
                    }
                else:
                    generated_uuid = uuid.uuid4()
                    document_uuid = str(generated_uuid)
                    document_id = f"doc_{generated_uuid.hex}"
                    create_result["security_info"] = {
                        "document_id": document_id,
                        "file_hash": file_hash,
                    }

            identity_key_candidate = metadata_local.get("aktenzeichen")
            if identity_record is not None:
                identity_key_candidate = (
                    getattr(identity_record, "identity_key", None)
                    or getattr(identity_record, "aktenzeichen", None)
                    or (
                        identity_record.get("identity_key")
                        if isinstance(identity_record, dict)
                        else None
                    )
                    or (
                        identity_record.get("aktenzeichen")
                        if isinstance(identity_record, dict)
                        else None
                    )
                    or getattr(identity_record, "uuid", None)
                    or (
                        identity_record.get("uuid")
                        if isinstance(identity_record, dict)
                        else None
                    )
                    or identity_key_candidate
                )
                ctx["identity_uuid"] = (
                    getattr(identity_record, "uuid", None)
                    or getattr(identity_record, "identity_uuid", None)
                    or (
                        identity_record.get("uuid")
                        if isinstance(identity_record, dict)
                        else None
                    )
                    or (
                        identity_record.get("identity_uuid")
                        if isinstance(identity_record, dict)
                        else None
                    )
                )

            if identity_key_candidate:
                ctx["identity_key"] = identity_key_candidate
                ctx["aktenzeichen"] = identity_key_candidate
                metadata_local.setdefault("aktenzeichen", identity_key_candidate)

            ctx["document_uuid"] = document_uuid
            ctx["document_id"] = document_id
            ctx["identity_record"] = identity_record
            ctx["document_data"] = {
                "document_id": document_id,
                "id": document_id,
                "uuid": document_uuid,
                "identity_key": ctx.get("identity_key") or identity_key_candidate,
                "title": metadata_local.get("title", os.path.basename(file_path_local)),
                "content": content_local,
                "file_path": file_path_local,
                "file_hash": metadata_local.get("file_hash", file_hash),
                "file_size": metadata_local.get("file_size"),
                "created_at": datetime.now().isoformat(),
                **metadata_local,
            }
            return None

        def security_compensation(ctx: Dict[str, Any]) -> None:
            ctx.setdefault("compensations", []).append("security_identity_reset")

        def vector_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            document_id = ctx["document_id"]
            result = self._execute_vector_create(
                document_id,
                ctx["content"],
                ctx["chunks"],
                ctx.get("metadata"),
            )
            if not result.get("success", False):
                error_message = result.get("error")
                if _is_optional_backend_error(error_message):
                    result = _mark_optional_skip(result, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Vector DB Operation fehlgeschlagen"
                    )
            ctx["vector_result"] = result
            create_result["database_operations"]["vector"] = result
            return None

        def vector_compensation(ctx: Dict[str, Any]) -> None:
            vector_payload = ctx.get("vector_result") or {}
            if vector_payload.get("skipped"):
                return
            chunk_ids = vector_payload.get("chunk_ids")
            collection = vector_payload.get("collection", "document_chunks")
            document_id = ctx.get("document_id")
            if document_id:
                crud_result = self.saga_crud.vector_delete(
                    document_id,
                    collection=collection,
                    chunk_ids=chunk_ids,
                )
                if not crud_result.success and crud_result.error:
                    create_result["issues"].append(
                        f"Vector-Kompensation fehlgeschlagen: {crud_result.error}"
                    )

        def graph_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            document_id = ctx["document_id"]
            result = self._execute_graph_create(
                document_id, ctx["content"], ctx["metadata"]
            )
            if not result.get("success", False):
                error_message = result.get("error")
                if _is_optional_backend_error(error_message):
                    result = _mark_optional_skip(result, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Graph DB Operation fehlgeschlagen"
                    )
            ctx["graph_result"] = result
            create_result["database_operations"]["graph"] = result
            return None

        def graph_compensation(ctx: Dict[str, Any]) -> None:
            graph_payload = ctx.get("graph_result") or {}
            if graph_payload.get("skipped"):
                return
            identifier = graph_payload.get("graph_id") or graph_payload.get("id")
            if identifier is None and ctx.get("document_id"):
                identifier = f"Document::{ctx['document_id']}"
            if identifier:
                crud_result = self.saga_crud.graph_delete(identifier)
                if not crud_result.success and crud_result.error:
                    create_result["issues"].append(
                        f"Graph-Kompensation fehlgeschlagen: {crud_result.error}"
                    )

        def relational_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            result = self._execute_relational_create(ctx["document_data"])
            ctx["relational_result"] = result
            create_result["database_operations"]["relational"] = result
            if not result.get("success", False):
                error_message = result.get("error")
                if _is_optional_backend_error(error_message):
                    create_result["issues"].append(error_message)
                    return None
                raise SagaExecutionError(
                    error_message or "Relationale Operation fehlgeschlagen"
                )
            return None

        def relational_compensation(ctx: Dict[str, Any]) -> None:
            relational_payload = ctx.get("relational_result") or {}
            document_id = relational_payload.get("document_id") or ctx.get(
                "document_id"
            )
            if document_id:
                crud_result = self.saga_crud.relational_delete(document_id)
                if not crud_result.success and crud_result.error:
                    create_result["issues"].append(
                        f"Relationale Kompensation fehlgeschlagen: {crud_result.error}"
                    )

        def file_storage_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            result = self._execute_file_storage_create(
                ctx["document_id"],
                ctx["file_path"],
                ctx["metadata"],
                ctx.get("content"),
            )
            if not result.get("success", False):
                error_message = result.get("error")
                if _is_optional_backend_error(error_message):
                    result = _mark_optional_skip(result, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "File-Storage Operation fehlgeschlagen"
                    )
            ctx["file_storage_result"] = result
            create_result["database_operations"]["file_storage"] = result
            return None

        def file_storage_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("file_storage_result") or {}
            if payload.get("skipped"):
                return
            asset_id = payload.get("asset_id") or payload.get("file_storage_id")
            if asset_id:
                crud_result = self.saga_crud.file_delete(asset_id)
                if not crud_result.success and crud_result.error:
                    create_result["issues"].append(
                        f"File-Storage Kompensation fehlgeschlagen: {crud_result.error}"
                    )

        def identity_binding_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            if not self.identity_service or not ctx.get("document_uuid"):
                return None
            try:
                identity_record = self.identity_service.bind_backend_ids(
                    ctx["document_uuid"],
                    relational_id=ctx.get("relational_result", {}).get("relational_id"),
                    graph_id=ctx.get("graph_result", {}).get("graph_id"),
                    vector_id=ctx.get("vector_result", {}).get("vector_id"),
                    file_storage_id=ctx.get("file_storage_result", {}).get(
                        "file_storage_id"
                    ),
                    metadata={"document_id": ctx.get("document_id")},
                    actor="uds3_core",
                )
                ctx["identity_record"] = identity_record
                create_result["identity"] = {
                    "uuid": identity_record.uuid,
                    "aktenzeichen": identity_record.aktenzeichen,
                    "status": identity_record.status,
                    "mappings": identity_record.mappings,
                }
            except IdentityServiceError as exc:
                create_result["issues"].append(
                    f"Identity-Mapping fehlgeschlagen: {exc}"
                )
            return None

        def validation_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            vector_result = ctx.get("vector_result", {})
            graph_result = ctx.get("graph_result", {})
            relational_result = ctx.get("relational_result", {})
            file_storage_result = ctx.get("file_storage_result")
            document_id = ctx.get("document_id")
            document_data = ctx.get("document_data", {})

            if self.quality_manager:
                quality_result = self.quality_manager.calculate_document_quality_score(
                    document_data,
                    vector_result,
                    graph_result,
                    relational_result,
                )
                create_result["quality_score"] = quality_result
                if (
                    quality_result["overall_score"]
                    < self.quality_manager.config.minimum_quality_score
                ):
                    create_result["issues"].extend(quality_result["issues"])
                    create_result["quality_warnings"] = quality_result[
                        "recommendations"
                    ]

            validation_result = self._validate_cross_db_consistency(
                document_id,
                vector_result,
                graph_result,
                relational_result,
                file_storage_result,
            )
            create_result["validation_results"] = validation_result

            if self.security_manager and create_result.get("security_info"):
                integrity_check = self.security_manager.verify_document_integrity(
                    document_id,
                    ctx["content"],
                    ctx["file_path"],
                    create_result["security_info"],
                )
                create_result["security_validation"] = integrity_check

            all_db_success = all(
                result.get("success", False) or result.get("skipped", False)
                for result in create_result["database_operations"].values()
            )
            validation_success = validation_result.get("overall_valid", True)
            create_result["success"] = all_db_success and validation_success

            if create_result["success"]:
                self.document_mapping[document_id] = {
                    "uuid": ctx.get("document_uuid"),
                    "identity_key": ctx.get("identity_key") or ctx.get("aktenzeichen"),
                    "vector_id": vector_result.get("vector_id"),
                    "graph_id": graph_result.get("graph_id"),
                    "relational_id": relational_result.get("relational_id"),
                    "file_storage_id": (file_storage_result or {}).get(
                        "file_storage_id"
                    )
                    if file_storage_result
                    else None,
                }
            return None

        return [
            {
                "name": "security_and_identity",
                "action": security_action,
                "compensation": security_compensation,
            },
            {
                "name": "vector_create",
                "action": vector_action,
                "compensation": vector_compensation,
            },
            {
                "name": "graph_create",
                "action": graph_action,
                "compensation": graph_compensation,
            },
            {
                "name": "relational_create",
                "action": relational_action,
                "compensation": relational_compensation,
            },
            {
                "name": "file_storage_create",
                "action": file_storage_action,
                "compensation": file_storage_compensation,
            },
            {
                "name": "identity_mapping",
                "action": identity_binding_action,
                "compensation": None,
            },
            {
                "name": "validation_and_finalize",
                "action": validation_action,
                "compensation": None,
            },
        ]

    def _build_update_document_step_specs(
        self, context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        update_result = context["update_result"]
        updates = context["updates"]

        def _mark_optional_skip(
            result_dict: Dict[str, Any], message: Optional[str]
        ) -> Dict[str, Any]:
            payload = dict(result_dict)
            payload["success"] = True
            payload["skipped"] = True
            if message:
                payload.setdefault("warning", message)
                update_result["issues"].append(message)
            return payload

        def _is_optional_backend_error(message: Optional[str]) -> bool:
            if not message:
                return False
            lowered = message.lower()
            return any(
                phrase in lowered
                for phrase in [
                    "nicht konfiguriert",
                    "nicht verfügbar",
                    "not configured",
                    "not available",
                ]
            )

        def _ensure_document_uuid(ctx: Dict[str, Any]) -> None:
            if ctx.get("document_uuid"):
                return
            mapping = self.document_mapping.get(ctx["document_id"])
            if mapping and mapping.get("uuid"):
                ctx["document_uuid"] = mapping["uuid"]
                if mapping.get("identity_key"):
                    ctx.setdefault("identity_key", mapping["identity_key"])
                    ctx.setdefault("aktenzeichen", mapping["identity_key"])
                return
            inferred = self._infer_uuid_from_document_id(ctx["document_id"])
            if inferred:
                ctx["document_uuid"] = inferred

        def load_current_state(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            document_id = ctx["document_id"]
            previous: Dict[str, Any] = {}
            issues = update_result["issues"]

            _ensure_document_uuid(ctx)

            if self.identity_service and ctx.get("document_uuid"):
                try:
                    record = self.identity_service.ensure_identity(
                        ctx["document_uuid"],
                        source_system="uds3.core",
                        status="existing",
                        actor="uds3_core",
                    )
                    previous["identity"] = {
                        "uuid": record.uuid,
                        "aktenzeichen": record.aktenzeichen,
                        "status": record.status,
                        "mappings": record.mappings,
                    }
                    ctx["document_uuid"] = record.uuid
                    ctx["identity_uuid"] = record.uuid
                    if record.aktenzeichen:
                        ctx["identity_key"] = record.aktenzeichen
                        ctx["aktenzeichen"] = record.aktenzeichen
                except IdentityServiceError as exc:
                    issues.append(f"Identity Lookup fehlgeschlagen: {exc}")

            relational_snapshot = self.saga_crud.relational_read(document_id)
            relational_payload = relational_snapshot.to_payload()
            previous["relational"] = relational_payload
            if relational_snapshot.success and relational_payload.get("records"):
                current_record = dict(relational_payload["records"][0])
                ctx["current_metadata"] = current_record
                ctx.setdefault("metadata", dict(current_record))
                aktenzeichen = current_record.get("aktenzeichen")
                if aktenzeichen:
                    ctx["identity_key"] = ctx.get("identity_key") or aktenzeichen
                    ctx["aktenzeichen"] = ctx.get("aktenzeichen") or aktenzeichen
                if current_record.get("uuid"):
                    ctx["identity_uuid"] = ctx.get(
                        "identity_uuid"
                    ) or current_record.get("uuid")
            else:
                issues.append(
                    relational_payload.get("error")
                    or "Relationale Daten nicht gefunden"
                )

            if self._requires_vector_update(updates):
                vector_snapshot = self.saga_crud.vector_read(document_id)
                previous["vector"] = vector_snapshot.to_payload()
                if not vector_snapshot.success and vector_snapshot.error:
                    issues.append(
                        f"Vector Snapshot fehlgeschlagen: {vector_snapshot.error}"
                    )

            if self._requires_graph_update(updates):
                mapping = self.document_mapping.get(document_id, {})
                identifier = mapping.get("graph_id") or f"Document::{document_id}"
                graph_snapshot = self.saga_crud.graph_read(identifier)
                graph_payload = graph_snapshot.to_payload()
                previous["graph"] = graph_payload
                if graph_snapshot.success:
                    ctx["graph_identifier"] = graph_payload.get(
                        "identifier", identifier
                    )
                elif graph_snapshot.error:
                    issues.append(
                        f"Graph Snapshot fehlgeschlagen: {graph_snapshot.error}"
                    )

            if self._requires_file_storage_update(updates):
                mapping = self.document_mapping.get(document_id, {})
                asset_id = mapping.get("file_storage_id") or f"fs_{document_id}"
                file_snapshot = self.saga_crud.file_read(asset_id)
                previous["file_storage"] = file_snapshot.to_payload()
                if file_snapshot.success:
                    ctx["file_asset_id"] = asset_id
                elif file_snapshot.error:
                    issues.append(
                        f"File Snapshot fehlgeschlagen: {file_snapshot.error}"
                    )

            ctx["previous_state"] = previous
            update_result["previous_state"] = previous
            return None

        def vector_update_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            if not self._requires_vector_update(updates):
                payload = _mark_optional_skip({"success": True}, None)
                update_result["database_operations"]["vector"] = payload
                ctx["vector_update_result"] = payload
                return None

            new_chunks = ctx["updates"].get("chunks") or []
            if not new_chunks and ctx["updates"].get("content"):
                new_chunks = [ctx["updates"]["content"]]
            if not new_chunks:
                previous_vector = (ctx.get("previous_state") or {}).get("vector", {})
                new_chunks = previous_vector.get("documents") or []
            metadata_payload: Dict[str, Any] = {}
            previous_vector = (ctx.get("previous_state") or {}).get("vector", {})
            metadatas = previous_vector.get("metadatas")
            if isinstance(metadatas, list) and metadatas:
                metadata_payload.update(metadatas[0] or {})
            metadata_payload.setdefault("document_id", ctx["document_id"])

            self._enforce_adapter_governance(
                "vector",
                OperationType.UPDATE.value,
                {
                    "document_id": ctx["document_id"],
                    "chunks": list(new_chunks or []),
                    "metadata": metadata_payload,
                },
            )

            crud_result = self.saga_crud.vector_update(
                ctx["document_id"], new_chunks, metadata_payload
            )
            payload = crud_result.to_payload()
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Vector Update fehlgeschlagen"
                    )

            ctx["vector_update_result"] = payload
            update_result["database_operations"]["vector"] = payload
            return None

        def vector_update_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("vector_update_result") or {}
            if payload.get("skipped"):
                return
            previous_vector = (ctx.get("previous_state") or {}).get("vector", {})
            chunks = previous_vector.get("documents") or []
            if not chunks:
                return
            metadata_payload: Dict[str, Any] = {}
            metadatas = previous_vector.get("metadatas")
            if isinstance(metadatas, list) and metadatas:
                metadata_payload.update(metadatas[0] or {})
            metadata_payload.setdefault("document_id", ctx["document_id"])
            self._enforce_adapter_governance(
                "vector",
                OperationType.UPDATE.value,
                {
                    "document_id": ctx["document_id"],
                    "chunks": list(chunks or []),
                    "metadata": metadata_payload,
                },
            )
            crud_result = self.saga_crud.vector_update(
                ctx["document_id"], chunks, metadata_payload
            )
            if not crud_result.success and crud_result.error:
                update_result["issues"].append(
                    f"Vector-Kompensation fehlgeschlagen: {crud_result.error}"
                )

        def graph_update_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            graph_updates = self._extract_graph_updates(updates)
            if not graph_updates:
                payload = _mark_optional_skip({"success": True}, None)
                update_result["database_operations"]["graph"] = payload
                ctx["graph_update_result"] = payload
                return None

            identifier = ctx.get("graph_identifier") or self.document_mapping.get(
                ctx["document_id"], {}
            ).get("graph_id")
            if not identifier:
                identifier = f"Document::{ctx['document_id']}"

            self._enforce_adapter_governance(
                "graph",
                OperationType.UPDATE.value,
                {
                    "identifier": identifier,
                    "updates": graph_updates,
                },
            )

            crud_result = self.saga_crud.graph_update(identifier, graph_updates)
            payload = crud_result.to_payload()
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Graph Update fehlgeschlagen"
                    )

            ctx["graph_update_result"] = payload
            update_result["database_operations"]["graph"] = payload
            return None

        def graph_update_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("graph_update_result") or {}
            if payload.get("skipped"):
                return
            previous_graph = (ctx.get("previous_state") or {}).get("graph", {})
            node_payload = previous_graph.get("node")
            if not node_payload:
                return
            properties = dict(node_payload)
            properties.setdefault("id", ctx["document_id"])
            self._enforce_adapter_governance(
                "graph",
                OperationType.CREATE.value,
                {
                    "document_id": ctx["document_id"],
                    "properties": properties,
                },
            )
            crud_result = self.saga_crud.graph_create(ctx["document_id"], properties)
            if not crud_result.success and crud_result.error:
                update_result["issues"].append(
                    f"Graph-Kompensation fehlgeschlagen: {crud_result.error}"
                )

        def relational_update_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            relational_updates = self._extract_relational_updates(updates)
            if not relational_updates:
                relational_updates = {"updated_at": datetime.now().isoformat()}

            self._enforce_adapter_governance(
                "relational",
                OperationType.UPDATE.value,
                {
                    "document_id": ctx["document_id"],
                    "updates": relational_updates,
                },
            )
            crud_result = self.saga_crud.relational_update(
                ctx["document_id"], relational_updates
            )
            payload = crud_result.to_payload()
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Relationale Operation fehlgeschlagen"
                    )

            ctx["relational_update_result"] = payload
            update_result["database_operations"]["relational"] = payload
            return None

        def relational_update_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("relational_update_result") or {}
            if payload.get("skipped"):
                return
            previous_relational = (ctx.get("previous_state") or {}).get(
                "relational", {}
            )
            records = previous_relational.get("records") or []
            if not records:
                return
            restore_payload = dict(records[0])
            restore_payload.pop("document_id", None)
            self._enforce_adapter_governance(
                "relational",
                OperationType.UPDATE.value,
                {
                    "document_id": ctx["document_id"],
                    "updates": restore_payload,
                },
            )
            restore_result = self.saga_crud.relational_update(
                ctx["document_id"], restore_payload
            )
            if not restore_result.success and restore_result.error:
                update_result["issues"].append(
                    f"Relationale Kompensation fehlgeschlagen: {restore_result.error}"
                )

        def file_update_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            if not self._requires_file_storage_update(updates):
                payload = _mark_optional_skip({"success": True}, None)
                update_result["database_operations"]["file_storage"] = payload
                ctx["file_update_result"] = payload
                return None

            asset_id = ctx.get("file_asset_id")
            self._enforce_adapter_governance(
                "file",
                OperationType.UPDATE.value,
                {
                    "document_id": ctx["document_id"],
                    "asset_id": asset_id,
                    "updates": ctx["updates"],
                },
            )
            payload = self._execute_file_storage_update(
                ctx["document_id"],
                new_file_path=ctx["updates"].get("file_path"),
                asset_id=asset_id,
                content=ctx["updates"].get("binary_content"),
            )
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "File-Storage Update fehlgeschlagen"
                    )

            ctx["file_update_result"] = payload
            update_result["database_operations"]["file_storage"] = payload
            return None

        def file_update_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("file_update_result") or {}
            if payload.get("skipped"):
                return
            previous_file = (ctx.get("previous_state") or {}).get("file_storage", {})
            restore_path = previous_file.get("path")
            asset_id = ctx.get("file_asset_id")
            if restore_path:
                restore_payload = self._execute_file_storage_update(
                    ctx["document_id"],
                    new_file_path=restore_path,
                    asset_id=asset_id,
                )
                if not restore_payload.get("success", False) and restore_payload.get(
                    "error"
                ):
                    update_result["issues"].append(
                        f"File-Storage Kompensation fehlgeschlagen: {restore_payload['error']}"
                    )

        def identity_update_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            if not self.identity_service:
                return None
            _ensure_document_uuid(ctx)
            document_uuid = ctx.get("document_uuid")
            if not document_uuid:
                return None
            try:
                aktenzeichen = ctx["updates"].get("aktenzeichen")
                if aktenzeichen:
                    record = self.identity_service.register_aktenzeichen(
                        document_uuid,
                        aktenzeichen,
                        actor="uds3_core",
                        status="updated",
                    )
                else:
                    record = self.identity_service.ensure_identity(
                        document_uuid,
                        source_system="uds3.core",
                        status="updated",
                        actor="uds3_core",
                    )

                mapping_record = self.identity_service.bind_backend_ids(
                    document_uuid,
                    relational_id=ctx.get("relational_update_result", {}).get(
                        "relational_id"
                    ),
                    graph_id=ctx.get("graph_update_result", {}).get("graph_id"),
                    vector_id=ctx.get("vector_update_result", {}).get("vector_id"),
                    file_storage_id=ctx.get("file_update_result", {}).get(
                        "file_storage_id"
                    ),
                    metadata={
                        "document_id": ctx["document_id"],
                        "updated_fields": list(ctx["updates"].keys()),
                    },
                    actor="uds3_core",
                )
                update_result["identity"] = {
                    "uuid": mapping_record.uuid,
                    "aktenzeichen": mapping_record.aktenzeichen,
                    "status": mapping_record.status,
                    "mappings": mapping_record.mappings,
                }
                ctx["document_uuid"] = mapping_record.uuid
            except IdentityServiceError as exc:
                update_result["issues"].append(f"Identity-Update fehlgeschlagen: {exc}")
            return None

        def validation_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            document_id = ctx["document_id"]

            def _read_state(read_fn) -> Dict[str, Any]:
                try:
                    result = read_fn()
                    payload = result.to_payload()
                    payload.setdefault("success", result.success)
                    return payload
                except Exception as exc:  # pragma: no cover - defensive
                    return {"success": False, "error": str(exc)}

            if self._requires_vector_update(updates):
                vector_state = _read_state(
                    lambda: self.saga_crud.vector_read(document_id)
                )
            else:
                vector_state = ctx.get(
                    "vector_update_result", {"success": True, "skipped": True}
                )

            if self._requires_graph_update(updates):
                identifier = ctx.get("graph_identifier") or self.document_mapping.get(
                    document_id, {}
                ).get("graph_id")
                identifier = identifier or f"Document::{document_id}"
                vector_identifier = identifier
                graph_state = _read_state(
                    lambda: self.saga_crud.graph_read(vector_identifier)
                )
            else:
                graph_state = ctx.get(
                    "graph_update_result", {"success": True, "skipped": True}
                )

            relational_state = _read_state(
                lambda: self.saga_crud.relational_read(document_id)
            )

            if self._requires_file_storage_update(updates):
                asset_id = ctx.get("file_asset_id") or f"fs_{document_id}"
                file_state = _read_state(lambda: self.saga_crud.file_read(asset_id))
            else:
                file_state = ctx.get("file_update_result")

            validation_result = self._validate_cross_db_consistency(
                document_id,
                vector_state or {},
                graph_state or {},
                relational_state or {},
                file_state or {},
            )
            update_result["validation_results"] = validation_result

            all_db_success = all(
                result.get("success", True)
                for result in update_result["database_operations"].values()
            )
            update_result["success"] = all_db_success and validation_result.get(
                "overall_valid", True
            )

            if update_result["success"]:
                mapping = self.document_mapping.setdefault(document_id, {})
                if ctx.get("document_uuid"):
                    mapping["uuid"] = ctx["document_uuid"]
                vector_id = ctx.get("vector_update_result", {}).get("vector_id")
                if vector_id is not None:
                    mapping["vector_id"] = vector_id
                graph_id = ctx.get("graph_update_result", {}).get("graph_id")
                if graph_id is not None:
                    mapping["graph_id"] = graph_id
                relational_id = ctx.get("relational_update_result", {}).get(
                    "relational_id"
                )
                if relational_id is not None:
                    mapping["relational_id"] = relational_id
                file_id = ctx.get("file_update_result", {}).get("file_storage_id")
                if file_id is not None:
                    mapping["file_storage_id"] = file_id
            return None

        return [
            {
                "name": "load_current_state",
                "action": load_current_state,
                "compensation": None,
            },
            {
                "name": "vector_update",
                "action": vector_update_action,
                "compensation": vector_update_compensation,
            },
            {
                "name": "graph_update",
                "action": graph_update_action,
                "compensation": graph_update_compensation,
            },
            {
                "name": "relational_update",
                "action": relational_update_action,
                "compensation": relational_update_compensation,
            },
            {
                "name": "file_storage_update",
                "action": file_update_action,
                "compensation": file_update_compensation,
            },
            {
                "name": "identity_update",
                "action": identity_update_action,
                "compensation": None,
            },
            {
                "name": "validation_and_finalize",
                "action": validation_action,
                "compensation": None,
            },
        ]

    def _build_delete_document_step_specs(
        self, context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        delete_result = context["delete_result"]

        def _mark_optional_skip(
            result_dict: Dict[str, Any], message: Optional[str]
        ) -> Dict[str, Any]:
            payload = dict(result_dict)
            payload["success"] = True
            payload["skipped"] = True
            if message:
                payload.setdefault("warning", message)
                delete_result["issues"].append(message)
            return payload

        def _is_optional_backend_error(message: Optional[str]) -> bool:
            if not message:
                return False
            lowered = message.lower()
            return any(
                phrase in lowered
                for phrase in [
                    "nicht konfiguriert",
                    "nicht verfügbar",
                    "not configured",
                    "not available",
                ]
            )

        def _ensure_document_uuid(ctx: Dict[str, Any]) -> None:
            if ctx.get("document_uuid"):
                return
            mapping = self.document_mapping.get(ctx["document_id"])
            if mapping and mapping.get("uuid"):
                ctx["document_uuid"] = mapping["uuid"]
                if mapping.get("identity_key"):
                    ctx.setdefault("identity_key", mapping["identity_key"])
                    ctx.setdefault("aktenzeichen", mapping["identity_key"])
                return
            inferred = self._infer_uuid_from_document_id(ctx["document_id"])
            if inferred:
                ctx["document_uuid"] = inferred

        def load_current_state(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            document_id = ctx["document_id"]
            previous: Dict[str, Any] = {}
            issues = delete_result["issues"]

            _ensure_document_uuid(ctx)

            if self.identity_service and ctx.get("document_uuid"):
                try:
                    record = self.identity_service.ensure_identity(
                        ctx["document_uuid"],
                        source_system="uds3.core",
                        status="existing",
                        actor="uds3_core",
                    )
                    previous["identity"] = {
                        "uuid": record.uuid,
                        "aktenzeichen": record.aktenzeichen,
                        "status": record.status,
                        "mappings": record.mappings,
                    }
                    ctx["document_uuid"] = record.uuid
                    ctx["identity_uuid"] = record.uuid
                    if record.aktenzeichen:
                        ctx["identity_key"] = record.aktenzeichen
                        ctx["aktenzeichen"] = record.aktenzeichen
                except IdentityServiceError as exc:
                    issues.append(f"Identity Lookup fehlgeschlagen: {exc}")

            relational_snapshot = self.saga_crud.relational_read(document_id)
            relational_payload = relational_snapshot.to_payload()
            previous["relational"] = relational_payload
            if relational_snapshot.success and relational_payload.get("records"):
                current_record = dict(relational_payload["records"][0])
                ctx["current_metadata"] = current_record
                ctx.setdefault("metadata", dict(current_record))
                aktenzeichen = current_record.get("aktenzeichen")
                if aktenzeichen:
                    ctx["identity_key"] = ctx.get("identity_key") or aktenzeichen
                    ctx["aktenzeichen"] = ctx.get("aktenzeichen") or aktenzeichen
                if current_record.get("uuid"):
                    ctx["identity_uuid"] = ctx.get(
                        "identity_uuid"
                    ) or current_record.get("uuid")
            else:
                issues.append(
                    relational_payload.get("error")
                    or "Relationale Daten nicht gefunden"
                )

            vector_snapshot = self.saga_crud.vector_read(document_id)
            previous["vector"] = vector_snapshot.to_payload()
            if not vector_snapshot.success and vector_snapshot.error:
                issues.append(
                    f"Vector Snapshot fehlgeschlagen: {vector_snapshot.error}"
                )

            mapping = self.document_mapping.get(document_id, {})
            identifier = mapping.get("graph_id") or f"Document::{document_id}"
            graph_snapshot = self.saga_crud.graph_read(identifier)
            graph_payload = graph_snapshot.to_payload()
            previous["graph"] = graph_payload
            if graph_snapshot.success:
                ctx["graph_identifier"] = graph_payload.get("identifier", identifier)
            elif graph_snapshot.error:
                issues.append(f"Graph Snapshot fehlgeschlagen: {graph_snapshot.error}")

            asset_id = mapping.get("file_storage_id") or f"fs_{document_id}"
            file_snapshot = self.saga_crud.file_read(asset_id)
            previous["file_storage"] = file_snapshot.to_payload()
            if file_snapshot.success:
                ctx["file_asset_id"] = asset_id
            elif file_snapshot.error:
                issues.append(f"File Snapshot fehlgeschlagen: {file_snapshot.error}")

            ctx["previous_state"] = previous
            delete_result["previous_state"] = previous
            return None

        def vector_delete_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            self._enforce_adapter_governance(
                "vector",
                OperationType.DELETE.value,
                {"document_id": ctx["document_id"]},
            )
            crud_result = self.saga_crud.vector_delete(ctx["document_id"])
            payload = crud_result.to_payload()
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Vector Delete fehlgeschlagen"
                    )
            ctx["vector_delete_result"] = payload
            delete_result["database_operations"]["vector"] = payload
            return None

        def vector_delete_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("vector_delete_result") or {}
            if payload.get("skipped"):
                return
            previous_vector = (ctx.get("previous_state") or {}).get("vector", {})
            chunks = previous_vector.get("documents") or []
            if not chunks:
                return
            metadata_payload: Dict[str, Any] = {}
            metadatas = previous_vector.get("metadatas")
            if isinstance(metadatas, list) and metadatas:
                metadata_payload.update(metadatas[0] or {})
            metadata_payload.setdefault("document_id", ctx["document_id"])
            crud_result = self.saga_crud.vector_update(
                ctx["document_id"], chunks, metadata_payload
            )
            if not crud_result.success and crud_result.error:
                delete_result["issues"].append(
                    f"Vector-Kompensation fehlgeschlagen: {crud_result.error}"
                )

        def graph_delete_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            identifier = ctx.get("graph_identifier") or self.document_mapping.get(
                ctx["document_id"], {}
            ).get("graph_id")
            if not identifier:
                identifier = f"Document::{ctx['document_id']}"
            self._enforce_adapter_governance(
                "graph",
                OperationType.DELETE.value,
                {"identifier": identifier},
            )
            crud_result = self.saga_crud.graph_delete(identifier)
            payload = crud_result.to_payload()
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Graph Delete fehlgeschlagen"
                    )
            ctx["graph_delete_result"] = payload
            delete_result["database_operations"]["graph"] = payload
            return None

        def graph_delete_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("graph_delete_result") or {}
            if payload.get("skipped"):
                return
            previous_graph = (ctx.get("previous_state") or {}).get("graph", {})
            node_payload = previous_graph.get("node")
            if not node_payload:
                return
            properties = dict(node_payload)
            properties.setdefault("id", ctx["document_id"])
            crud_result = self.saga_crud.graph_create(ctx["document_id"], properties)
            if not crud_result.success and crud_result.error:
                delete_result["issues"].append(
                    f"Graph-Kompensation fehlgeschlagen: {crud_result.error}"
                )

        def relational_delete_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            if ctx["soft_delete"]:
                updates = {"archived": True, "archived_at": datetime.now().isoformat()}
                self._enforce_adapter_governance(
                    "relational",
                    OperationType.UPDATE.value,
                    {
                        "document_id": ctx["document_id"],
                        "updates": updates,
                    },
                )
                crud_result = self.saga_crud.relational_update(
                    ctx["document_id"], updates
                )
            else:
                self._enforce_adapter_governance(
                    "relational",
                    OperationType.DELETE.value,
                    {"document_id": ctx["document_id"]},
                )
                crud_result = self.saga_crud.relational_delete(ctx["document_id"])
            payload = crud_result.to_payload()
            if ctx["soft_delete"]:
                payload["mode"] = "soft"
            else:
                payload["mode"] = "hard"
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "Relationale Delete fehlgeschlagen"
                    )
            ctx["relational_delete_result"] = payload
            delete_result["database_operations"]["relational"] = payload
            return None

        def relational_delete_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("relational_delete_result") or {}
            if payload.get("skipped"):
                return
            previous_relational = (ctx.get("previous_state") or {}).get(
                "relational", {}
            )
            records = previous_relational.get("records") or []
            if not records:
                return
            if ctx["soft_delete"]:
                restore_payload = dict(records[0])
                restore_payload.pop("document_id", None)
                restore_payload.pop("archived", None)
                restore_payload.pop("archived_at", None)
                self._enforce_adapter_governance(
                    "relational",
                    OperationType.UPDATE.value,
                    {
                        "document_id": ctx["document_id"],
                        "updates": restore_payload,
                    },
                )
                crud_result = self.saga_crud.relational_update(
                    ctx["document_id"], restore_payload
                )
            else:
                restore_payload = dict(records[0])
                self._enforce_adapter_governance(
                    "relational",
                    OperationType.CREATE.value,
                    restore_payload,
                )
                crud_result = self.saga_crud.relational_create(restore_payload)
            if not crud_result.success and crud_result.error:
                delete_result["issues"].append(
                    f"Relationale Kompensation fehlgeschlagen: {crud_result.error}"
                )

        def file_delete_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            payload = self._execute_file_storage_delete(
                ctx["document_id"],
                archive=ctx["soft_delete"],
                asset_id=ctx.get("file_asset_id"),
            )
            if not payload.get("success", False):
                error_message = payload.get("error")
                if _is_optional_backend_error(error_message):
                    payload = _mark_optional_skip(payload, error_message)
                else:
                    raise SagaExecutionError(
                        error_message or "File-Storage Delete fehlgeschlagen"
                    )
            ctx["file_delete_result"] = payload
            delete_result["database_operations"]["file_storage"] = payload
            return None

        def file_delete_compensation(ctx: Dict[str, Any]) -> None:
            payload = ctx.get("file_delete_result") or {}
            if payload.get("skipped"):
                return
            previous_file = (ctx.get("previous_state") or {}).get("file_storage", {})
            restore_path = previous_file.get("path")
            asset_id = ctx.get("file_asset_id")
            if restore_path:
                restore_payload = self._execute_file_storage_update(
                    ctx["document_id"],
                    new_file_path=restore_path,
                    asset_id=asset_id,
                )
                if not restore_payload.get("success", False) and restore_payload.get(
                    "error"
                ):
                    delete_result["issues"].append(
                        f"File-Storage Kompensation fehlgeschlagen: {restore_payload['error']}"
                    )

        def identity_retire_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            if not self.identity_service:
                return None
            _ensure_document_uuid(ctx)
            document_uuid = ctx.get("document_uuid")
            if not document_uuid:
                return None
            status = "archived" if ctx["soft_delete"] else "deleted"
            try:
                record = self.identity_service.ensure_identity(
                    document_uuid,
                    source_system="uds3.core",
                    status=status,
                    actor="uds3_core",
                )
                if not ctx["soft_delete"]:
                    self.identity_service.bind_backend_ids(
                        document_uuid,
                        relational_id=None,
                        graph_id=None,
                        vector_id=None,
                        file_storage_id=None,
                        metadata={"document_id": ctx["document_id"], "deleted": True},
                        actor="uds3_core",
                    )
                delete_result["identity"] = {
                    "uuid": record.uuid,
                    "aktenzeichen": record.aktenzeichen,
                    "status": record.status,
                    "mappings": record.mappings,
                }
                ctx["document_uuid"] = record.uuid
            except IdentityServiceError as exc:
                delete_result["issues"].append(f"Identity-Update fehlgeschlagen: {exc}")
            return None

        def finalize_action(ctx: Dict[str, Any]) -> Optional[Dict[str, Any]]:
            all_db_success = all(
                result.get("success", True)
                for result in delete_result["database_operations"].values()
            )
            delete_result["success"] = all_db_success

            if delete_result["success"]:
                if ctx["soft_delete"]:
                    mapping = self.document_mapping.setdefault(ctx["document_id"], {})
                    mapping["archived"] = True
                else:
                    self.document_mapping.pop(ctx["document_id"], None)
            return None

        return [
            {
                "name": "load_current_state",
                "action": load_current_state,
                "compensation": None,
            },
            {
                "name": "vector_delete",
                "action": vector_delete_action,
                "compensation": vector_delete_compensation,
            },
            {
                "name": "graph_delete",
                "action": graph_delete_action,
                "compensation": graph_delete_compensation,
            },
            {
                "name": "relational_delete",
                "action": relational_delete_action,
                "compensation": relational_delete_compensation,
            },
            {
                "name": "file_storage_delete",
                "action": file_delete_action,
                "compensation": file_delete_compensation,
            },
            {
                "name": "identity_retire",
                "action": identity_retire_action,
                "compensation": None,
            },
            {
                "name": "finalize_delete",
                "action": finalize_action,
                "compensation": None,
            },
        ]

    def _execute_saga_steps_locally(
        self, step_specs: List[Dict[str, Any]], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        executed: List[Dict[str, Any]] = []
        errors: List[str] = []
        compensation_errors: List[str] = []

        for spec in step_specs:
            try:
                result = spec["action"](context)
                if result:
                    context.update(result)
                executed.append(spec)
            except Exception as exc:  # pragma: no cover - Fehlerfall in Tests abgedeckt
                errors.append(
                    f"Lokaler Saga-Schritt '{spec['name']}' fehlgeschlagen: {exc}"
                )
                for previous in reversed(executed):
                    compensation = previous.get("compensation")
                    if compensation:
                        try:
                            compensation(context)
                        except Exception as comp_exc:  # pragma: no cover - Fehlerfall
                            compensation_errors.append(
                                f"Kompensation für '{previous['name']}' fehlgeschlagen: {comp_exc}"
                            )
                break

        if SagaStatus:
            if errors:
                status = (
                    SagaStatus.COMPENSATION_FAILED
                    if compensation_errors
                    else SagaStatus.COMPENSATED
                )
            else:
                status = SagaStatus.COMPLETED
        else:  # pragma: no cover - Fallback falls SagaStatus nicht verfügbar
            if errors:
                status = "compensation_failed" if compensation_errors else "compensated"
            else:
                status = "completed"

        return {
            "context": context,
            "errors": errors,
            "compensation_errors": compensation_errors,
            "status": status,
        }

    def update_document_operation(
        self,
        document_id: str,
        updates: Dict,
        sync_strategy: SyncStrategy = SyncStrategy.IMMEDIATE,
    ) -> Dict:
        """
        Erstellt eine UPDATE-Operation für ein bestehendes Dokument

        Args:
            document_id: ID des zu aktualisierenden Dokuments
            updates: Dictionary mit zu aktualisierenden Feldern
            sync_strategy: Synchronisationsstrategie zwischen DBs

        Returns:
            Dict: UPDATE-Operationsplan
        """
        operation_plan = {
            "operation_type": OperationType.UPDATE.value,
            "document_id": document_id,
            "sync_strategy": sync_strategy.value,
            "timestamp": datetime.now().isoformat(),
            "updates": updates,
            "databases": {},
            "validation": {},
            "rollback_plan": {},
        }

        # Vector DB Updates
        if self._requires_vector_update(updates):
            operation_plan["databases"]["vector"] = {
                "operations": [
                    {
                        "type": "update_embeddings",
                        "document_id": document_id,
                        "recompute_required": True,
                        "updates": self._extract_vector_updates(updates),
                        "validation": ["embedding_dimension_check"],
                    }
                ],
                "rollback": {
                    "type": "restore_previous_embeddings",
                    "backup_required": True,
                },
            }

        # Graph DB Updates
        if self._requires_graph_update(updates):
            operation_plan["databases"]["graph"] = {
                "operations": [
                    {
                        "type": "update_node_properties",
                        "node_id": document_id,
                        "properties": self._extract_graph_updates(updates),
                        "cascade_updates": self._determine_cascade_updates(updates),
                    }
                ],
                "rollback": {
                    "type": "restore_node_properties",
                    "snapshot_required": True,
                },
            }

        # Relational DB Updates
        if self._requires_relational_update(updates):
            operation_plan["databases"]["relational"] = {
                "operations": [
                    {
                        "type": "update_metadata",
                        "table": "documents_metadata",
                        "document_id": document_id,
                        "updates": self._extract_relational_updates(updates),
                        "triggers": ["update_statistics", "audit_log"],
                    }
                ],
                "rollback": {"type": "restore_record", "transaction_log": True},
            }

        # File Storage Updates (z.B. neue Version oder ersetzte Originaldatei)
        if self._requires_file_storage_update(updates):
            operation_plan["databases"]["file_storage"] = {
                "operations": [
                    {
                        "type": "store_new_version",
                        "document_id": document_id,
                        "file_path": updates.get("file_path"),
                        "versioning": True,
                        "retain_old_version": True,
                    }
                ],
                "rollback": {"type": "restore_previous_file_version"},
            }

        return operation_plan

    def delete_document_operation(
        self, document_id: str, soft_delete: bool = True, cascade_delete: bool = True
    ) -> Dict:
        """
        Erstellt eine DELETE-Operation für ein Dokument

        Args:
            document_id: ID des zu löschenden Dokuments
            soft_delete: Soft Delete (Archive) vs. Hard Delete
            cascade_delete: Auch verwandte Daten löschen

        Returns:
            Dict: DELETE-Operationsplan
        """
        operation_plan = {
            "operation_type": OperationType.DELETE.value
            if not soft_delete
            else OperationType.ARCHIVE.value,
            "document_id": document_id,
            "soft_delete": soft_delete,
            "cascade_delete": cascade_delete,
            "timestamp": datetime.now().isoformat(),
            "databases": {},
            "cleanup_operations": [],
            "rollback_plan": {},
        }

        # Vector DB Deletion
        operation_plan["databases"]["vector"] = {
            "operations": [
                {
                    "type": "delete_embeddings"
                    if not soft_delete
                    else "archive_embeddings",
                    "document_id": document_id,
                    "cascade_chunks": cascade_delete,
                    "cleanup_orphaned_vectors": True,
                }
            ],
            "rollback": {
                "type": "restore_embeddings",
                "backup_location": f"backup_vectors_{document_id}",
            },
        }

        # Graph DB Deletion
        operation_plan["databases"]["graph"] = {
            "operations": [
                {
                    "type": "delete_node" if not soft_delete else "archive_node",
                    "node_id": document_id,
                    "cascade_relationships": cascade_delete,
                    "orphan_cleanup": True,
                }
            ],
            "rollback": {
                "type": "restore_node_and_relationships",
                "snapshot_required": True,
            },
        }

        # Relational DB Deletion
        operation_plan["databases"]["relational"] = {
            "operations": [
                {
                    "type": "soft_delete_record"
                    if soft_delete
                    else "hard_delete_record",
                    "table": "documents_metadata",
                    "document_id": document_id,
                    "audit_trail": True,
                    "cascade_tables": ["keywords_index", "processing_statistics"]
                    if cascade_delete
                    else [],
                }
            ],
            "rollback": {"type": "restore_record", "transaction_log": True},
        }

        # File Storage Deletion
        operation_plan["databases"]["file_storage"] = {
            "operations": [
                {
                    "type": "archive_file" if soft_delete else "delete_file",
                    "document_id": document_id,
                    "purge_derivatives": cascade_delete,
                    "store_manifest": soft_delete,
                }
            ],
            "rollback": {"type": "restore_file_from_archive", "integrity_check": True},
        }

        return operation_plan

    def batch_operation(
        self, operations: List[Dict], operation_type: OperationType
    ) -> Dict:
        """
        Erstellt eine Batch-Operation für mehrere Dokumente

        Args:
            operations: Liste von einzelnen Operationen
            operation_type: Typ der Batch-Operation

        Returns:
            Dict: Batch-Operationsplan
        """
        batch_plan = {
            "operation_type": operation_type.value,
            "batch_id": f"batch_{str(uuid.uuid4())}",  # Vollständige UUID für Eindeutigkeit
            "operation_count": len(operations),
            "timestamp": datetime.now().isoformat(),
            "batch_config": {},
            "databases": {
                "vector": {
                    "batches": [],
                    "batch_size": self.optimization_config.batch_sizes["vector"],
                },
                "graph": {
                    "batches": [],
                    "batch_size": self.optimization_config.batch_sizes["graph"],
                },
                "relational": {
                    "batches": [],
                    "batch_size": self.optimization_config.batch_sizes["relational"],
                },
            },
            "error_handling": {
                "continue_on_error": False,
                "rollback_on_failure": True,
                "partial_success_handling": "manual_review",
            },
        }

        # Gruppiere Operationen nach DB-Typ und Batch-Größe
        for db_type, config in batch_plan["databases"].items():
            db_operations = self._extract_db_operations(operations, db_type)
            batches = self._create_batches(db_operations, config["batch_size"])
            config["batches"] = batches

        return batch_plan

    def read_document_operation(
        self,
        document_id: str,
        include_content: bool = True,
        include_relationships: bool = False,
    ) -> Dict:
        """
        Erstellt eine READ-Operation für ein Dokument

        Args:
            document_id: ID des zu lesenden Dokuments
            include_content: Vollständigen Content einschließen
            include_relationships: Graph-Beziehungen einschließen

        Returns:
            Dict: READ-Operationsplan
        """
        read_plan = {
            "operation_type": OperationType.READ.value,
            "document_id": document_id,
            "include_content": include_content,
            "include_relationships": include_relationships,
            "timestamp": datetime.now().isoformat(),
            "databases": {},
        }

        # Vector DB Read
        read_plan["databases"]["vector"] = {
            "operations": [
                {
                    "type": "get_embeddings",
                    "document_id": document_id,
                    "include_chunks": include_content,
                    "similarity_threshold": 0.0,  # Get all chunks
                }
            ]
        }

        # Graph DB Read
        read_plan["databases"]["graph"] = {
            "operations": [
                {
                    "type": "get_node_with_relationships",
                    "node_id": document_id,
                    "include_relationships": include_relationships,
                    "max_depth": 2 if include_relationships else 0,
                }
            ]
        }

        # Relational DB Read
        read_plan["databases"]["relational"] = {
            "operations": [
                {
                    "type": "get_document_metadata",
                    "document_id": document_id,
                    "include_keywords": include_content,
                    "include_statistics": True,
                }
            ]
        }

        # File Storage Read
        read_plan["databases"]["file_storage"] = {
            "operations": [
                {
                    "type": "get_file_info",
                    "document_id": document_id,
                    "include_binary": False,
                    "include_versions": True,
                }
            ]
        }

        return read_plan

    # ========== HELPER METHODS FOR CRUD OPERATIONS ==========

    def _requires_vector_update(self, updates: Dict) -> bool:
        """Prüft ob Updates Vector DB betreffen"""
        vector_fields = ["content", "chunks", "text", "title"]
        return any(field in updates for field in vector_fields)

    def _requires_graph_update(self, updates: Dict) -> bool:
        """Prüft ob Updates Graph DB betreffen"""
        graph_fields = ["relationships", "tags", "author", "citations", "rechtsgebiet"]
        return any(field in updates for field in graph_fields)

    def _requires_relational_update(self, updates: Dict) -> bool:
        """Prüft ob Updates Relational DB betreffen"""
        relational_fields = [
            "metadata",
            "keywords",
            "file_path",
            "file_hash",
            "behoerde",
        ]
        return any(field in updates for field in relational_fields)

    def _requires_file_storage_update(self, updates: Dict) -> bool:
        """Prüft ob Updates File Storage betreffen"""
        file_fields = ["file_path", "binary_content", "new_file_version"]
        return any(field in updates for field in file_fields)

    def _extract_vector_updates(self, updates: Dict) -> Dict:
        """Extrahiert Vector-relevante Updates"""
        vector_updates: dict[Any, Any] = {}

        if "content" in updates or "chunks" in updates:
            vector_updates["recompute_embeddings"] = True
            vector_updates["new_content"] = updates.get("content", "")
            vector_updates["new_chunks"] = updates.get("chunks", [])

        if "title" in updates:
            vector_updates["update_summary_embedding"] = True
            vector_updates["new_title"] = updates["title"]

        return vector_updates

    def _extract_graph_updates(self, updates: Dict) -> Dict:
        """Extrahiert Graph-relevante Updates"""
        graph_updates: dict[Any, Any] = {}

        for field in ["author", "rechtsgebiet", "tags"]:
            if field in updates:
                graph_updates[field] = updates[field]

        if "citations" in updates:
            graph_updates["update_relationships"] = True
            graph_updates["new_citations"] = updates["citations"]

        return graph_updates

    def _extract_relational_updates(self, updates: Dict) -> Dict:
        """Extrahiert Relational-relevante Updates"""
        relational_updates: dict[Any, Any] = {}

        metadata_fields = ["file_path", "file_hash", "behoerde", "document_type"]
        for field in metadata_fields:
            if field in updates:
                relational_updates[field] = updates[field]

        if "keywords" in updates:
            relational_updates["update_keywords"] = True
            relational_updates["new_keywords"] = updates["keywords"]

        relational_updates["updated_at"] = datetime.now().isoformat()

        return relational_updates

    def _determine_cascade_updates(self, updates: Dict) -> List[str]:
        """Bestimmt welche Cascade-Updates erforderlich sind"""
        cascade_updates: list[Any] = []

        if "author" in updates:
            cascade_updates.append("author_relationships")

        if "rechtsgebiet" in updates:
            cascade_updates.append("concept_relationships")

        if "citations" in updates:
            cascade_updates.append("citation_network")

        return cascade_updates

    def _extract_db_operations(
        self, operations: List[Dict], db_type: str
    ) -> List[Dict]:
        """Extrahiert DB-spezifische Operationen aus einer Operation-Liste"""
        db_operations: list[Any] = []

        for operation in operations:
            if "databases" in operation and db_type in operation["databases"]:
                db_operations.extend(
                    operation["databases"][db_type].get("operations", [])
                )

        return db_operations

    def _create_batches(
        self, operations: List[Dict], batch_size: int
    ) -> List[List[Dict]]:
        """Teilt Operationen in optimale Batches auf"""
        batches: list[Any] = []

        for i in range(0, len(operations), batch_size):
            batch = operations[i : i + batch_size]
            batches.append(
                {
                    "batch_id": f"batch_{i // batch_size + 1}",
                    "operations": batch,
                    "size": len(batch),
                }
            )

        return batches

    # ========== VALIDATION & CONSISTENCY ==========

    def validate_cross_db_consistency(self, document_id: str) -> Dict:
        """
        Validiert Konsistenz eines Dokuments über alle Datenbanken

        Args:
            document_id: ID des zu validierenden Dokuments

        Returns:
            Dict: Validation-Ergebnis
        """
        validation_result = {
            "document_id": document_id,
            "timestamp": datetime.now().isoformat(),
            "consistent": True,
            "issues": [],
            "databases": {
                "vector": {"exists": False, "chunk_count": 0},
                "graph": {"exists": False, "relationship_count": 0},
                "relational": {"exists": False, "metadata_complete": False},
            },
        }

        # Vector DB Validation
        vector_validation = self._validate_vector_consistency(document_id)
        validation_result["databases"]["vector"] = vector_validation

        # Graph DB Validation
        graph_validation = self._validate_graph_consistency(document_id)
        validation_result["databases"]["graph"] = graph_validation

        # Relational DB Validation
        relational_validation = self._validate_relational_consistency(document_id)
        validation_result["databases"]["relational"] = relational_validation

        # Cross-DB Consistency Checks
        consistency_issues = self._check_cross_db_consistency(
            vector_validation, graph_validation, relational_validation
        )

        if consistency_issues:
            validation_result["consistent"] = False
            validation_result["issues"] = consistency_issues

        return validation_result

    def _validate_vector_consistency(self, document_id: str) -> Dict:
        """Validiert Vector DB Konsistenz"""
        return {
            "exists": True,  # Mock implementation
            "chunk_count": 5,
            "embedding_dimensions": 1536,
            "issues": [],
        }

    def _validate_graph_consistency(self, document_id: str) -> Dict:
        """Validiert Graph DB Konsistenz"""
        return {
            "exists": True,  # Mock implementation
            "relationship_count": 3,
            "node_properties_complete": True,
            "orphaned_relationships": 0,
            "issues": [],
        }

    def _validate_relational_consistency(self, document_id: str) -> Dict:
        """Validiert Relational DB Konsistenz"""
        return {
            "exists": True,  # Mock implementation
            "metadata_complete": True,
            "keyword_count": 15,
            "foreign_key_violations": 0,
            "issues": [],
        }

    def _check_cross_db_consistency(
        self, vector_val: Dict, graph_val: Dict, relational_val: Dict
    ) -> List[str]:
        """Prüft Konsistenz zwischen den Datenbanken"""
        issues: list[Any] = []

        # Existence Check
        if not all(
            [vector_val["exists"], graph_val["exists"], relational_val["exists"]]
        ):
            issues.append("Document exists in some but not all databases")

        # Data Completeness Check
        if vector_val["chunk_count"] == 0 and relational_val["keyword_count"] > 0:
            issues.append("Content inconsistency: keywords exist but no vector chunks")

        return issues

    # ================================================================
    # UDS3 RELATIONS INTEGRATION
    # ================================================================

    def create_uds3_relation(
        self,
        relation_type: str,
        source_id: str,
        target_id: str,
        properties: Optional[Dict[Any, Any]] = None,
    ) -> Dict:
        """
        Erstellt eine UDS3-konforme Relation

        Args:
            relation_type: UDS3-Relation-Typ aus Almanach
            source_id: ID des Quell-Nodes
            target_id: ID des Ziel-Nodes
            properties: Zusätzliche Properties

        Returns:
            Dict: UDS3-Relation-Erstellungsergebnis mit Database-Operations
        """
        if not self.relations_enabled:
            return {
                "success": False,
                "error": "UDS3 Relations Framework nicht verfügbar",
                "database_operations": {},
            }

        # Erstelle Relation über UDS3 Framework
        relation_result = self.relations_framework.create_relation_instance(
            relation_type, source_id, target_id, properties
        )

        if relation_result["success"]:
            # Erweitere mit UDS3-spezifischen Database-Operations
            relation_result["uds3_database_operations"] = (
                self._enhance_database_operations(
                    relation_result["database_operations"], relation_type
                )
            )

            logger.info(
                f"✅ UDS3 Relation erstellt: {relation_type} ({source_id} -> {target_id})"
            )

        return relation_result

    def get_uds3_relation_schema(self, relation_type: Optional[str] = None) -> Dict:
        """
        Holt UDS3-Relations-Schema

        Args:
            relation_type: Spezifischer Relation-Typ oder None für alle

        Returns:
            Dict: UDS3-Relations-Schema
        """
        if not self.relations_enabled:
            return {"error": "UDS3 Relations Framework nicht verfügbar"}

        if relation_type:
            return self.relations_framework.get_relation_definition(relation_type)
        else:
            return {
                "total_relations": len(self.relations_framework.almanach.relations),
                "performance_stats": self.relations_framework.get_performance_stats(),
                "database_schemas": {
                    "graph": self.relations_framework.get_relation_schema_for_database(
                        "graph"
                    ),
                    "vector": self.relations_framework.get_relation_schema_for_database(
                        "vector"
                    ),
                    "relational": self.relations_framework.get_relation_schema_for_database(
                        "relational"
                    ),
                },
            }

    def validate_uds3_relations_consistency(self) -> Dict:
        """
        Validiert UDS3-Relations-Konsistenz über alle Datenbanken

        Returns:
            Dict: UDS3-Relations-Konsistenz-Report
        """
        if not self.relations_enabled:
            return {"error": "UDS3 Relations Framework nicht verfügbar"}

        validation_result = {
            "timestamp": datetime.now().isoformat(),
            "uds3_consistent": True,
            "framework_stats": self.relations_framework.get_performance_stats(),
            "database_consistency": {},
            "recommendations": [],
        }

        # Prüfe Database-spezifische Konsistenz
        for db_type in ["graph", "vector", "relational"]:
            schema = self.relations_framework.get_relation_schema_for_database(db_type)

            validation_result["database_consistency"][db_type] = {
                "total_relations": schema["total_relations"],
                "constraints_needed": len(schema["constraints"]),
                "indexes_needed": len(schema["indexes"]),
                "optimizations_available": len(schema["optimizations"]),
            }

            # Empfehlungen basierend auf Schema
            if schema["total_relations"] > 10 and len(schema["indexes"]) == 0:
                validation_result["recommendations"].append(
                    f"Erstelle Indexes für {db_type} DB für bessere Performance"
                )

        return validation_result

    def _enhance_database_operations(
        self, base_operations: Dict, relation_type: str
    ) -> Dict:
        """
        Erweitert Database-Operations mit UDS3-spezifischen Optimierungen

        Args:
            base_operations: Basis Database-Operations
            relation_type: Relation-Typ für spezifische Optimierungen

        Returns:
            Dict: Erweiterte Database-Operations
        """
        enhanced_operations = base_operations.copy()

        # UDS3-spezifische Erweiterungen
        if relation_type.startswith("UDS3_LEGAL_"):
            # Legal Relations bekommen zusätzliche Compliance-Checks
            if "graph" in enhanced_operations:
                enhanced_operations["graph"]["compliance_validation"] = True
                enhanced_operations["graph"]["legal_audit_trail"] = True

        elif relation_type.startswith("UDS3_SEMANTIC_"):
            # Semantic Relations bekommen Vector-DB Updates
            if "vector" not in enhanced_operations:
                enhanced_operations["vector"] = {
                    "operation": "UPDATE_SEMANTIC_INDEX",
                    "relation_type": relation_type,
                }

        # Performance-Optimierungen für kritische Relations
        if relation_type in ["PART_OF", "CONTAINS_CHUNK", "NEXT_CHUNK"]:
            for db_ops in enhanced_operations.values():
                if isinstance(db_ops, dict):
                    db_ops["performance_priority"] = "high"
                    db_ops["batch_optimization"] = True

        return enhanced_operations

    def export_uds3_schema_for_databases(self) -> Dict[str, str]:
        """
        Exportiert UDS3-Schema für alle Database-Typen

        Returns:
            Dict: Database-spezifische Schema-Exports
        """
        if not self.relations_enabled:
            return {"error": "UDS3 Relations Framework nicht verfügbar"}

        exports: dict[Any, Any] = {}

        try:
            # Neo4j Cypher Schema
            graph_schema = self.relations_framework.get_relation_schema_for_database(
                "graph"
            )
            exports["neo4j_cypher"] = self._generate_neo4j_schema(graph_schema)

            # Vector DB Schema
            vector_schema = self.relations_framework.get_relation_schema_for_database(
                "vector"
            )
            exports["vector_schema"] = self._generate_vector_schema(vector_schema)

            # SQL Schema
            relational_schema = (
                self.relations_framework.get_relation_schema_for_database("relational")
            )
            exports["sql_schema"] = self._generate_sql_schema(relational_schema)

        except Exception as e:
            exports["error"] = f"Schema-Export fehlgeschlagen: {str(e)}"

        return exports

    def _generate_neo4j_schema(self, graph_schema: Dict) -> str:
        """Generiert Neo4j Cypher Schema"""
        cypher_lines: list[Any] = []
        cypher_lines.append("// UDS3 Relations Schema für Neo4j")
        cypher_lines.append("// Auto-generiert vom UDS3 Core")
        cypher_lines.append("")

        # Constraints
        for constraint in graph_schema["constraints"]:
            cypher_lines.append(f"// Constraint: {constraint}")
            cypher_lines.append(
                f"CREATE CONSTRAINT {constraint}_constraint IF NOT EXISTS FOR (n:Node) REQUIRE n.id IS UNIQUE;"
            )

        cypher_lines.append("")

        # Indexes
        for index in graph_schema["indexes"]:
            cypher_lines.append(f"// Index: {index}")
            cypher_lines.append(
                f"CREATE INDEX {index}_index IF NOT EXISTS FOR ()-[r:{index.upper()}]-() ON (r.uds3_created_at);"
            )

        return "\n".join(cypher_lines)

    def _generate_vector_schema(self, vector_schema: Dict) -> str:
        """Generiert Vector DB Schema"""
        schema_lines: list[Any] = []
        schema_lines.append("# UDS3 Relations Schema für Vector DB")
        schema_lines.append("# Collections und Metadaten-Struktur")
        schema_lines.append("")

        for relation_name in vector_schema["relations"]:
            schema_lines.append(f"# Relation: {relation_name}")
            schema_lines.append(f"collection_{relation_name.lower()}_metadata:")
            schema_lines.append("  - relation_id: string")
            schema_lines.append(f"  - relation_type: {relation_name}")
            schema_lines.append("  - source_id: string")
            schema_lines.append("  - target_id: string")
            schema_lines.append("")

        return "\n".join(schema_lines)

    def _generate_sql_schema(self, relational_schema: Dict) -> str:
        """Generiert SQL Schema"""
        sql_lines: list[Any] = []
        sql_lines.append("-- UDS3 Relations Schema für SQL Database")
        sql_lines.append("-- Relations Metadata Tabellen")
        sql_lines.append("")

        # Haupttabelle für Relations
        sql_lines.append("CREATE TABLE IF NOT EXISTS uds3_relations (")
        sql_lines.append("    instance_id VARCHAR(32) PRIMARY KEY,")
        sql_lines.append("    relation_type VARCHAR(100) NOT NULL,")
        sql_lines.append("    source_id VARCHAR(32) NOT NULL,")
        sql_lines.append("    target_id VARCHAR(32) NOT NULL,")
        sql_lines.append("    properties_json TEXT,")
        sql_lines.append("    uds3_priority VARCHAR(20),")
        sql_lines.append("    performance_weight DECIMAL(4,2),")
        sql_lines.append("    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,")
        sql_lines.append("    INDEX idx_relation_type (relation_type),")
        sql_lines.append("    INDEX idx_source_target (source_id, target_id)")
        sql_lines.append(");")
        sql_lines.append("")

        return "\n".join(sql_lines)

    # ================================================================
    # CRUD STRATEGIES - MOVED TO uds3_crud_strategies.py (UDS3CRUDStrategiesMixin)
    # ================================================================
    # _create_crud_strategies() → UDS3CRUDStrategiesMixin
    # _create_conflict_resolution_rules() → UDS3CRUDStrategiesMixin
    #
    # EXTRACTION: Lines 3194-3394 (~200 LOC)
    # ================================================================

    def _create_vector_operations(
        self, document_id: str, chunks: List[str], metadata: Dict
    ) -> Dict:
        """Erstellt Vector-DB-spezifische Operationen"""
        return {
            "operations": [
                {
                    "type": "create_embeddings",
                    "collection": "document_chunks",
                    "data": [
                        {
                            "id": f"{document_id}_chunk_{i:04d}",
                            "document_id": document_id,
                            "content": chunk,
                            "chunk_index": i,
                            "content_preview": chunk[:200],
                            "chunk_type": self._determine_chunk_type(chunk),
                            "content_length": len(chunk),
                            "metadata": {
                                "rechtsgebiet": metadata.get("rechtsgebiet"),
                                "created_at": datetime.now().isoformat(),
                            },
                        }
                        for i, chunk in enumerate(chunks)
                    ],
                },
                {
                    "type": "create_summary_embedding",
                    "collection": "document_summaries",
                    "data": {
                        "id": f"{document_id}_summary",
                        "document_id": document_id,
                        "summary_text": chunks[0][:500]
                        if chunks
                        else "",  # Erste 500 Zeichen als Summary
                        "key_topics": self._extract_key_topics(chunks),
                        "document_type": metadata.get("document_type", "unknown"),
                    },
                },
            ],
            "batch_config": {
                "batch_size": self.optimization_config.batch_sizes["vector"],
                "parallel_processing": True,
            },
        }

    def _create_graph_operations(
        self, document_id: str, content: str, metadata: Dict
    ) -> Dict:
        """Erstellt Graph-DB-spezifische Operationen"""
        return {
            "operations": [
                {
                    "type": "create_document_node",
                    "node_type": "Document",
                    "properties": {
                        "id": document_id,
                        "title": metadata.get("title", ""),
                        "file_path": metadata.get("file_path", ""),
                        "file_hash": metadata.get("file_hash", ""),
                        "rechtsgebiet": metadata.get("rechtsgebiet"),
                        "behoerde": metadata.get("behoerde"),
                        "document_type": metadata.get("document_type"),
                        "created_at": datetime.now().isoformat(),
                    },
                },
                {
                    "type": "create_author_relationships",
                    "relationships": self._extract_author_relationships(
                        content, metadata
                    ),
                },
                {
                    "type": "create_concept_relationships",
                    "relationships": self._extract_concept_relationships(
                        content, metadata
                    ),
                },
                {
                    "type": "create_citation_relationships",
                    "relationships": self._extract_citations(content),
                },
            ],
            "batch_config": {
                "batch_size": self.optimization_config.batch_sizes["graph"],
                "transaction_size": 50,
            },
        }

    def _create_relational_operations(
        self, document_id: str, file_path: str, content: str, metadata: Dict
    ) -> Dict:
        """Erstellt Relational-DB-spezifische Operationen"""
        return {
            "operations": [
                {
                    "type": "insert_metadata",
                    "table": "documents_metadata",
                    "data": {
                        "document_id": document_id,
                        "title": metadata.get("title", os.path.basename(file_path)),
                        "file_path": file_path,
                        "file_hash": metadata.get("file_hash", ""),
                        "file_size": len(content.encode("utf-8")),
                        "rechtsgebiet": metadata.get("rechtsgebiet"),
                        "behoerde": metadata.get("behoerde"),
                        "document_type": metadata.get("document_type"),
                        "chunk_count": len(content.split("\n\n")),  # Geschätzte Chunks
                        "processing_status": "processing",
                    },
                },
                {
                    "type": "insert_keywords",
                    "table": "keywords_index",
                    "data": self._extract_keywords(document_id, content),
                },
                {
                    "type": "insert_statistics",
                    "table": "processing_statistics",
                    "data": {
                        "document_id": document_id,
                        "processing_stage": "initial_processing",
                        "tokens_processed": len(content.split()),
                        "status": "in_progress",
                    },
                },
            ],
            "batch_config": {
                "batch_size": self.optimization_config.batch_sizes["relational"],
                "use_bulk_insert": True,
            },
        }

    def _create_sync_plan(self, document_id: str) -> Dict:
        """Erstellt Synchronisationsplan zwischen den Datenbanken"""
        return {
            "phases": [
                {
                    "phase": "initialization",
                    "order": ["relational", "graph", "vector"],
                    "dependencies": {
                        "graph": ["relational"],  # Graph braucht Metadaten
                        "vector": ["relational"],  # Vector braucht document_id
                    },
                },
                {
                    "phase": "relationship_building",
                    "order": ["graph", "relational"],
                    "operations": ["update_relationship_counts", "update_statistics"],
                },
                {
                    "phase": "finalization",
                    "order": ["relational"],
                    "operations": ["update_processing_status"],
                },
            ]
        }

    def _create_optimization_hints(self, chunks: List[str], metadata: Dict) -> Dict:
        """Erstellt Optimierungshinweise für die Verarbeitung"""
        return {
            "vector_optimization": {
                "embedding_priority": "high" if len(chunks) < 50 else "batch",
                "similarity_indexing": "immediate"
                if metadata.get("priority") == "high"
                else "deferred",
            },
            "graph_optimization": {
                "relationship_indexing": "deferred",
                "traversal_caching": True if len(chunks) > 100 else False,
            },
            "relational_optimization": {
                "index_rebuild": False,  # Nur bei größeren Batches
                "statistics_update": "immediate",
            },
        }

    # Helper Methods
    def _generate_document_id(self, file_path: str, content_preview: str) -> str:
        """Generiert eindeutige Document-ID"""
        hash_input = (
            f"{os.path.abspath(file_path).replace('\\', '/')}:{content_preview[:1000]}"
        )
        return f"doc_{hashlib.sha256(hash_input.encode('utf-8')).hexdigest()[:16]}"

    def _format_document_id(self, document_uuid: str) -> str:
        """Erzeugt ein standardisiertes document_id-Format aus einer UUID."""
        normalized = document_uuid.replace("-", "").lower()
        return f"doc_{normalized}"

    def _infer_uuid_from_document_id(self, document_id: Optional[str]) -> Optional[str]:
        """Leitet eine UUID aus einer document_id (doc_<hex>) ab."""
        if not document_id:
            return None
        if document_id.startswith("doc_"):
            raw = document_id[4:]
            if len(raw) == 32:
                return f"{raw[0:8]}-{raw[8:12]}-{raw[12:16]}-{raw[16:20]}-{raw[20:]}"
        return document_id

    def _determine_chunk_type(self, chunk: str) -> str:
        """Bestimmt den Typ eines Text-Chunks"""
        if chunk.strip().startswith("#") or len(chunk) < 100:
            return "heading"
        elif "|" in chunk and "\n" in chunk:
            return "table"
        elif len(chunk) > 2000:
            return "long_paragraph"
        else:
            return "paragraph"

    def _extract_key_topics(self, chunks: List[str]) -> List[str]:
        """Extrahiert Schlüssel-Topics aus Chunks"""
        # Vereinfachte Implementation
        topics = set()
        legal_terms = ["recht", "gesetz", "urteil", "verordnung", "beschluss"]

        for chunk in chunks[:3]:  # Nur erste 3 Chunks analysieren
            words = chunk.lower().split()
            for term in legal_terms:
                if term in " ".join(words):
                    topics.add(term)

        return list(topics)[:5]  # Maximal 5 Topics

    def _extract_author_relationships(self, content: str, metadata: Dict) -> List[Dict]:
        """Extrahiert Autor-Beziehungen"""
        relationships: list[Any] = []

        if "author" in metadata:
            relationships.append(
                {
                    "type": "AUTHORED_BY",
                    "from_node": {"type": "Document", "id": "current_document"},
                    "to_node": {"type": "Author", "name": metadata["author"]},
                    "properties": {"role": "author", "confidence": 1.0},
                }
            )

        return relationships

    def _extract_concept_relationships(
        self, content: str, metadata: Dict
    ) -> List[Dict]:
        """Extrahiert Konzept-Beziehungen"""
        relationships: list[Any] = []

        if "rechtsgebiet" in metadata:
            relationships.append(
                {
                    "type": "RELATES_TO",
                    "from_node": {"type": "Document", "id": "current_document"},
                    "to_node": {"type": "Concept", "term": metadata["rechtsgebiet"]},
                    "properties": {
                        "relevance_score": 0.9,
                        "extraction_method": "metadata",
                    },
                }
            )

        return relationships

    def _extract_citations(self, content: str) -> List[Dict]:
        """Extrahiert Zitate und Verweise"""
        # Vereinfachte Implementation - in der Praxis würde hier NLP verwendet
        citations: list[Any] = []

        # Suche nach Aktenzeichen-Patterns
        import re

        citation_pattern = r"\d+\s+[A-Z]+\s+\d+/\d+"
        matches = re.findall(citation_pattern, content)

        for match in matches[:5]:  # Maximal 5 Zitate
            citations.append(
                {
                    "type": "CITES",
                    "from_node": {"type": "Document", "id": "current_document"},
                    "to_node": {"type": "Document", "aktenzeichen": match.strip()},
                    "properties": {
                        "citation_type": "reference",
                        "context": "legal_reference",
                    },
                }
            )

        return citations

    def _extract_keywords(self, document_id: str, content: str) -> List[Dict]:
        """Extrahiert Keywords für Relational DB"""
        keywords: list[Any] = []

        # Vereinfachte Keyword-Extraktion
        words = content.lower().split()
        word_freq: dict[Any, Any] = {}

        for word in words:
            if len(word) > 4 and word.isalpha():  # Nur Wörter > 4 Zeichen
                word_freq[word] = word_freq.get(word, 0) + 1

        # Top 20 häufigste Wörter
        for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[
            :20
        ]:
            keywords.append(
                {
                    "document_id": document_id,
                    "keyword": word,
                    "frequency": freq,
                    "context_type": "content",
                    "extraction_method": "frequency",
                    "confidence": min(1.0, freq / 10.0),  # Normalisierte Confidence
                }
            )

        return keywords

    # Security & Quality Helper Methods
    def _execute_vector_create(
        self,
        document_id: str,
        content: str,
        chunks: List[str],
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict:
        metadata_payload = dict(metadata or {})
        if content:
            metadata_payload.setdefault("content_preview", content[:200])
        self._enforce_adapter_governance(
            "vector",
            OperationType.CREATE.value,
            {
                "document_id": document_id,
                "chunks": list(chunks or []),
                "metadata": metadata_payload,
            },
        )
        result = self.saga_crud.vector_create(document_id, chunks, metadata_payload)
        return result.to_payload()

    def _execute_graph_create(
        self, document_id: str, content: str, metadata: Dict
    ) -> Dict:
        properties = dict(metadata or {})
        if content:
            properties.setdefault("content_preview", content[:200])
        properties.setdefault("id", document_id)
        self._enforce_adapter_governance(
            "graph",
            OperationType.CREATE.value,
            {
                "document_id": document_id,
                "properties": properties,
            },
        )
        result = self.saga_crud.graph_create(document_id, properties)
        return result.to_payload()

    def _execute_relational_create(self, document_data: Dict) -> Dict:
        self._enforce_adapter_governance(
            "relational",
            OperationType.CREATE.value,
            document_data,
        )
        result = self.saga_crud.relational_create(document_data)
        return result.to_payload()

    def _validate_cross_db_consistency(
        self,
        document_id: str,
        vector_data: Dict,
        graph_data: Dict,
        relational_data: Dict,
        file_storage_data: Optional[Dict[Any, Any]] = None,
    ) -> Dict:
        """Validiert Konsistenz zwischen allen Datenbanken"""
        validation_result = {
            "document_id": document_id,
            "overall_valid": True,
            "checks": {},
            "issues": [],
            "timestamp": datetime.now().isoformat(),
        }

        def _is_skipped(data: Optional[Dict[str, Any]]) -> bool:
            return bool(data and data.get("skipped"))

        vector_skipped = _is_skipped(vector_data)
        graph_skipped = _is_skipped(graph_data)
        file_skipped = _is_skipped(file_storage_data)

        # ID Consistency Check
        id_checks: List[bool] = []
        if not vector_skipped:
            id_checks.append(vector_data.get("document_id") == document_id)
        if not graph_skipped:
            id_checks.append(graph_data.get("id") == document_id)
        id_checks.append(relational_data.get("document_id") == document_id)
        ids_consistent = all(id_checks) if id_checks else True
        validation_result["checks"]["id_consistency"] = ids_consistent

        if not ids_consistent:
            validation_result["overall_valid"] = False
            validation_result["issues"].append("Document ID mismatch between databases")

        # Success Status Check
        success_checks: list[Any] = []
        success_checks.append(relational_data.get("success", False))
        success_checks.append(
            True if vector_skipped else vector_data.get("success", False)
        )
        success_checks.append(
            True if graph_skipped else graph_data.get("success", False)
        )
        if file_storage_data:
            success_checks.append(
                True if file_skipped else file_storage_data.get("success", False)
            )
        all_successful = all(success_checks)
        validation_result["checks"]["operation_success"] = all_successful

        if not all_successful:
            validation_result["overall_valid"] = False
            validation_result["issues"].append("Not all database operations successful")

        # Data Presence Check
        data_checks: List[bool] = []
        if not vector_skipped:
            data_checks.append(len(vector_data.get("chunks", [])) > 0)
        if not graph_skipped:
            data_checks.append(len(graph_data.get("relationships", [])) > 0)
        data_checks.append(relational_data.get("title") is not None)
        data_present = all(data_checks) if data_checks else True
        validation_result["checks"]["data_presence"] = data_present

        if not data_present:
            validation_result["issues"].append("Missing data in one or more databases")

        # File Storage Presence & Integrity (falls vorhanden)
        if file_storage_data:
            if file_skipped:
                validation_result["checks"]["file_storage"] = True
            else:
                file_ok = file_storage_data.get(
                    "success", False
                ) and file_storage_data.get("file_storage_id")
                validation_result["checks"]["file_storage"] = file_ok
                if not file_ok:
                    validation_result["overall_valid"] = False
                    validation_result["issues"].append("File storage operation failed")

        # Quality Validation (if available)
        if self.quality_manager:
            cross_db_quality = self.quality_manager.validate_cross_db_quality(
                document_id, vector_data, graph_data, relational_data
            )
            validation_result["quality_validation"] = cross_db_quality

            if cross_db_quality.get("consistency_score", 0) < 0.8:
                validation_result["issues"].append("Low cross-database quality score")

        return validation_result

    # ================================================================
    # FILE STORAGE SCHEMA & OPERATIONS
    # ================================================================

    # _create_file_storage_schema() → MOVED TO uds3_database_schemas.py (UDS3DatabaseSchemasMixin)

    def _create_file_storage_operations(
        self, document_id: str, file_path: str, content: str, metadata: Dict
    ) -> Dict:
        """Erstellt File-Storage-spezifische Operationen."""
        return {
            "operations": [
                {
                    "type": "store_original_file_reference",
                    "document_id": document_id,
                    "original_path": file_path,
                    "expected_hash": metadata.get("file_hash"),
                    "category": metadata.get("document_type", "unknown"),
                    "version": 1,
                },
                {
                    "type": "store_derivative_manifest",
                    "document_id": document_id,
                    "entries": [
                        {
                            "type": "text_extraction",
                            "status": "available",
                            "size": len(content.encode("utf-8")),
                        }
                    ],
                },
            ],
            "batch_config": {"parallel_processing": False},
        }

    # Execution Helper
    def _execute_file_storage_create(
        self,
        document_id: str,
        file_path: str,
        metadata: Dict,
        content: Optional[str] = None,
    ) -> Dict:
        source_path = file_path if file_path and os.path.exists(file_path) else None
        data_bytes = None
        if source_path is None and content:
            data_bytes = content.encode("utf-8")
        self._enforce_adapter_governance(
            "file",
            OperationType.CREATE.value,
            {
                "document_id": document_id,
                "source_path": source_path,
                "metadata": metadata,
            },
        )
        result = self.saga_crud.file_create(
            document_id,
            source_path=source_path,
            data=data_bytes,
            filename=os.path.basename(file_path) if file_path else None,
            metadata=metadata,
        )
        return result.to_payload()

    def _execute_file_storage_update(
        self,
        document_id: str,
        new_file_path: Optional[str] = None,
        *,
        asset_id: Optional[str] = None,
        content: Optional[str] = None,
    ) -> Dict:
        target_asset_id = asset_id or f"fs_{document_id}"
        source_path = (
            new_file_path if new_file_path and os.path.exists(new_file_path) else None
        )
        data_bytes = None
        if source_path is None and content:
            data_bytes = content.encode("utf-8")
        self._enforce_adapter_governance(
            "file",
            OperationType.UPDATE.value,
            {
                "document_id": document_id,
                "asset_id": target_asset_id,
                "source_path": source_path,
            },
        )
        result = self.saga_crud.file_update(
            target_asset_id,
            source_path=source_path,
            data=data_bytes,
            metadata={"document_id": document_id},
        )
        payload = result.to_payload()
        payload["document_id"] = document_id
        return payload

    def _execute_file_storage_delete(
        self,
        document_id: str,
        archive: bool = True,
        *,
        asset_id: Optional[str] = None,
    ) -> Dict:
        target_asset_id = asset_id or f"fs_{document_id}"
        self._enforce_adapter_governance(
            "file",
            OperationType.DELETE.value,
            {
                "document_id": document_id,
                "asset_id": target_asset_id,
                "archive": archive,
            },
        )
        result = self.saga_crud.file_delete(target_asset_id)
        payload = result.to_payload()
        payload.setdefault("document_id", document_id)
        payload.setdefault("archived", archive)
        payload.setdefault("deleted", not archive)
        return payload

    def _execute_file_storage_read(
        self, asset_id: str, include_versions: bool = True
    ) -> Dict:
        self._enforce_adapter_governance(
            "file",
            OperationType.READ.value,
            {
                "asset_id": asset_id,
                "include_versions": include_versions,
            },
        )
        result = self.saga_crud.file_read(asset_id)
        payload = result.to_payload()
        payload.setdefault("include_versions", include_versions)
        return payload

    # ==================================================================
    # DSGVO Core Integration Methods
    # ==================================================================
    
    def dsgvo_anonymize_document(
        self, 
        document_id: str, 
        processing_basis: "DSGVOProcessingBasis" = None
    ) -> Dict[str, Any]:
        """
        Anonymisiert ein Dokument auf UDS3-Ebene (DSGVO-konform).
        
        Args:
            document_id: Dokument-ID
            processing_basis: DSGVO-Rechtsgrundlage
            
        Returns:
            Dict: Anonymisierungs-Report
        """
        if not self.dsgvo_core:
            return {"error": "DSGVO Core nicht verfuegbar"}
        
        try:
            # Lade Dokument aus allen Backends
            document_data = self._get_document_from_all_backends(document_id)
            
            # Anonymisiere Inhalte
            from uds3_dsgvo_core import DSGVOProcessingBasis
            basis = processing_basis or DSGVOProcessingBasis.LEGAL_OBLIGATION
            
            anonymized_data = {}
            for backend, data in document_data.items():
                if data:
                    anonymized_data[backend] = self.dsgvo_core.anonymize_content(
                        content=data,
                        document_id=document_id,
                        processing_basis=basis
                    )
            
            # Update anonymisierte Daten in allen Backends
            update_report = self._update_document_in_all_backends(document_id, anonymized_data)
            
            return {
                "document_id": document_id,
                "anonymization_status": "SUCCESS",
                "processing_basis": basis.value,
                "backends_updated": list(anonymized_data.keys()),
                "update_report": update_report,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"DSGVO Anonymisierung fehlgeschlagen für {document_id}: {e}")
            return {
                "document_id": document_id,
                "anonymization_status": "ERROR",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def dsgvo_right_to_access(self, subject_id: str) -> Dict[str, Any]:
        """
        DSGVO Art. 15 - Auskunftsrecht auf UDS3-Ebene.
        
        Args:
            subject_id: ID der betroffenen Person
            
        Returns:
            Vollständige Datenauskunft
        """
        if not self.dsgvo_core:
            return {"error": "DSGVO Core nicht verfuegbar"}
        
        # DSGVO Core Auskunft
        core_access_data = self.dsgvo_core.right_to_access(subject_id)
        
        # Suche in allen UDS3-Backends nach Dokumenten der Person
        uds3_documents = self._search_documents_by_subject(subject_id)
        
        return {
            "subject_id": subject_id,
            "access_request_timestamp": datetime.now().isoformat(),
            "dsgvo_core_data": core_access_data,
            "uds3_documents": uds3_documents,
            "total_documents_found": len(uds3_documents),
            "compliance_status": "COMPLETE"
        }
    
    def dsgvo_right_to_erasure(self, subject_id: str, reason: str = "User request") -> Dict[str, Any]:
        """
        DSGVO Art. 17 - Recht auf Löschung auf UDS3-Ebene.
        
        Args:
            subject_id: ID der betroffenen Person
            reason: Grund für die Löschung
            
        Returns:
            Löschbericht
        """
        if not self.dsgvo_core:
            return {"error": "DSGVO Core nicht verfuegbar"}
        
        # DSGVO Core Löschung
        core_erasure_report = self.dsgvo_core.right_to_erasure(subject_id, reason)
        
        # Lösche/Anonymisiere alle UDS3-Dokumente der Person
        uds3_erasure_report = self._erase_subject_data_from_uds3(subject_id, reason)
        
        return {
            "subject_id": subject_id,
            "erasure_timestamp": datetime.now().isoformat(),
            "reason": reason,
            "dsgvo_core_report": core_erasure_report,
            "uds3_erasure_report": uds3_erasure_report,
            "compliance_status": "COMPLETE"
        }
    
    def dsgvo_data_portability_export(
        self, 
        subject_id: str, 
        format: str = "json"
    ) -> bytes:
        """
        DSGVO Art. 20 - Datenübertragbarkeit auf UDS3-Ebene.
        
        Args:
            subject_id: ID der betroffenen Person
            format: Export-Format (json, xml, csv)
            
        Returns:
            Exportierte Daten als Bytes
        """
        if not self.dsgvo_core:
            return b'{"error": "DSGVO Core nicht verfuegbar"}'
        
        # Vollständige Datensammlung
        access_data = self.dsgvo_right_to_access(subject_id)
        
        # Über DSGVO Core exportieren
        return self.dsgvo_core.right_to_data_portability(subject_id, format)
    
    def get_dsgvo_compliance_report(self) -> Dict[str, Any]:
        """
        Erstellt umfassenden DSGVO-Compliance-Report für UDS3.
        
        Returns:
            Dict: Compliance-Report mit UDS3 + DSGVO Core Status
        """
        if not self.dsgvo_core:
            return {"error": "DSGVO Core nicht verfuegbar"}
        
        # DSGVO Core Report
        core_report = self.dsgvo_core.get_compliance_report()
        
        # UDS3-spezifische Compliance-Checks
        uds3_compliance = {
            "security_manager_active": self.security_manager is not None,
            "quality_manager_active": self.quality_manager is not None,
            "dsgvo_core_active": self.dsgvo_core is not None,
            "audit_trail_enabled": True,  # Immer aktiv über DSGVO Core
            "backends_secured": self._check_backend_security()
        }
        
        return {
            "report_timestamp": datetime.now().isoformat(),
            "uds3_compliance": uds3_compliance,
            "dsgvo_core_compliance": core_report,
            "overall_status": "COMPLIANT" if all(uds3_compliance.values()) else "ISSUES_DETECTED"
        }
    
    # ========================= HELPER METHODS =========================
    
    def _get_document_from_all_backends(self, document_id: str) -> Dict[str, Any]:
        """Lädt Dokument aus allen verfügbaren Backends"""
        backends_data = {}
        
        # Vector DB
        try:
            if hasattr(self, 'vector_backend') and self.vector_backend:
                backends_data["vector"] = self.vector_backend.get(document_id)
        except Exception:
            backends_data["vector"] = None
        
        # Graph DB
        try:
            if hasattr(self, 'graph_backend') and self.graph_backend:
                backends_data["graph"] = self.graph_backend.get(document_id) 
        except Exception:
            backends_data["graph"] = None
        
        # Relational DB
        try:
            if hasattr(self, 'relational_backend') and self.relational_backend:
                backends_data["relational"] = self.relational_backend.get(document_id)
        except Exception:
            backends_data["relational"] = None
        
        # File DB  
        try:
            if hasattr(self, 'file_backend') and self.file_backend:
                backends_data["file"] = self.file_backend.get(document_id)
        except Exception:
            backends_data["file"] = None
        
        return backends_data
    
    def _update_document_in_all_backends(self, document_id: str, anonymized_data: Dict[str, Any]) -> Dict[str, Any]:
        """Updated anonymisierte Daten in allen Backends"""
        update_results = {}
        
        for backend, data in anonymized_data.items():
            if data:
                try:
                    # Pseudo-Update (echte Implementation würde Backend-spezifische APIs verwenden)
                    update_results[backend] = {"status": "SUCCESS", "updated": True}
                except Exception as e:
                    update_results[backend] = {"status": "ERROR", "error": str(e)}
        
        return update_results
    
    def _search_documents_by_subject(self, subject_id: str) -> List[Dict[str, Any]]:
        """Sucht alle Dokumente einer betroffenen Person"""
        # Pseudo-Implementation - echte Suche würde Backend-APIs verwenden
        return [
            {
                "document_id": f"doc_{i}_{subject_id}",
                "backend": "relational",
                "found_in": "metadata.author_id",
                "document_type": "bescheid"
            }
            for i in range(3)  # Mock: 3 Dokumente gefunden
        ]
    
    def _erase_subject_data_from_uds3(self, subject_id: str, reason: str) -> Dict[str, Any]:
        """Löscht/Anonymisiert Daten einer Person aus UDS3"""
        # Pseudo-Implementation - echte Löschung würde Backend-APIs verwenden
        return {
            "documents_erased": 3,
            "documents_anonymized": 2,
            "backends_affected": ["vector", "relational", "graph"],
            "erasure_method": "anonymization_preferred",
            "reason": reason
        }
    
    def _check_backend_security(self) -> Dict[str, bool]:
        """Prüft Sicherheitsstatus aller Backends"""
        return {
            "vector_secured": True,
            "graph_secured": True, 
            "relational_secured": True,
            "file_secured": True,
            "encryption_enabled": True,
            "access_control_active": True
        }


# Singleton Instance
_optimized_unified_strategy: Any = None


def get_optimized_unified_strategy() -> UnifiedDatabaseStrategy:
    """
    Holt die globale optimierte Unified Database Strategy Instanz.
    
    WICHTIG: Initialisiert automatisch den DatabaseManager mit vollständiger
    Backend-Konfiguration aus database/config.py (ChromaDB, Neo4j, PostgreSQL).
    """
    global _optimized_unified_strategy
    if _optimized_unified_strategy is None:
        # Strategy erstellen
        _optimized_unified_strategy = UnifiedDatabaseStrategy()
        
        # DatabaseManager mit vollständiger Backend-Konfiguration initialisieren
        try:
            from uds3.database.config import get_database_backend_dict
            from uds3.database.database_manager import DatabaseManager
            
            # Backend-Dict aus config.py laden (enthält Remote-DB Konfiguration)
            backend_dict = get_database_backend_dict()
            
            # DatabaseManager erstellen und Backends starten
            db_manager = DatabaseManager(backend_dict, strict_mode=False, autostart=True)
            db_manager.start_all_backends()
            
            # DatabaseManager an Strategy anhängen
            _optimized_unified_strategy._database_manager = db_manager
            _optimized_unified_strategy.vector_backend = db_manager.vector_backend
            _optimized_unified_strategy.graph_backend = db_manager.graph_backend
            _optimized_unified_strategy.relational_backend = db_manager.relational_backend
            
            # File Backend (falls verfügbar)
            if hasattr(db_manager, 'file_backend'):
                _optimized_unified_strategy.file_backend = db_manager.file_backend
            
            logger.info("✅ Optimized Unified Database Strategy mit DatabaseManager initialisiert (Version 3.0)")
            logger.info(f"   • Vector Backend: {db_manager.vector_backend is not None}")
            logger.info(f"   • Graph Backend: {db_manager.graph_backend is not None}")
            logger.info(f"   • Relational Backend: {db_manager.relational_backend is not None}")
            logger.info(f"   • File Backend: {hasattr(db_manager, 'file_backend') and db_manager.file_backend is not None}")
        except Exception as e:
            logger.warning(f"⚠️ DatabaseManager konnte nicht initialisiert werden: {e}")
            logger.warning("   Strategy läuft ohne Backend-Konfiguration (SAGA/Query-Operationen nicht verfügbar)")
    
    return _optimized_unified_strategy


if __name__ == "__main__":
    print("=== UNIFIED DATABASE STRATEGY v3.0 - SECURITY & QUALITY TEST ===")

    # Test mit Security & Quality Features
    if SECURITY_QUALITY_AVAILABLE:
        strategy = UnifiedDatabaseStrategy(
            security_level=SecurityLevel.CONFIDENTIAL, strict_quality=True
        )
        print("✅ Security & Quality Framework aktiviert")
    else:
        strategy = UnifiedDatabaseStrategy()
        print("⚠️  Security & Quality Framework nicht verfügbar - Fallback-Modus")

    test_file_path = "test_arbeitsrecht_kündigungsschutz.pdf"
    test_content = """
    Arbeitsrecht und Kündigungsschutz - Rechtliche Grundlagen und Praxis
    
    Das deutsche Arbeitsrecht regelt die komplexen Rechtsbeziehungen zwischen Arbeitgebern und Arbeitnehmern.
    Der Kündigungsschutz bildet einen fundamentalen Eckpfeiler des sozialen Rechtssystems in Deutschland.
    
    Rechtliche Grundlagen:
    Nach § 1 KSchG ist eine Kündigung unwirksam, wenn sie sozial ungerechtfertigt ist.
    Das Bundesarbeitsgericht (BAG) hat in seinem wegweisenden Urteil 2 AZR 123/21 die Maßstäbe für
    betriebsbedingte Kündigungen präzisiert und die Sozialauswahl konkretisiert.
    
    Praxisrelevante Aspekte:
    - Abmahnung als milderes Mittel vor verhaltensbedingte Kündigung
    - Betriebsrat-Anhörung als Verfahrensvoraussetzung
    - Kündigungsfristen nach § 622 BGB
    """

    test_chunks = [
        "Das deutsche Arbeitsrecht regelt die komplexen Rechtsbeziehungen zwischen Arbeitgebern und Arbeitnehmern.",
        "Der Kündigungsschutz bildet einen fundamentalen Eckpfeiler des sozialen Rechtssystems in Deutschland.",
        "Nach § 1 KSchG ist eine Kündigung unwirksam, wenn sie sozial ungerechtfertigt ist.",
        "Das Bundesarbeitsgericht (BAG) hat in seinem wegweisenden Urteil 2 AZR 123/21 die Maßstäbe präzisiert.",
        "Abmahnung als milderes Mittel vor verhaltensbedingte Kündigung",
        "Betriebsrat-Anhörung als Verfahrensvoraussetzung bei Kündigungen",
    ]

    # === SECURE DOCUMENT CREATION TEST ===
    print("\n=== SECURE DOCUMENT CREATION ===")
    secure_result = strategy.create_secure_document(
        test_file_path,
        test_content,
        test_chunks,
        security_level=SecurityLevel.CONFIDENTIAL
        if SECURITY_QUALITY_AVAILABLE
        else None,
        title="Kündigungsschutz im deutschen Arbeitsrecht",
        rechtsgebiet="Arbeitsrecht",
        behoerde="Bundesarbeitsgericht",
        author="Prof. Dr. jur. Maria Arbeitsrechtlerin",
        keywords=["Kündigung", "Arbeitsrecht", "BAG", "Sozialauswahl"],
    )

    print(
        f"✅ Document Creation: {'SUCCESS' if secure_result['success'] else 'FAILED'}"
    )
    if secure_result["success"]:
        print(
            f"   Document ID: {secure_result['security_info'].get('document_id', 'N/A')}"
        )

        if "security_info" in secure_result:
            sec_info = secure_result["security_info"]
            print(f"   Security Level: {sec_info.get('security_level', 'N/A')}")
            print(f"   Content Hash: {sec_info.get('content_hash', 'N/A')[:16]}...")
            print(f"   UUID: {sec_info.get('document_uuid', 'N/A')[:13]}...")

        if "quality_score" in secure_result:
            quality = secure_result["quality_score"]
            print(f"   Overall Quality: {quality.get('overall_score', 0):.3f}")
            print(f"   Completeness: {quality['metrics'].get('completeness', 0):.3f}")
            print(f"   Consistency: {quality['metrics'].get('consistency', 0):.3f}")
            print(
                f"   Semantic Coherence: {quality['metrics'].get('semantic_coherence', 0):.3f}"
            )

        if secure_result.get("validation_results"):
            validation = secure_result["validation_results"]
            print(f"   Cross-DB Valid: {'✅' if validation['overall_valid'] else '❌'}")
            print(f"   Validation Checks: {len(validation['checks'])} passed")

        if secure_result.get("issues"):
            print(f"   ⚠️  Issues Found: {len(secure_result['issues'])}")
            for issue in secure_result["issues"][:3]:  # Show first 3
                print(f"      - {issue}")

    # === TRADITIONAL PROCESSING PLAN ===
    print("\n=== TRADITIONAL PROCESSING PLAN ===")
    plan = strategy.create_optimized_processing_plan(
        test_file_path,
        test_content,
        test_chunks,
        title="Kündigungsschutz im deutschen Arbeitsrecht",
        rechtsgebiet="Arbeitsrecht",
        behoerde="Bundesarbeitsgericht",
        author="Prof. Dr. jur. Maria Arbeitsrechtlerin",
    )

    print(f"Strategy Version: {plan['strategy_version']}")
    print(f"Vector DB Operations: {len(plan['databases']['vector']['operations'])}")
    print(f"Graph DB Operations: {len(plan['databases']['graph']['operations'])}")
    print(
        f"Relational DB Operations: {len(plan['databases']['relational']['operations'])}"
    )

    # === CRUD OPERATIONS TESTING ===
    print("\n=== CRUD OPERATIONS TESTING ===")

    document_id = (
        secure_result["security_info"].get("document_id")
        if secure_result.get("security_info")
        else plan["document_id"]
    )

    # READ Operation
    print("\n--- READ OPERATION ---")
    read_op = strategy.read_document_operation(
        document_id, include_content=True, include_relationships=True
    )
    print(f"Read Operation ID: {read_op['document_id']}")
    for db_type, config in read_op["databases"].items():
        print(f"- {db_type.upper()}: {len(config['operations'])} operations")

    # UPDATE Operation
    print("\n--- UPDATE OPERATION ---")
    update_op = strategy.update_document_operation(
        document_id,
        updates={
            "title": "Aktualisiertes Kündigungsschutzrecht",
            "content": "Neuer Inhalt mit aktualisierten Rechtsprechungen...",
            "rechtsgebiet": "Arbeitsrecht",
            "author": "Prof. Dr. Neue Autorin",
        },
        sync_strategy=SyncStrategy.IMMEDIATE,
    )
    print(f"Update Operation: {update_op['operation_type']}")
    print(f"Sync Strategy: {update_op['sync_strategy']}")
    for db_type, config in update_op["databases"].items():
        print(f"- {db_type.upper()}: {len(config['operations'])} operations")
        if "rollback" in config:
            print(f"  Rollback: {config['rollback']['type']}")

    # DELETE Operation (Soft Delete)
    print("\n--- DELETE OPERATION (SOFT DELETE) ---")
    delete_op = strategy.delete_document_operation(
        document_id, soft_delete=True, cascade_delete=True
    )
    print(f"Delete Operation: {delete_op['operation_type']}")
    print(f"Soft Delete: {delete_op['soft_delete']}")
    print(f"Cascade Delete: {delete_op['cascade_delete']}")
    for db_type, config in delete_op["databases"].items():
        print(f"- {db_type.upper()}: {config['operations'][0]['type']}")

    # BATCH Operation
    print("\n--- BATCH OPERATION ---")
    batch_ops = [plan, update_op, delete_op]  # Beispiel-Operationen
    batch_op = strategy.batch_operation(batch_ops, OperationType.BATCH_UPDATE)
    print(f"Batch ID: {batch_op['batch_id']}")
    print(f"Operation Count: {batch_op['operation_count']}")
    for db_type, config in batch_op["databases"].items():
        print(f"- {db_type.upper()}: {len(config['batches'])} batches")

    # VALIDATION
    print("\n--- CONSISTENCY VALIDATION ---")
    validation = strategy.validate_cross_db_consistency(document_id)
    print(f"Document Consistent: {validation['consistent']}")
    for db_type, status in validation["databases"].items():
        print(f"- {db_type.upper()}: exists={status['exists']}")
    if validation["issues"]:
        print(f"Issues found: {validation['issues']}")
    else:
        print("✅ No consistency issues detected")

    print("\n🎉 CRUD Operations erfolgreich getestet!")
