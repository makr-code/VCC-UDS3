#!/usr/bin/env python3
"""
VERITAS Protected Module
WARNING: This file contains embedded protection keys.
Modification will be detected and may result in license violations.
"""

# === VERITAS PROTECTION KEYS (DO NOT MODIFY) ===
module_name = "uds3_validation_worker"
module_licenced_organization = "VERITAS_TECH_GMBH"
module_licence_key = "eyJjbGllbnRfaWQi...pVkj5w=="  # Gekuerzt fuer Sicherheit
module_organization_key = (
    "5426bf69e2c7e091053f8e2732e9ec8319e68164b33889e0622e0741b5e1d554"
)
module_file_key = "555472ec2bc00ee2472e16fe838fa467e341ba8d616ac5c1457ad9ae0563fced"
module_version = "1.0"
module_protection_level = 1
# === END PROTECTION KEYS ===
"""
NLP/LLM Validation Worker f√ºr UDS3-Rechtsgrundlagen und Begr√ºndungen
Integration in die bestehende ThreadCoordinator-Pipeline

L√§uft nach NLP/LLM-Verarbeitung und vor/anstelle des Quality Workers
"""

import logging
import time
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """Ergebnis der NLP/LLM-Validierung"""

    success: bool
    overall_score: float
    passed: bool
    detailed_scores: Dict[str, float]
    validation_results: Dict[str, Any]
    issues: List[str]
    warnings: List[str]
    recommendations: List[str]
    processing_time: float
    error_message: Optional[str] = None


class UDS3ValidationWorker:
    """
    NLP/LLM Worker f√ºr die Validierung von UDS3-Rechtsgrundlagen und Begr√ºndungen
    Integriert in die bestehende ThreadCoordinator-Pipeline
    """

    def __init__(self, quality_threshold: float = 0.75):
        self.quality_threshold = quality_threshold

        # Lade NLP/LLM Interface
        try:
            import sys
            from pathlib import Path

            project_root = Path(__file__).parent
            sys.path.insert(0, str(project_root))

            try:
                from nlp_llm_worker_interface import UDS3_NLP_LLM_Interface  # type: ignore

                self.nlp_interface = UDS3_NLP_LLM_Interface()
            except Exception:
                from typing import Any

                UDS3_NLP_LLM_Interface = Any  # type: ignore
                self.nlp_interface = None
            logger.info("ü§ñ UDS3 NLP/LLM Interface erfolgreich geladen")
        except ImportError as e:
            logger.error(f"‚ùå Fehler beim Laden des NLP/LLM Interface: {e}")
            self.nlp_interface = None

    def process_document(
        self, task: Dict[str, Any], progress_callback=None
    ) -> ValidationResult:
        """
        F√ºhrt NLP/LLM-Validierung f√ºr UDS3-Rechtsgrundlagen und Begr√ºndungen durch

        Args:
            task: Pipeline-Task mit UDS3-Metadaten
            progress_callback: Optional callback f√ºr Fortschrittsmeldungen

        Returns:
            ValidationResult mit Validierungsergebnissen
        """
        start_time = time.time()

        # Debug-Log f√ºr Task-Typ und -Inhalt
        logger.info(f"üîç UDS3-Validierung gestartet - Task-Typ: {type(task)}")
        if isinstance(task, dict):
            logger.debug(f"Task-Keys: {list(task.keys())}")
        else:
            logger.warning(f"Task ist kein Dictionary: {str(task)[:200]}...")

        if progress_callback:
            progress_callback("NLP/LLM-Validierung gestartet", 0)

        try:
            # Task-Daten extrahieren - Robust gegen verschiedene Eingabetypen
            if isinstance(task, str):
                # Falls task ein JSON-String ist, versuche zu parsen
                try:
                    import json

                    task = json.loads(task)
                    logger.info("Task von JSON-String zu Dictionary konvertiert")
                except json.JSONDecodeError:
                    logger.error(f"Task ist String aber kein g√ºltiges JSON: {task}")
                    return ValidationResult(
                        success=False,
                        overall_score=0.0,
                        passed=False,
                        detailed_scores={},
                        validation_results={},
                        issues=[
                            f"Ung√ºltiger Task-Typ: String statt Dictionary - {str(task)[:100]}"
                        ],
                        warnings=["Task-Format-Problem"],
                        recommendations=["Task muss als Dictionary √ºbergeben werden"],
                        processing_time=0,
                    )

            if not isinstance(task, dict):
                logger.error(
                    f"Task hat ung√ºltigen Typ: {type(task)} - {str(task)[:100]}"
                )
                return ValidationResult(
                    success=False,
                    overall_score=0.0,
                    passed=False,
                    detailed_scores={},
                    validation_results={},
                    issues=[f"Ung√ºltiger Task-Typ: {type(task)} statt Dictionary"],
                    warnings=["Task-Typ-Problem"],
                    recommendations=["Task muss als Dictionary √ºbergeben werden"],
                    processing_time=0,
                )

            metadata = task.get("metadata", {})
            job_id = task.get("job_id", "unknown")
            file_path = task.get("file_path", "")

            # Metadata-Validierung und -Konvertierung
            if isinstance(metadata, str):
                try:
                    import json

                    metadata = json.loads(metadata)
                    logger.info("Metadata von JSON-String zu Dictionary konvertiert")
                except json.JSONDecodeError:
                    logger.warning(
                        f"Metadata ist String aber kein g√ºltiges JSON: {metadata[:100]}..."
                    )
                    # Fallback: leeres Dictionary
                    metadata: dict[Any, Any] = {}

            if not isinstance(metadata, dict):
                logger.warning(
                    f"Metadata hat ung√ºltigen Typ: {type(metadata)}, verwende leeres Dictionary"
                )
                metadata: dict[Any, Any] = {}

            if not metadata:
                return ValidationResult(
                    success=False,
                    overall_score=0.0,
                    passed=False,
                    detailed_scores={},
                    validation_results={},
                    issues=["Keine Metadaten f√ºr Validierung verf√ºgbar"],
                    warnings=[],
                    recommendations=[
                        "Stelle sicher, dass UDS3-Extraktion vor Validierung l√§uft"
                    ],
                    processing_time=time.time() - start_time,
                    error_message="Keine Metadaten verf√ºgbar",
                )

            if progress_callback:
                progress_callback("Pr√ºfe verf√ºgbare Daten", 10)

            # Pr√ºfe, ob Rechtsgrundlagen/Begr√ºndungen vorhanden sind
            has_rechtsgrundlagen = self._check_rechtsgrundlagen_data(metadata)
            has_begr√ºndungen = self._check_begr√ºndungen_data(metadata)

            if not has_rechtsgrundlagen and not has_begr√ºndungen:
                return ValidationResult(
                    success=True,
                    overall_score=0.5,  # Neutral - keine Daten zu validieren
                    passed=True,
                    detailed_scores={"data_availability": 0.5},
                    validation_results={
                        "skip_reason": "Keine Rechtsgrundlagen/Begr√ºndungen gefunden"
                    },
                    issues=[],
                    warnings=[
                        "Keine Rechtsgrundlagen oder Begr√ºndungen f√ºr NLP/LLM-Validierung gefunden"
                    ],
                    recommendations=["Pr√ºfe UDS3-Extraktion auf Vollst√§ndigkeit"],
                    processing_time=time.time() - start_time,
                )

            if progress_callback:
                progress_callback("Bereite NLP/LLM-Tasks vor", 20)

            # Bestimme anwendbare NLP-Tasks
            applicable_tasks: list[Any] = []

            if has_rechtsgrundlagen:
                applicable_tasks.extend(
                    ["rechtsgrundlagen_validierung", "zeitpunkt_konsistenz"]
                )

            if has_begr√ºndungen:
                applicable_tasks.append("begr√ºndungsqualit√§t")

            logger.info(
                f"üîç F√ºhre {len(applicable_tasks)} NLP/LLM-Validierungen durch: {applicable_tasks}"
            )

            # F√ºhre alle anwendbaren Validierungen durch
            validation_results: dict[Any, Any] = {}
            detailed_scores: dict[Any, Any] = {}
            all_issues: list[Any] = []
            all_warnings: list[Any] = []
            all_recommendations: list[Any] = []

            total_tasks = len(applicable_tasks)
            for i, task_name in enumerate(applicable_tasks):
                if progress_callback:
                    progress_callback(
                        f"Validierung: {task_name}", 30 + (i * 40 // total_tasks)
                    )

                try:
                    # Bereite Task vor
                    prompt_package = self.nlp_interface.prepare_nlp_task(
                        task_name, metadata
                    )

                    # F√ºhre Mock-Validierung durch (in echtem System: LLM-Aufruf)
                    task_result = self._execute_validation_task(
                        prompt_package, task_name
                    )

                    validation_results[task_name] = task_result
                    detailed_scores[task_name] = task_result.get("konfidenzscore", 0.5)

                    # Sammle Issues/Warnings/Empfehlungen
                    if "empfehlungen" in task_result:
                        all_recommendations.extend(task_result["empfehlungen"])
                    if "issues" in task_result:
                        all_issues.extend(task_result["issues"])
                    if "warnings" in task_result:
                        all_warnings.extend(task_result["warnings"])

                    logger.debug(
                        f"‚úÖ Task {task_name} abgeschlossen: Score {detailed_scores[task_name]:.2f}"
                    )

                except Exception as e:
                    logger.error(
                        f"‚ùå Validierungs-Task {task_name} fehlgeschlagen: {e}"
                    )
                    detailed_scores[task_name] = 0.0
                    validation_results[task_name] = {"error": str(e)}
                    all_issues.append(f"Validierung {task_name} fehlgeschlagen: {e}")

            if progress_callback:
                progress_callback("Berechne Gesamtbewertung", 90)

            # Berechne Gesamtbewertung
            if detailed_scores:
                overall_score = sum(detailed_scores.values()) / len(detailed_scores)
            else:
                overall_score = 0.0

            passed = overall_score >= self.quality_threshold
            processing_time = time.time() - start_time

            if progress_callback:
                progress_callback("NLP/LLM-Validierung abgeschlossen", 100)

            logger.info(
                f"üéØ NLP/LLM-Validierung abgeschlossen: Score {overall_score:.2f}, Passed: {passed}"
            )

            return ValidationResult(
                success=True,
                overall_score=overall_score,
                passed=passed,
                detailed_scores=detailed_scores,
                validation_results=validation_results,
                issues=all_issues,
                warnings=all_warnings,
                recommendations=all_recommendations,
                processing_time=processing_time,
            )

        except Exception as e:
            error_msg = f"NLP/LLM-Validierung fehlgeschlagen: {str(e)}"
            logger.error(error_msg)

            return ValidationResult(
                success=False,
                overall_score=0.0,
                passed=False,
                detailed_scores={},
                validation_results={},
                issues=[error_msg],
                warnings=[],
                recommendations=["Pr√ºfe NLP/LLM-System-Konfiguration"],
                processing_time=time.time() - start_time,
                error_message=error_msg,
            )

    def _check_rechtsgrundlagen_data(self, metadata: Dict[str, Any]) -> bool:
        """Pr√ºft, ob Rechtsgrundlagen-Daten f√ºr Validierung verf√ºgbar sind"""
        rechtsgrundlagen_fields = [
            "anwendbare_gesetze",
            "anwendbare_verordnungen",
            "rechtsgrundlagen_zum_zeitpunkt",
            "erm√§chtigungsgrundlagen_spezifisch",
        ]

        return any(
            metadata.get(field) and len(metadata[field]) > 0
            for field in rechtsgrundlagen_fields
            if isinstance(metadata.get(field), (list, str))
        )

    def _check_begr√ºndungen_data(self, metadata: Dict[str, Any]) -> bool:
        """Pr√ºft, ob Begr√ºndungs-Daten f√ºr Validierung verf√ºgbar sind"""
        begr√ºndung_fields = [
            "begr√ºndung_nebenbestimmungen",
            "verh√§ltnism√§√üigkeitspr√ºfung",
            "sachverst√§ndigengutachten_referenzen",
            "begr√ºndungsqualit√§t",
        ]

        return any(
            metadata.get(field) and len(metadata[field]) > 0
            for field in begr√ºndung_fields
            if isinstance(metadata.get(field), (list, str))
        )

    def _execute_validation_task(
        self, prompt_package, task_name: str
    ) -> Dict[str, Any]:
        """
        F√ºhrt einzelne Validierungs-Task durch

        HINWEIS: In der echten Implementierung w√ºrde hier ein LLM-Aufruf stattfinden
        Aktuell: Mock-Implementation mit realistischen Ergebnissen
        """

        # Mock-Ergebnisse basierend auf Task-Typ
        if task_name == "rechtsgrundlagen_validierung":
            return {
                "validierung_ergebnis": "VOLLST√ÑNDIG",
                "korrekte_rechtsgrundlagen": ["BImSchG ¬ß 4", "VwVfG ¬ß 28"],
                "fehlende_rechtsgrundlagen": [],
                "empfehlungen": ["EU-Richtlinien-Bezug k√∂nnte spezifiziert werden"],
                "konfidenzscore": 0.87,
            }

        elif task_name == "zeitpunkt_konsistenz":
            return {
                "konsistenz_bewertung": "KONSISTENT",
                "zeitpunkt_analyse": {
                    "bescheid_datum": "2023-07-15",
                    "angewandte_rechtsversion": "korrekt",
                },
                "empfehlungen": ["Versionsst√§nde sind aktuell"],
                "konfidenzscore": 0.92,
            }

        elif task_name == "begr√ºndungsqualit√§t":
            return {
                "begr√ºndungsqualit√§t_gesamt": "GUT",
                "einzelbewertungen": {
                    "vollst√§ndigkeit": 0.80,
                    "rechtliche_fundierung": 0.85,
                    "verh√§ltnism√§√üigkeit": 0.75,
                },
                "empfehlungen": [
                    "Verh√§ltnism√§√üigkeitspr√ºfung k√∂nnte detaillierter sein"
                ],
                "konfidenzscore": 0.80,
            }

        else:
            return {"status": "unknown_task", "konfidenzscore": 0.5}


def integrate_uds3_validation_worker():
    """
    Integration der UDS3-Validierung in die bestehende ThreadCoordinator-Pipeline

    INTEGRATION POINT: Ersetzt oder erg√§nzt den bestehenden Quality Worker
    """

    # Integration Code - wird in ingestion_core_components.py eingef√ºgt
    integration_code = """
    
    # In ThreadCoordinator.__init__ hinzuf√ºgen:
    self.uds3_validation_workers: list[Any] = []
    self.uds3_validation_queue = queue.Queue()  # NEU: UDS3 Validation Queue
    
    # In thread_configs hinzuf√ºgen:
    {'type': 'uds3_validation', 'count': 1, 'queue': self.uds3_validation_queue},
    
    # In _process_task hinzuf√ºgen:
    elif worker_type == 'uds3_validation':
        self._process_uds3_validation_task(task, worker_id)
    
    # In _get_workers_by_type hinzuf√ºgen:
    'uds3_validation': self.uds3_validation_workers,
    
    # In _get_worker_counts hinzuf√ºgen:
    'uds3_validation': len(self.uds3_validation_workers),
    
    # In _get_next_pipeline_job_for_worker_type hinzuf√ºgen:
    'uds3_validation': ['file'],  # UDS3-Validation nach NLP/LLM
    """

    return integration_code


if __name__ == "__main__":
    print("ü§ñ UDS3 NLP/LLM Validation Worker")
    print("Bereit f√ºr Integration in ThreadCoordinator-Pipeline")
    print("Position: Nach NLP/LLM-Processing, vor/anstelle Quality Worker")

    # Demo der Funktionalit√§t
    worker = UDS3ValidationWorker()

    # Mock-Task f√ºr Demonstration
    mock_task = {
        "job_id": "test_123",
        "file_path": "test_bescheid.txt",
        "metadata": {
            "anwendbare_gesetze": ["BImSchG", "VwVfG"],
            "begr√ºndung_nebenbestimmungen": ["Schutz vor Umwelteinwirkungen"],
            "rechtsgrundlagen_zum_zeitpunkt": ["2023-07-15"],
        },
    }

    def mock_progress(stage, progress):
        print(f"[{progress:3d}%] {stage}")

    result = worker.process_document(mock_task, mock_progress)

    print("\nüéØ ERGEBNIS:")
    print(f"   Success: {result.success}")
    print(f"   Overall Score: {result.overall_score:.2f}")
    print(f"   Passed: {result.passed}")
    print(f"   Processing Time: {result.processing_time:.2f}s")
    print(f"   Recommendations: {len(result.recommendations)}")
