#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
database_api.py

Einheitliche Database API für Veritas RAG System
Unterstützt verschiedene Database-Backends über eine gemeinsame Schnittstelle
Mit integriertem Adaptive Batch Processing für optimale Performance

Part of UDS3 (Unified Database Strategy v3)
Author: Martin Krüger (ma.krueger@outlook.com)
License: MIT with Government Partnership Commons Clause
Repository: https://github.com/makr-code/VCC-UDS3
"""

import logging
import sqlite3
import json
import uuid
import time
import threading
import queue
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Tuple, Callable
from datetime import datetime
import os
from pathlib import Path

from . import config
from .database_manager import DatabaseManager
from .database_api_base import DatabaseBackend, VectorDatabaseBackend, GraphDatabaseBackend, RelationalDatabaseBackend
from uds3.uds3_admin_types import AdminDocumentType, AdminLevel, AdminDomain, ProcedureStage

# Mock AdaptiveBatchProcessor für Stabilität
class AdaptiveBatchProcessor:
    """Mock Adaptive Batch Processor für UDS3-Kompatibilität"""
    
    def __init__(self, operation_func=None, max_batch_size=1000, min_batch_size=100, **kwargs):
        self.operation_func = operation_func
        self.max_batch_size = max_batch_size  
        self.min_batch_size = min_batch_size
        self.processed_count = 0
        
    def process_batch(self, items, **kwargs):
        """Mock Batch Processing"""
        try:
            if self.operation_func and callable(self.operation_func):
                result = self.operation_func(items, **kwargs)
            else:
                result = {'processed': len(items), 'success': True}
            
            self.processed_count += len(items)
            return result
        except Exception as e:
            logging.error(f"Mock AdaptiveBatchProcessor Error: {e}")
            return {'processed': 0, 'success': False, 'error': str(e)}
    
    def get_stats(self):
        """Mock Statistics"""
        return {
            'total_processed': self.processed_count,
            'batch_size_range': f"{self.min_batch_size}-{self.max_batch_size}",
            'status': 'active'
        }

# =============================================================================
# KGE QUEUE, ENRICHMENT LOGGING & SIMPLE ACL (SQLite)
# =============================================================================
# Eine schlanke, unabhängige SQLite-Unterstützung innerhalb von database_api.py,
# damit KGE-Worker und Orchestrator persistente Tasks/Ergebnisse/Logs ablegen können.

_KGE_DB_PATH = os.environ.get(
    "VERITAS_SQLITE_PATH",
    os.path.join(os.path.dirname(__file__), "veritas_backend.sqlite"),
)
_KGE_DB_LOCK = threading.RLock()
_KGE_CONN: Optional[sqlite3.Connection] = None


def _kge_get_conn() -> sqlite3.Connection:
    global _KGE_CONN
    with _KGE_DB_LOCK:
        if _KGE_CONN is None:
            _KGE_CONN = sqlite3.connect(_KGE_DB_PATH, check_same_thread=False)
            _KGE_CONN.row_factory = sqlite3.Row
            _kge_init_schema(_KGE_CONN)
        return _KGE_CONN

def _kge_init_schema(conn: Optional[sqlite3.Connection] = None):
    conn = conn or _kge_get_conn()
    with conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS kge_tasks (
                task_id TEXT PRIMARY KEY,
                task_type TEXT NOT NULL,
                parameters TEXT NOT NULL,
                document_id TEXT,
                priority INTEGER DEFAULT 1,
                status TEXT DEFAULT 'queued',
                created_at REAL,
                updated_at REAL,
                error TEXT,
                result_id TEXT
            )
            """
        )
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS kge_results (
                id TEXT PRIMARY KEY,
                task_id TEXT NOT NULL,
                success INTEGER NOT NULL,
                processing_time REAL DEFAULT 0.0,
                embeddings_generated INTEGER DEFAULT 0,
                similar_nodes_found INTEGER DEFAULT 0,
                error TEXT,
                metadata TEXT,
                created_at REAL,
                FOREIGN KEY(task_id) REFERENCES kge_tasks(task_id)
            )
            """
        )
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS enrichment_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                action TEXT,
                status TEXT,
                details TEXT,
                created_at REAL,
                triggered_by TEXT
            )
            """
        )
    # Keine ACL-Tabellen erforderlich

def kge_enqueue_task(task_id: str, task_type: str, parameters: Dict[str, Any], document_id: Optional[str], priority: int = 1) -> str:
    now = time.time()
    with _KGE_DB_LOCK, _kge_get_conn() as conn:
        conn.execute(
            "INSERT OR REPLACE INTO kge_tasks(task_id, task_type, parameters, document_id, priority, status, created_at, updated_at) VALUES(?,?,?,?,?,?,?,?)",
            (task_id, task_type, json.dumps(parameters or {}), document_id, priority, "queued", now, now),
        )
    return task_id

def kge_fetch_next_pending_task() -> Optional[sqlite3.Row]:
    with _KGE_DB_LOCK:
        conn = _kge_get_conn()
        row = conn.execute(
            "SELECT * FROM kge_tasks WHERE status='queued' ORDER BY priority DESC, created_at ASC LIMIT 1"
        ).fetchone()
        if row:
            conn.execute(
                "UPDATE kge_tasks SET status='running', updated_at=? WHERE task_id=?",
                (time.time(), row["task_id"]),
            )
            conn.commit()
        return row

def kge_set_task_status(task_id: str, status: str, error: Optional[str] = None):
    with _KGE_DB_LOCK, _kge_get_conn() as conn:
        conn.execute(
            "UPDATE kge_tasks SET status=?, error=?, updated_at=? WHERE task_id=?",
            (status, error, time.time(), task_id),
        )

def kge_store_result(task_id: str, result_id: str, success: bool, processing_time: float, embeddings_generated: int, similar_nodes_found: int, error: Optional[str], metadata: Dict[str, Any]):
    now = time.time()
    with _KGE_DB_LOCK, _kge_get_conn() as conn:
        conn.execute(
            "INSERT OR REPLACE INTO kge_results(id, task_id, success, processing_time, embeddings_generated, similar_nodes_found, error, metadata, created_at) VALUES(?,?,?,?,?,?,?,?,?)",
            (result_id, task_id, 1 if success else 0, processing_time, embeddings_generated, similar_nodes_found, error, json.dumps(metadata or {}), now),
        )
        conn.execute(
            "UPDATE kge_tasks SET status=?, result_id=?, updated_at=? WHERE task_id=?",
            ("done" if success else "error", result_id, now, task_id),
        )

def kge_get_task_result(task_id: str) -> Optional[Dict[str, Any]]:
    with _KGE_DB_LOCK:
        conn = _kge_get_conn()
        task = conn.execute("SELECT * FROM kge_tasks WHERE task_id=?", (task_id,)).fetchone()
        if not task:
            return None
        res = None
        if task["result_id"]:
            res = conn.execute("SELECT * FROM kge_results WHERE id=?", (task["result_id"],)).fetchone()
        return {"task": dict(task), "result": dict(res) if res else None}

def kge_get_queue_size() -> int:
    with _KGE_DB_LOCK:
        conn = _kge_get_conn()
        row = conn.execute("SELECT COUNT(1) AS c FROM kge_tasks WHERE status IN ('queued','running')").fetchone()
        return int(row["c"] if row else 0)

def enrichment_log(action: str, status: str, details: Dict[str, Any], triggered_by: Optional[str] = None):
    with _KGE_DB_LOCK, _kge_get_conn() as conn:
        conn.execute(
            "INSERT INTO enrichment_logs(action, status, details, created_at, triggered_by) VALUES(?,?,?,?,?)",
            (action, status, json.dumps(details or {}), time.time(), triggered_by),
        )

class PersistentBackupManager:
    """Persistent Backup & Caching Manager für Crash-Recovery und Performance-Optimierung"""
    
    def __init__(self, backend_name: str, backup_dir: str = "./database_backup"):
        self.backend_name = backend_name
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(exist_ok=True)
        
        # Multi-Storage Backup Strategy
        self.sqlite_backup_path = self.backup_dir / f"{backend_name}_backup.db"
        self.json_backup_path = self.backup_dir / f"{backend_name}_backup.json"
        self.cache_path = self.backup_dir / f"{backend_name}_cache.db"
        
        # Initialisiere Backup Storages
        self._init_sqlite_backup()
        self._init_json_backup()
        self._init_cache_storage()
        
        logging.info(f"Persistent Backup Manager für {backend_name} initialisiert")
    
    def _init_sqlite_backup(self):
        """Initialisiert SQLite Backup für strukturierte Operationen"""
        try:
            import sqlite3
            conn = sqlite3.connect(self.sqlite_backup_path, check_same_thread=False)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS pending_operations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp REAL,
                    operation_type TEXT,
                    operation_data TEXT,
                    retry_count INTEGER DEFAULT 0,
                    priority INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'pending'
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS operation_cache (
                    cache_key TEXT PRIMARY KEY,
                    cache_value TEXT,
                    timestamp REAL,
                    expiry REAL,
                    hit_count INTEGER DEFAULT 0
                )
            """)
            conn.commit()
            conn.close()
            logging.info(f"SQLite Backup initialisiert: {self.sqlite_backup_path}")
        except Exception as e:
            logging.error(f"SQLite Backup Initialisierung fehlgeschlagen: {e}")
    
    def _init_json_backup(self):
        """Initialisiert JSON Backup für einfache Operationen"""
        if not self.json_backup_path.exists():
            backup_data = {
                'pending_operations': [],
                'cache': {},
                'metadata': {
                    'created': time.time(),
                    'backend': self.backend_name,
                    'version': '1.0'
                }
            }
            with open(self.json_backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, indent=2)
    
    def _init_cache_storage(self):
        """Initialisiert separates Cache-Storage für Performance"""
        try:
            import sqlite3
            conn = sqlite3.connect(self.cache_path, check_same_thread=False)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS query_cache (
                    query_hash TEXT PRIMARY KEY,
                    query_text TEXT,
                    result_data TEXT,
                    timestamp REAL,
                    expiry REAL,
                    hit_count INTEGER DEFAULT 0,
                    size_bytes INTEGER DEFAULT 0
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS batch_cache (
                    batch_id TEXT PRIMARY KEY,
                    operations TEXT,
                    result TEXT,
                    timestamp REAL,
                    success_rate REAL
                )
            """)
            conn.commit()
            conn.close()
            logging.info(f"Cache Storage initialisiert: {self.cache_path}")
        except Exception as e:
            logging.error(f"Cache Storage Initialisierung fehlgeschlagen: {e}")


        """Backend-specific connection logic"""
        pass
    
    def batch_add_documents(self, collection_name: str, documents: List[str], 
                           metadatas: List[Dict], ids: List[str]) -> bool:
        """Queue documents for adaptive batch processing"""
        operation_data = {
            'type': 'add_documents',
            'collection_name': collection_name,
            'documents': documents,
            'metadatas': metadatas,
            'ids': ids
        }
        return self.queue_batch_operation('add_documents', operation_data)
    
    def batch_add_vector(self, vector_id: str, vector: List[float], 
                        metadata: Dict = None, collection_name: str = None) -> bool:
        """Queue vector for adaptive batch processing"""
        operation_data = {
            'type': 'add_vector',
            'vector_id': vector_id,
            'vector': vector,
            'metadata': metadata,
            'collection_name': collection_name
        }
        return self.queue_batch_operation('add_vectors', operation_data)
    
    @abstractmethod
    def create_collection(self, name: str, metadata: Dict = None) -> bool:
        """Erstelle eine neue Collection"""
        pass
    
    @abstractmethod
    def get_collection(self, name: str):
        """Hole eine bestehende Collection"""
        pass
    
    @abstractmethod
    def list_collections(self) -> List[str]:
        """Liste alle Collections"""
        pass
    
    @abstractmethod
    def add_documents(self, collection_name: str, documents: List[str], 
                     metadatas: List[Dict], ids: List[str]) -> bool:
        """Füge Dokumente zur Collection hinzu"""
        pass
    
    @abstractmethod
    def search_similar(self, collection_name: str, query: str, 
                      n_results: int = 5) -> List[Dict]:
        """Suche ähnliche Dokumente"""
        pass

class GraphDatabaseBackend(DatabaseBackend):
    """Abstract Base für Graph-Datenbanken mit Adaptive Batch Processing"""
    
    def __init__(self, config: Dict = None):
        super().__init__(config)
    
    def connect(self) -> bool:
        """Enhanced connect with adaptive batch initialization"""
        success = self._backend_connect()
        if success:
            self._is_connected = True
            self._initialize_adaptive_batch_processing()
            # Initialize graph-specific processors
            self._initialize_graph_batch_processors()
        return success
    
    def _initialize_graph_batch_processors(self):
        """Initialize graph-specific adaptive batch processors"""
        try:
            # Graph-specific operations
            self._adaptive_processors['create_nodes'] = AdaptiveBatchProcessor(
                self, "create_nodes",
                base_batch_size=30,  # Nodes can be batched efficiently
                max_batch_size=300,
                batch_executor=self._batch_create_nodes_executor if hasattr(self, '_batch_create_nodes_executor') else None
            )
            
            self._adaptive_processors['create_edges'] = AdaptiveBatchProcessor(
                self, "create_edges",
                base_batch_size=20,  # Edges might need more validation
                max_batch_size=200,
                batch_executor=self._batch_create_edges_executor if hasattr(self, '_batch_create_edges_executor') else None
            )
            
            self.logger.info("✅ Graph-specific adaptive batch processors initialized")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize graph batch processors: {e}")
    
    @abstractmethod
    def _backend_connect(self) -> bool:
        """Backend-specific connection logic"""
        pass
    
    def batch_create_node(self, node_type: str, properties: Dict) -> bool:
        """Queue node creation for adaptive batch processing"""
        operation_data = {
            'type': 'create_node',
            'node_type': node_type,
            'properties': properties
        }
        return self.queue_batch_operation('create_nodes', operation_data)
    
    def batch_create_edge(self, from_id: str, to_id: str, edge_type: str, 
                         properties: Dict = None) -> bool:
        """Queue edge creation for adaptive batch processing"""
        operation_data = {
            'type': 'create_edge',
            'from_id': from_id,
            'to_id': to_id,
            'edge_type': edge_type,
            'properties': properties or {}
        }
        return self.queue_batch_operation('create_edges', operation_data)
    
    @abstractmethod
    def create_node(self, node_type: str, properties: Dict) -> str:
        """Erstelle einen Knoten"""
        pass
    
    @abstractmethod
    def create_edge(self, from_id: str, to_id: str, edge_type: str, 
                   properties: Dict = None) -> str:
        """Erstelle eine Kante zwischen Knoten"""
        pass
    
    @abstractmethod
    def get_node(self, node_id: str) -> Optional[Dict]:
        """Hole einen Knoten"""
        pass
    
    @abstractmethod
    def find_nodes(self, node_type: str, filters: Dict = None) -> List[Dict]:
        """Finde Knoten nach Typ und Filtern"""
        pass
    
    @abstractmethod
    def get_relationships(self, node_id: str, direction: str = "both") -> List[Dict]:
        """Hole Beziehungen eines Knotens"""
        pass

class RelationalDatabaseBackend(DatabaseBackend):
    """Abstract Base für relationale Datenbanken mit Adaptive Batch Processing"""
    
    def __init__(self, config: Dict = None):
        super().__init__(config)
    
    def connect(self) -> bool:
        """Enhanced connect with adaptive batch initialization"""
        success = self._backend_connect()
        if success:
            self._is_connected = True
            self._initialize_adaptive_batch_processing()
            # Initialize relational-specific processors
            self._initialize_relational_batch_processors()
        return success
    
    def _initialize_relational_batch_processors(self):
        """Initialize relational-specific adaptive batch processors"""
        try:
            # Relational-specific operations
            self._adaptive_processors['insert_records'] = AdaptiveBatchProcessor(
                self, "insert_records",
                base_batch_size=100,  # SQL INSERTs can handle large batches
                max_batch_size=1000,
                batch_executor=self._batch_insert_records_executor if hasattr(self, '_batch_insert_records_executor') else None
            )
            
            self._adaptive_processors['update_records'] = AdaptiveBatchProcessor(
                self, "update_records",
                base_batch_size=50,  # UPDATEs might be more complex
                max_batch_size=500,
                batch_executor=self._batch_update_records_executor if hasattr(self, '_batch_update_records_executor') else None
            )
            
            self.logger.info("✅ Relational-specific adaptive batch processors initialized")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize relational batch processors: {e}")
    
    @abstractmethod
    def _backend_connect(self) -> bool:
        """Backend-specific connection logic"""
        pass
    
    def batch_insert_record(self, table_name: str, data: Dict) -> bool:
        """Queue record insertion for adaptive batch processing"""
        operation_data = {
            'type': 'insert_record',
            'table_name': table_name,
            'data': data
        }
        return self.queue_batch_operation('insert_records', operation_data)
    
    def batch_update_record(self, table_name: str, record_id: Any, data: Dict) -> bool:
        """Queue record update for adaptive batch processing"""
        operation_data = {
            'type': 'update_record',
            'table_name': table_name,
            'record_id': record_id,
            'data': data
        }
        return self.queue_batch_operation('update_records', operation_data)
    
    @abstractmethod
    def execute_query(self, query: str, params: Tuple = None) -> List[Dict]:
        """Führe SQL-Query aus"""
        pass
    
    @abstractmethod
    def create_table(self, table_name: str, schema: Dict) -> bool:
        """Erstelle Tabelle"""
        pass
    
    @abstractmethod
    def insert_record(self, table_name: str, data: Dict) -> Any:
        """Füge Datensatz ein"""
        pass
    
    @abstractmethod
    def update_record(self, table_name: str, record_id: Any, data: Dict) -> bool:
        """Update Datensatz"""
        pass

# ===== GLOBALER MANAGER =====
_database_manager = None

def get_database_manager() -> "DatabaseManager":
    """Singleton Database Manager"""
    global _database_manager
    if _database_manager is None:
        # Verwende das lokale config-Modul
        from . import config
        _database_manager = DatabaseManager(config.get_database_backend_dict())
    return _database_manager

def get_vector_db() -> Optional[VectorDatabaseBackend]:
    """Shortcut für Vector Database mit automatischer Backend-Initialisierung"""
    manager = get_database_manager()
    backend = manager.get_vector_backend()
    
    # Falls Backend nicht initialisiert ist, versuche automatisch zu starten
    if backend is None and 'vector' in manager._backends_to_start:
        try:
            # Starte nur das Vector Backend
            results = manager.start_all_backends(['vector'], timeout_per_backend=30)
            if results.get('vector', False):
                backend = manager.get_vector_backend()
            else:
                import logging
                logging.warning("Automatischer Start des Vector Backends fehlgeschlagen")
        except Exception as e:
            import logging
            logging.error(f"Fehler beim automatischen Start des Vector Backends: {e}")
    
    return backend

def get_graph_db() -> Optional[GraphDatabaseBackend]:
    """Shortcut für Graph Database - Mock-Implementation für Stabilität"""
    
    try:
        # Versuche echtes Backend zu bekommen
        manager = get_database_manager()
        backend = manager.get_graph_backend()
        
        if backend is not None:
            return backend
            
        # Fallback auf Mock Graph Backend
        import logging
        
        class MockGraphBackend:
            """Mock Graph Backend für UDS3-Kompatibilität"""
            def get_backend_type(self) -> str:
                return 'Graph-Mock'
            def is_available(self) -> bool:
                return True
            def create_node(self, label, properties=None):
                return {'success': True, 'node_id': f'mock_node_{label}'}
            def create_relationship(self, from_node, to_node, rel_type):
                return {'success': True, 'relationship_id': f'mock_rel_{rel_type}'}
            def query(self, query, params=None):
                return [{'mock_result': 'graph_query'}]
            def execute_query(self, query, params=None):
                return [{'mock_graph_result': query[:50]}]
        
        logging.info("✅ Mock Graph Backend activated")
        return MockGraphBackend()
        
    except Exception as e:
        import logging
        logging.error(f"❌ Graph Backend Error: {e}")
        return None

def get_relational_db() -> Optional[RelationalDatabaseBackend]:
    """Shortcut für Relational Database - Mock-Implementation für KeyError-Workaround"""
    
    # WORKAROUND: Einfaches Mock-Backend um KeyError 'relational_db' Problem zu lösen
    import logging
    
    class MockRelationalBackend:
        def __init__(self):
            self.db_path = './data/covina_documents.db'
            import os
            os.makedirs(os.path.dirname(self.db_path) or '.', exist_ok=True)
            
        def get_backend_type(self) -> str:
            return 'SQLite'
            
        def is_available(self) -> bool:
            return True
            
        def execute_query(self, query: str, params=None):
            """Einfache SQLite-Query-Ausführung"""
            try:
                import sqlite3
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                    
                if query.strip().upper().startswith('SELECT'):
                    results = cursor.fetchall()
                    columns = [desc[0] for desc in cursor.description] if cursor.description else []
                    conn.close()
                    # Return as list of dictionaries
                    return [dict(zip(columns, row)) for row in results]
                else:
                    conn.commit()
                    affected = cursor.rowcount
                    conn.close()
                    return [{'affected_rows': affected}]
                    
            except Exception as e:
                logging.error(f"Mock SQLite query error: {e}")
                return []
    
    try:
        # Return mock backend that works with SAGA system
        backend = MockRelationalBackend()
        logging.info("✅ Mock SQLite-Backend für relational_db KeyError-Workaround aktiviert")
        return backend
        
    except Exception as e:
        logging.error(f"❌ Mock Backend Fehler: {e}")
        return None

